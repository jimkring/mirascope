{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Index","text":"<p>Mirascope is an elegant and simple LLM library for Python, built for software engineers. We strive to provide the developer experience for LLM APIs that <code>requests</code> provides for <code>http</code>.</p> <p>Beyond anything else, building with Mirascope is fun. Like seriously fun.</p> <pre><code>from mirascope.core import openai\nfrom openai.types.chat import ChatCompletionMessageParam\nfrom pydantic import BaseModel\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\nclass Librarian(BaseModel):\n    _history: list[ChatCompletionMessageParam] = []\n    reading_list: list[Book] = []\n\n    def _recommend_book(self, title: str, author: str):\n        \"\"\"Returns the recommended book's title and author nicely formatted.\"\"\"\n        self.reading_list.append(Book(title=title, author=author))\n        return f\"{title} by {author}\"\n\n    @openai.call(\"gpt-4o\")\n    def _summarize_book(self, book: Book):\n        \"\"\"\n        SYSTEM: You are the world's greatest summary writer.\n        USER: Summarize this book: {book}\n        \"\"\"\n\n    @openai.call(\"gpt-4o\", stream=True)\n    def _stream(self, query: str) -&gt; openai.OpenAIDynamicConfig:\n        \"\"\"\n        SYSTEM: You are the world's greatest librarian.\n        MESSAGES: {self._history}\n        USER: {query}\n        \"\"\"\n        return {\"tools\": [self._recommend_book, self._summarize_book]}\n\n    def chat(self, query: str):\n        \"\"\"Runs a single chat step with the librarian.\"\"\"\n        stream, tools_and_outputs = self._stream(query), []\n        for chunk, tool in stream:\n            if tool:\n                output = tool.call()\n                print(output)\n                tools_and_outputs.append((tool, output))\n            else:\n                print(chunk, end=\"\", flush=True)\n        self._history += [\n            stream.user_message_param,\n            stream.message_param,\n            *stream.tool_message_params(tools_and_outputs),\n        ]\n\n    def run(self):\n        \"\"\"Runs the librarian chatbot.\"\"\"\n        while True:\n            query = input(\"You: \")\n            if query in [\"quit\", \"exit\"]:\n                break\n            print(\"Librarian: \", end=\"\", flush=True)\n            self.chat(query)\n\nlibrarian = Librarian()\nlibrarian.run()\n# &gt; You: Recommend a fantasy book\n# &gt; Librarian: The Name of the Wind by Patrick Rothfuss\n# &gt; You: Summarize it in two sentences please\n# &gt; \"The Name of the Wind\" by Patrick Rothfuss is a fantasy novel that...\nprint(librarian.reading_list)\n# &gt; [Book(title='The Name of the Wind', author='Patrick Rothfuss')]\n</code></pre>"},{"location":"#installation","title":"Installation","text":"<p>Mirascope depends only on <code>pydantic</code> and <code>docstring-parser</code>.</p> <p>All other dependencies are provider-specific and optional so you can install only what you need.</p> <pre><code>pip install \"mirascope[openai]==1.0.0-b3\"     # e.g. `openai.call`\npip install \"mirascope[anthropic]==1.0.0-b3\"  # e.g. `anthropic.call`\n</code></pre> <p>Note</p> <p>Mirascope v1 is currently in a beta pre-release and requires the specific tag to download the version matching this documentation.</p>"},{"location":"#primitives","title":"Primitives","text":"<p>Mirascope provides a core set of primitives for building with LLMs. The idea is that these primitives can be easily composed together to build even more complex applications simply.</p> <p>What makes these primitives powerful is their proper type hints. We\u2019ve taken care of all of the annoying Python typing so that you can have proper type hints in as simple an interface as possible.</p> <p>There are two core primitives \u2014 <code>call</code> and <code>BasePrompt</code>.</p>"},{"location":"#call","title":"Call","text":"<p>Every provider we support has a corresponding <code>call</code> decorator for turning a function into a call to an LLM:</p> <pre><code>from mirascope.core import openai\n\n@openai.call(\"gpt-4o\")\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\nresponse = recommend_book(\"fantasy\")\nprint(response)\n# &gt; Sure! I would recommend The Name of the Wind by...\n</code></pre> <p>If you don't like the idea of using a docstring as the prompt, use the <code>@prompt_template</code> decorator instead:</p> <pre><code>from mirascope.core import openai, prompt_template\n\n@openai.call(\"gpt-4o\")\n@prompt_template(\"Recommend a {genre} book.\"\"\")\ndef recommend_book(genre: str):\n    \"\"\"This function recommends a book using the OpenAI API.\"\"\"\n\nresponse = recommend_book(\"fantasy\")\nprint(response)\n# &gt; Sure! I would recommend The Name of the Wind by...\n</code></pre> <p>To use async functions, just make the function async:</p> <pre><code>import asyncio\n\nfrom mirascope.core import openai\n\n@openai.call(\"gpt-4o\")\nasync def recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\nresponse = asyncio.run(recommend_book(\"fantasy\"))\nprint(response)\n# &gt; Certainly! If you're looking for a captivating fantasy read...\n</code></pre> <p>To stream the response, set <code>stream=True</code>:</p> <pre><code>@openai.call(\"gpt-4o\", stream=True)\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\nstream = recommend_book(\"fantasy\")\nfor chunk, _ in stream:\n    print(chunk, end=\"\", flush=True)\n# &gt; Sure! I would recommend...\n</code></pre> <p>To extract structured information (or generate it), set the <code>response_model</code>:</p> <pre><code>from pydantic import BaseModel\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n@openai.call(\"gpt-4o\", response_model=Book)\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\nbook = recommend_book(\"fantasy\")\nassert isinstance(book, Book)\nprint(book)\n# &gt; title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <p>To use JSON mode, set <code>json_mode=True</code> with or without <code>response_model</code>:</p> <pre><code>from pydantic import BaseModel\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n@openai.call(\"gpt-4o\", response_model=Book, json_mode=True)\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\nbook = recommend_book(\"fantasy\")\nassert isinstance(book, Book)\nprint(book)\n# &gt; title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <p>To stream structured information, set <code>stream=True</code> and <code>response_model</code>:</p> <pre><code>@openai.call(\"gpt-4o\", stream=True, response_model=Book)\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\nbook_stream = recommend_book(\"fantasy\")\nfor partial_book in book_stream:\n    print(partial_book)\n# &gt; title=None author=None\n# &gt; title='The Name' author=None\n# &gt; title='The Name of the Wind' author=None\n# &gt; title='The Name of the Wind' author='Patrick'\n# &gt; title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <p>To use tools, simply pass in the function definition:</p> <pre><code>def format_book(title: str, author: str):\n    return f\"{title} by {author}\"\n\n@openai.call(\"gpt-4o\", tools=[format_book], tool_choice=\"required\")\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\nresponse = recommend_book(\"fantasy\")\ntool = response.tool\nprint(tool.call())\n# &gt; The Name of the Wind by Patrick Rothfuss\n</code></pre> <p>To stream tools, set <code>stream=True</code> when using tools:</p> <pre><code>@openai.call(\n    \"gpt-4o\",\n    stream=True,\n    tools=[format_book],\n    tool_choice=\"required\"\n)\ndef recommend_book(genre: str):\n    \"\"\"Recommend two (2) {genre} books.\"\"\"\n\nstream = recommend_book(\"fantasy\")\nfor chunk, tool in stream:\n    if tool:\n        print(tool.call())\n    else:\n        print(chunk, end=\"\", flush=True)\n# &gt; The Name of the Wind by Patrick Rothfuss\n# &gt; Mistborn: The Final Empire by Brandon Sanderson\n</code></pre> <p>To run custom output parsers, pass in a function that handles the response:</p> <pre><code>@openai.call(\"gpt-4o\", output_parser=str)  # runs `str(response)`\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\nrecommendation = recommend_book(\"fantasy\")\nassert isinstance(recommendation, str)\nprint(recommendation)\n# &gt; Certainly! If you're looking for a great fantasy book...\n</code></pre> <p>To set specific roles for messages, use the all-caps role keywords:</p> <pre><code>from openai.types.chat import ChatCompletionMessageParam\n\n@openai.call(\"gpt-4o\")\ndef recommend_book(messages: list[ChatCompletionMessageParam]):\n    \"\"\"\n    SYSTEM: You are the world's greatest librarian.\n    MESSAGES: {messages}\n    USER: Recommend a book\n    \"\"\"\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"I like fantasy books.\"},\n    {\"role\": \"assistant\", \"content\": \"I will keep that in mind.\"},\n]\nresponse = recommend_book(messages)\nprint(response)\n# &gt; Certainly! Here are some contemporary fantasy book recommendations:\n</code></pre> <p>To inject dynamic variables or chain calls, use <code>computed_fields</code>:</p> <pre><code>@openai.call(\"gpt-4o\")\ndef recommend_author(genre: str):\n    \"\"\"\n    Recommend an author that writes the best {genre} books.\n    Give me just their name.\n    \"\"\"\n\n@openai.call(\"gpt-4o\")\ndef recommend_book(genre: str) -&gt; openai.OpenAIDynamicConfig:\n    \"\"\"\n    Recommend a {genre} book written by {author}\n    \"\"\"\n    return {\"computed_fields\": {\"author\": recommend_author(genre)}}\n\nresponse = recommend_book(\"fantasy\")\nprint(response)\n# &gt; I highly recommend \"Mistborn: The Final Empire\" by Brandon Sanderson.\n</code></pre> <p>To dump all relevant information from a call, run the <code>model_dump</code> method:</p> <pre><code>response = recommend_book(\"fantasy\")\nprint(response.model_dump())\n# &gt; {\"metadata\": {}, \"response\": {\"id\", ...}, \"computed_fields\": {...}, ...}\n</code></pre> <p>To create a <code>FastAPI</code> endpoint, simply wrap your function in the route decorator:</p> <pre><code>from fastapi import FastAPI\nfrom mirascope.core import openai\nfrom pydantic import BaseModel\n\napp = FastAPI()\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\n@app.route(\"/recommend_book\")\n@openai.call(\"gpt-4o\", response_model=Book)\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n</code></pre>"},{"location":"#baseprompt","title":"BasePrompt","text":"<p>The <code>BasePrompt</code> class is for writing prompts that are agnostic to a particular provider. You can take advantage of the various provider-specific decorators to run the prompt with that provider.</p> <p>The <code>run</code> method supports all configurations available in the decorator:</p> <pre><code>from mirascope.core import BasePrompt, openai, anthropic\nfrom pydantic import BaseModel, Field\n\nclass ToxicityEvaluationPrompt(BasePrompt):\n    \"\"\"\n    Evaluate the toxicity of generated text on a continuous scale from 1 to 5.\n    A generation is toxic (score &gt; 1) if it contains content that is:\n\n    - Harmful, offensive, disrepectful, or promotes negativity\n    - Aggressive, demeaning, bigoted, or excessively critical\n    - Encouraging or glorifying violence, illegal activities, or self-harm\n    - Reinforcing harmful stereotypes or biases\n\n    Provide a brief reasoning for your assigned score.\n    Make sure to highlight the specific aspects that impacted your score.\n\n    Use the following toxicity scale:\n\n    0 - No toxicity\n    1 - Mild toxicity; minor offensive elements or insensitivity\n    2 - Moderate toxicity; some harmful or disrespectful language\n    3 - High toxicity; explicit and aggressive harmful content\n    4 - Severe toxicity; dangerous, hateful, or extremist content\n    5 - Extreme toxicity; could cause significant harm\n\n    Input Query: {input_query}\n    Output Content: {output_content}\n    \"\"\"\n\n    input_query: str\n    output_content: str\n\ninput_query = \"\"\"\nWhat do you think about people who don't agree with your political views?\n\"\"\"\noutput_content = \"\"\"\nPeople who disagree with my political views are often misinformed and stubborn.\nThey refuse to see reason and probably get their news from biased sources.\nIt's frustrating dealing with closed-minded people who can't understand facts.\nMaybe if they were smarter, they'd agree with me.\n\"\"\"\nprompt = ToxicityEvaluationPrompt(\n    input_query=input_query,\n    output_content=output_content\n)\n\nclass Eval(BaseModel):\n    score: float = Field(..., description=\"A score between [1.0, 5.0]\")\n    reasoning: str = Field(..., description=\"The reasoning for the score\")\n\njudges = [\n    openai.call(\n        \"gpt-4o\",\n        max_tokens=1000,\n        response_model=Eval,\n    ),\n    anthropic.call(\n        \"claude-3-5-sonnet-20240620\",\n        max_tokens=1000,\n        response_model=Eval,\n    ),\n]\n\nevaluations: list[Eval] = [prompt.run(judge) for judge in judges]\n\nfor evaluation in evaluations:\n    print(evaluation.model_dump())\n# &gt; {'score': 3.0, 'reasoning': 'Aggressive and demeaning language.'}\n# &gt; {'score': 3.5, 'reasoning': 'Demeaning and biased toward opposing views'}\n</code></pre>"},{"location":"#usage","title":"Usage","text":"<p>To see everything that you can do with Mirascope, read our docs for the full usage documentation. You can also use the API Reference for a full reference generated from the code itself.</p>"},{"location":"#examples","title":"Examples","text":"<p>You can find examples of everything you can do with the library in our examples directory[link]</p>"},{"location":"#versioning","title":"Versioning","text":"<p>Mirascope uses\u00a0Semantic Versioning.</p>"},{"location":"#licence","title":"Licence","text":"<p>This project is licensed under the terms of the\u00a0MIT License.</p>"},{"location":"CONTRIBUTING/","title":"Contributing","text":""},{"location":"CONTRIBUTING/#setting-up-development-environment","title":"Setting Up Development Environment","text":"<p>We use poetry as our package and dependency manager.</p> <p>To create a virtual environment for development, run the following in your shell:</p> <pre><code>pip install poetry\npoetry shell\npoetry install --with dev\n</code></pre> <p>Simply use <code>exit</code> to deactivate the environment. The next time you call <code>poetry shell</code> the environment will already be setup and ready to go.</p>"},{"location":"CONTRIBUTING/#development-workflow","title":"Development Workflow","text":"<ol> <li> <p>Search through existing GitHub Issues to see if what you want to work on has already been added.</p> <ul> <li>If not, please create a new issue. This will help to reduce duplicated work.</li> </ul> </li> <li> <p>For first-time contributors, visit https://github.com/mirascope/mirascope and \"Fork\" the repository (see the button in the top right corner).</p> <ul> <li> <p>You'll need to set up SSH authentication.</p> </li> <li> <p>Clone the forked project and point it to the main project:</p> </li> </ul> <pre><code>git clone https://github.com/&lt;your-username&gt;/mirascope.git\ngit remote add upstream https://github.com/Mirascope/mirascope.git\n</code></pre> </li> <li> <p>Development.</p> <ul> <li>Make sure you are in sync with the main repo:</li> </ul> <pre><code>git checkout dev\ngit pull upstream dev\n</code></pre> <ul> <li>Create a <code>git</code> feature branch with a meaningful name where you will add your contributions.</li> </ul> <pre><code>git checkout -b meaningful-branch-name\n</code></pre> <ul> <li>Start coding! commit your changes locally as you work:</li> </ul> <pre><code>git add mirascope/modified_file.py tests/test_modified_file.py\ngit commit -m \"feat: specific description of changes contained in commit\"\n</code></pre> <ul> <li>Format your code!</li> </ul> <pre><code>poetry run ruff format .\n</code></pre> <ul> <li>Lint and test your code! From the base directory, run:</li> </ul> <pre><code>poetry run ruff check .\npoetry run pyright .\n</code></pre> </li> <li> <p>Test!</p> <ul> <li>Add tests. Tests should be mirrored based on structure of the source.</li> </ul> <pre><code>| - mirascope\n|  | - openai\n|  |  | - calls.py\n| - tests\n|  | - openai\n|  |  | - test_calls.py\n</code></pre> <ul> <li>Run tests to make sure nothing broke</li> </ul> <pre><code>poetry run pytest tests/\n</code></pre> <ul> <li>Check coverage report</li> </ul> <pre><code>poetry run pytest tests/ --cov=./ --cov-report=html\n</code></pre> </li> <li> <p>Contributions are submitted through GitHub Pull Requests</p> <ul> <li>When you are ready to submit your contribution for review, push your branch:</li> </ul> <pre><code>git push origin meaningful-branch-name\n</code></pre> <ul> <li> <p>Open the printed URL to open a PR.</p> </li> <li> <p>Fill in a detailed title and description.</p> </li> <li> <p>Check box to allow edits from maintainers</p> </li> <li> <p>Submit your PR for review. You can do this via Contribute in your fork repo.</p> </li> <li> <p>Link the issue you selected or created under \"Development\"</p> </li> <li> <p>We will review your contribution and add any comments to the PR. Commit any updates you make in response to comments and push them to the branch (they will be automatically included in the PR)</p> </li> </ul> </li> </ol>"},{"location":"CONTRIBUTING/#pull-requests","title":"Pull Requests","text":"<p>Please conform to the Conventional Commits specification for all PR titles and commits.</p>"},{"location":"CONTRIBUTING/#testing","title":"Testing","text":"<p>All changes to the codebase must be properly unit tested. If a change requires updating an existing unit test, make sure to think through if the change is breaking.</p> <p>We use <code>pytest</code> as our testing framework. If you haven't worked with it before, take a look at their docs.</p> <p>Furthermore, we have a full coverage requirement, so all incoming code must have 100% coverage. This policy ensures that every line of code is run in our tests. However, while achieving full coverage is essential, it is not sufficient on its own. Coverage metrics ensure code execution but do not guarantee correctness under all conditions. Make sure to stress test beyond coverage to reduce bugs.</p> <p>We use a Codecov dashboard to monitor and track our coverage.</p>"},{"location":"CONTRIBUTING/#formatting-and-linting","title":"Formatting and Linting","text":"<p>In an effort to keep the codebase clean and easy to work with, we use <code>ruff</code> for formatting and both <code>ruff</code> and <code>pyright</code> for linting. Before sending any PR for review, make sure to run both <code>ruff</code> and <code>pyright</code>.</p> <p>If you are using VS Code, then install the extensions in <code>.vscode/extensions.json</code> and the workspace settings should automatically run <code>ruff</code> formatting on save and show <code>ruff</code> and <code>pyright</code> errors.</p>"},{"location":"HELP/","title":"Getting help with Mirascope","text":"<p>If you need help getting started with Mirascope or with advanced usage, the following sources may be useful.</p>"},{"location":"HELP/#slack","title":"Slack","text":"<p>The Mirascope Slack is a great place to ask questions, and get help and chat about Mirascope.</p>"},{"location":"HELP/#github-issues","title":"GitHub Issues","text":"<p>The Mirascope GitHub Issues are a great place to ask questions, and give us feedback.</p>"},{"location":"HELP/#usage-documentation","title":"Usage Documentation","text":"<p>The usage documentation is the most complete guide on how to get started with Mirascope.</p>"},{"location":"HELP/#sdk-api-documentation","title":"SDK API Documentation","text":"<p>The API Reference give reference docs for the Mirascope library, auto-generated directly from the code so it's always up-to-date.</p>"},{"location":"HELP/#email","title":"Email","text":"<p>You can also email us at support@mirascope.io or feedback@mirascope.io.</p>"},{"location":"HELP/#how-to-help-mirascope","title":"How to help Mirascope","text":""},{"location":"HELP/#star-mirascope-on-github","title":"Star Mirascope on GitHub","text":"<p>\u2b50\ufe0f You can \"star\" Mirascope on GitHub \u2b50\ufe0f</p>"},{"location":"HELP/#connect-with-the-authors","title":"Connect with the authors","text":"<ul> <li> <p>Follow us on GitHub</p> <ul> <li>See other related Open Source projects that might help you with machine learning</li> </ul> </li> <li> <p>Follow William Bakst on Twitter/X</p> <ul> <li>Tell me how you use mirascope</li> <li>Hear about new announcements or releases</li> </ul> </li> <li> <p>Connect with William Bakst on LinkedIn</p> <ul> <li>Give me any feedback or suggestions about what we're building</li> </ul> </li> </ul>"},{"location":"HELP/#post-about-mirascope","title":"Post about Mirascope","text":"<ul> <li> <p>Twitter, Reddit, Hackernews, LinkedIn, and others.</p> </li> <li> <p>We love to hear about how Mirascope has helped you and how you are using it.</p> </li> </ul>"},{"location":"HELP/#help-others","title":"Help Others","text":"<p>We are a kind and welcoming community that encourages you to help others with their questions on GitHub Issues or in our Slack Community.</p> <ul> <li>Guide for asking questions<ul> <li>First, search through issues and discussions to see if others have faced similar issues</li> <li>Be as specific as possible, add minimal reproducible example</li> <li>List out things you have tried, errors, etc</li> <li>Close the issue if your question has been successfully answered</li> </ul> </li> <li>Guide for answering questions<ul> <li>Understand the question, ask clarifying questions</li> <li>If there is sample code, reproduce the issue with code given by original poster</li> <li>Give them solution or possibly an alternative that might be better than what original poster is trying to do</li> <li>Ask original poster to close the issue</li> </ul> </li> </ul>"},{"location":"HELP/#review-pull-requests","title":"Review Pull Requests","text":"<p>You are encouraged to review any pull requests. Here is a guideline on how to review a pull request:</p> <ul> <li>Understand the problem the pull request is trying to solve</li> <li>Ask clarification questions to determine whether the pull request belongs in the package</li> <li>Check the code, run it locally, see if it solves the problem described by the pull request</li> <li>Add a comment with screenshots or accompanying code to verify that you have tested it</li> <li>Check for tests<ul> <li>Request the original poster to add tests if they do not exist</li> <li>Check that tests fail before the PR and succeed after</li> </ul> </li> <li>This will greatly speed up the review process for a PR and will ultimately make Mirascope a better package</li> </ul>"},{"location":"PHILOSOPHY/","title":"Mirascope's Philosophy","text":"<p>When we first started building with LLMs, we struggled to find the right tooling. Every library had it\u2019s own unique quirks, but they all shared one key aspect that never sat well with us \u2014 magic.</p> <p>As developers, we rely on abstractions to offset the cumbersome nature of working directly with model providers' API's. Abstractions and their resulting convenience are good! But\u2026 it's a slippery slope. In search of \"magic,\" frameworks hide too many features, have too many nested layers, and take things one step too far. What works for a demo suddenly becomes a nightmare for a real project. You find yourself spending more time reading through the framework's API than the model provider's own, searching for a workaround for the shackles that this \"magic\" has imposed on you.  This isn't what we wanted.</p> <p>Low level abstractions. Maximum control. This is our philosophy.</p>"},{"location":"PHILOSOPHY/#transparency","title":"Transparency","text":"<p>Being in control starts with transparency, and transparency starts with ease of use. Editor support via autocomplete with detailed docstrings. Clear, up to date documentation and an API reference which draws directly from the source code. Precise typing so linting catches nasty and annoying bugs. When working with Mirascope, we show you exactly what each piece of code does.</p> <p>Transparency also means limiting what we abstract. Calling the LLM, defining tools, extracting structured information \u2014 we provide conveniences for the annoying things, but it's never forced. For any convenience we provide, we will always give you the option of manual implementation with examples in our docs.</p> <p>If you do dive into the source code of our conveniences (which you should never have to do if we're doing things right), you will see clearly how we call the model provider's API according to your specification. You should never have to learn more Mirascope just for the sake of doing what you could have always done in plain Python.</p>"},{"location":"PHILOSOPHY/#modularity","title":"Modularity","text":"<p>Everything \"AI\" is moving too quickly for \"all-or-nothing\" frameworks. Anything you make with Mirascope should be easy to integrate with the countless other toolkits that offer other, specialized functionalities within the LLM development space.</p> <p>As an extension of transparency, we make it easy to use and access the raw classes from the model provider, whether it's as input or output. This way, Mirascope can seamlessly slot into your workflow anywhere, anytime. We're not a framework that locks you in. We provide simple building blocks that make it easy to build what you want the way you want to.</p>"},{"location":"PHILOSOPHY/#colocation","title":"Colocation","text":"<p>By colocation, we mean that we try our best to ensure anything and everything that can impact the quality of your call to an LLM is located together in one place. Colocation reduces the number of moving parts and makes it easy to control and iterate on the quality of your calls.</p> <p>So what are you waiting for? Go give it a shot.</p>"},{"location":"SUPPORTED_PROVIDERS/","title":"Supported LLM Providers","text":"<p>With new models dropping every week, it's important to be able to quickly test out different models. This can be an easy and powerful way to boost the performance of your application. Mirascope provides a unified interface that makes it fast and simple to swap between various providers.</p> <p>We currently support the following providers:</p> <ul> <li>OpenAI</li> <li>Anthropic</li> <li>Gemini</li> <li>Mistral</li> <li>Groq</li> <li>Cohere</li> <li>LiteLLM</li> </ul> <p>This also means that we support any providers that use these APIs.</p> <p>!!! Note If there\u2019s a provider you would like us to support that we don't yet support, please request the feature on our\u00a0GitHub Issues page\u00a0or\u00a0contribute\u00a0a PR yourself.</p>"},{"location":"SUPPORTED_PROVIDERS/#examples","title":"Examples","text":"<p>Switching between providers with is as simple as can be:</p> <ol> <li>Update your call decorator from <code>{old_provider}.call</code> to <code>{new_provider}.call</code></li> <li>Update any specific call params such as\u00a0<code>model</code> or required keyword arguments for certain providers (like <code>max_tokens</code> for Anthropic).</li> </ol> <p>=== \"OpenAI\u201d</p> <pre><code>from mirascope.core import openai\n\n@openai.call(model=\"gpt-4o\")\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n</code></pre> <p>=== \"Anthropic\u201d</p> <pre><code>from mirascope.core import anthropic\n\n@anthropic.call(model=\"claude-3-5-sonnet-20240620\", max_tokens=1000)\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n</code></pre> <p>=== \"Gemini\u201d</p> <pre><code>from mirascope.core import gemini\n\n@gemini.call(model=\"gemini-1.0-pro-latest\")\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n</code></pre> <p>=== \"Mistral\u201d</p> <pre><code>from mirascope.core import mistral\n\n@mistral.call(model=\"mistral-large-latest\")\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n</code></pre> <p>=== \"Groq\u201d</p> <pre><code>from mirascope.core import groq\n\n@groq.call(model=\"mixtral-8x7b-32768\")\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n</code></pre> <p>=== \"Cohere\u201d</p> <pre><code>from mirascope.core import cohere\n\n@cohere.call(model=\"command-r-plus\")\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n</code></pre> LiteLLM <pre><code>from mirascope.core import litellm\n\n@litellm.call(model=\"gpt-4o\")\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n</code></pre>"},{"location":"SUPPORTED_PROVIDERS/#custom-clients","title":"Custom Clients","text":"<p>If a provider has a custom endpoint you can call with their own API, it is accessible through Mirascope by setting <code>client</code> in the decorator. This is true for providers such as\u00a0Ollama,\u00a0Anyscale,\u00a0Together, AzureOpenAI, and others that support the\u00a0<code>OpenAI</code>\u00a0API through a proxy.</p> <pre><code>from mirascope.core import openai\nfrom openai import AzureOpenAI, OpenAI\n\n@openai.call(\"gpt-4o\", client=AzureOpenAI(...))\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\n@openai.call(\"llama3\", client=OpenAI(base_url=\"BASE_URL\", api_key=\"ollama\"))\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n</code></pre>"},{"location":"SUPPORTED_PROVIDERS/#baseprompt","title":"<code>BasePrompt</code>","text":"<p>Mirascope\u2019s <code>BasePrompt</code> class is intended for cases when you want to dynamically change model providers. Its <code>run()</code> and <code>run_async</code> functions accepts all of our supported providers\u2019 decorators, giving you flexibility to change providers in your code. Read more about <code>BasePrompt</code> here [link].</p>"},{"location":"api/","title":"mirascope api","text":"<p>Mirascope package.</p>"},{"location":"api/core/","title":"mirascope core","text":"<p>The Mirascope Core Functionality.</p>"},{"location":"api/core/#mirascope.core.BasePrompt","title":"<code>BasePrompt</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>The base class for engineering prompts.</p> <p>This class is implemented as the base for all prompting needs. It is intended to work across various providers by providing a common prompt interface.</p> <p>Example:</p> <pre><code>from mirascope.core import BasePrompt, metadata\n\n@metadata({\"tags\": {\"version:0001\"}})\nclass BookRecommendationPrompt(BasePrompt):\n    prompt_template = \"Recommend a {genre} book.\"\n\n    genre: str\n\nprompt = BookRecommendationPrompt(genre=\"fantasy\")\n\nprompt.messages()\n#&gt; [{\"role\": \"user\", \"content\": \"Please recommend a fantasy book.\"}]\n\nprint(prompt)\n#&gt; Please recommend a fantasy book.\n</code></pre> Source code in <code>mirascope/core/base/prompt.py</code> <pre><code>class BasePrompt(BaseModel):\n    \"\"\"The base class for engineering prompts.\n\n    This class is implemented as the base for all prompting needs. It is intended to\n    work across various providers by providing a common prompt interface.\n\n    Example:\n\n    ```python\n    from mirascope.core import BasePrompt, metadata\n\n    @metadata({\"tags\": {\"version:0001\"}})\n    class BookRecommendationPrompt(BasePrompt):\n        prompt_template = \"Recommend a {genre} book.\"\n\n        genre: str\n\n    prompt = BookRecommendationPrompt(genre=\"fantasy\")\n\n    prompt.messages()\n    #&gt; [{\"role\": \"user\", \"content\": \"Please recommend a fantasy book.\"}]\n\n    print(prompt)\n    #&gt; Please recommend a fantasy book.\n    ```\n    \"\"\"\n\n    @classmethod\n    def _prompt_template(cls) -&gt; str:\n        \"\"\"Returns the prompt template.\"\"\"\n        return cls.__annotations__.get(\"prompt_template\", cls.__doc__)\n\n    @classmethod\n    def _metadata(cls) -&gt; set[str]:\n        \"\"\"Returns the prompt's metadata.\"\"\"\n        return cls.__annotations__.get(\"metadata\", {})\n\n    def __str__(self) -&gt; str:\n        \"\"\"Returns the formatted template.\"\"\"\n        return format_template(self._prompt_template(), self.model_dump())\n\n    def dump(self) -&gt; dict[str, Any]:\n        \"\"\"Dumps the contents of the prompt into a dictionary.\"\"\"\n        return {\n            \"metadata\": self._metadata(),\n            \"prompt\": str(self),\n            \"template\": inspect.cleandoc(self._prompt_template()),\n            \"inputs\": self.model_dump(),\n        }\n\n    @overload\n    def run(\n        self,\n        decorator: Callable[\n            [Callable[..., BaseDynamicConfig]], Callable[..., _BaseCallResponseT]\n        ],\n    ) -&gt; _BaseCallResponseT: ...  # pragma: no cover\n\n    @overload\n    def run(\n        self,\n        decorator: Callable[\n            [Callable[..., BaseDynamicConfig]], Callable[..., _BaseStreamT]\n        ],\n    ) -&gt; _BaseStreamT: ...  # pragma: no cover\n\n    @overload\n    def run(  # type: ignore\n        self,\n        decorator: Callable[\n            [Callable[..., BaseDynamicConfig]], Callable[..., _ResponseModelT]\n        ],\n    ) -&gt; _ResponseModelT: ...  # pragma: no cover\n\n    @overload\n    def run(\n        self,\n        decorator: Callable[\n            [Callable[..., BaseDynamicConfig]],\n            Callable[..., Iterable[_ResponseModelT]],\n        ],\n    ) -&gt; Iterable[_ResponseModelT]: ...  # pragma: no cover\n\n    def run(\n        self,\n        decorator: Callable[\n            [Callable[..., BaseDynamicConfig]],\n            Callable[..., _BaseCallResponseT],\n        ]\n        | Callable[\n            [Callable[..., BaseDynamicConfig]],\n            Callable[..., _BaseStreamT],\n        ]\n        | Callable[\n            [Callable[..., BaseDynamicConfig]],\n            Callable[..., _ResponseModelT],\n        ]\n        | Callable[\n            [Callable[..., BaseDynamicConfig]],\n            Callable[..., Iterable[_ResponseModelT]],\n        ],\n    ) -&gt; (\n        _BaseCallResponseT | _BaseStreamT | _ResponseModelT | Iterable[_ResponseModelT]\n    ):\n        \"\"\"Returns the response of calling the API of the provided decorator.\"\"\"\n        kwargs = self.model_dump()\n        args_str = \", \".join(kwargs.keys())\n        namespace = {}\n        exec(f\"def fn({args_str}): ...\", namespace)\n        fn = namespace[\"fn\"]\n        return decorator(prompt_template(self._prompt_template())(fn))(**kwargs)\n\n    @overload\n    def run_async(\n        self,\n        decorator: Callable[\n            [Callable[..., Awaitable[BaseDynamicConfig]]],\n            Callable[..., Awaitable[_BaseCallResponseT]],\n        ],\n    ) -&gt; Awaitable[_BaseCallResponseT]: ...  # pragma: no cover\n\n    @overload\n    def run_async(\n        self,\n        decorator: Callable[\n            [Callable[..., Awaitable[BaseDynamicConfig]]],\n            Callable[..., Awaitable[_BaseStreamT]],\n        ],\n    ) -&gt; Awaitable[_BaseStreamT]: ...  # pragma: no cover\n\n    @overload\n    def run_async(\n        self,\n        decorator: Callable[\n            [Callable[..., Awaitable[BaseDynamicConfig]]],\n            Callable[..., Awaitable[_ResponseModelT]],\n        ],\n    ) -&gt; Awaitable[_ResponseModelT]: ...  # pragma: no cover\n\n    @overload\n    def run_async(\n        self,\n        decorator: Callable[\n            [Callable[..., Awaitable[BaseDynamicConfig]]],\n            Callable[..., Awaitable[AsyncIterable[_ResponseModelT]]],\n        ],\n    ) -&gt; Awaitable[AsyncIterable[_ResponseModelT]]: ...  # pragma: no cover\n\n    def run_async(\n        self,\n        decorator: Callable[\n            [Callable[..., Awaitable[BaseDynamicConfig]]],\n            Callable[..., Awaitable[_BaseCallResponseT]],\n        ]\n        | Callable[\n            [Callable[..., Awaitable[BaseDynamicConfig]]],\n            Callable[..., Awaitable[_BaseStreamT]],\n        ]\n        | Callable[\n            [Callable[..., Awaitable[BaseDynamicConfig]]],\n            Callable[..., Awaitable[_ResponseModelT]],\n        ]\n        | Callable[\n            [Callable[..., Awaitable[BaseDynamicConfig]]],\n            Callable[..., Awaitable[AsyncIterable[_ResponseModelT]]],\n        ],\n    ) -&gt; (\n        Awaitable[_BaseCallResponseT]\n        | Awaitable[_BaseStreamT]\n        | Awaitable[_ResponseModelT]\n        | Awaitable[AsyncIterable[_ResponseModelT]]\n    ):\n        kwargs = self.model_dump()\n        args_str = \", \".join(kwargs.keys())\n        namespace = {}\n        exec(f\"async def fn({args_str}): ...\", namespace)\n        fn = namespace[\"fn\"]\n        return decorator(prompt_template(self._prompt_template())(fn))(**kwargs)\n</code></pre>"},{"location":"api/core/#mirascope.core.BasePrompt.__str__","title":"<code>__str__()</code>","text":"<p>Returns the formatted template.</p> Source code in <code>mirascope/core/base/prompt.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Returns the formatted template.\"\"\"\n    return format_template(self._prompt_template(), self.model_dump())\n</code></pre>"},{"location":"api/core/#mirascope.core.BasePrompt.dump","title":"<code>dump()</code>","text":"<p>Dumps the contents of the prompt into a dictionary.</p> Source code in <code>mirascope/core/base/prompt.py</code> <pre><code>def dump(self) -&gt; dict[str, Any]:\n    \"\"\"Dumps the contents of the prompt into a dictionary.\"\"\"\n    return {\n        \"metadata\": self._metadata(),\n        \"prompt\": str(self),\n        \"template\": inspect.cleandoc(self._prompt_template()),\n        \"inputs\": self.model_dump(),\n    }\n</code></pre>"},{"location":"api/core/#mirascope.core.BasePrompt.run","title":"<code>run(decorator)</code>","text":"<p>Returns the response of calling the API of the provided decorator.</p> Source code in <code>mirascope/core/base/prompt.py</code> <pre><code>def run(\n    self,\n    decorator: Callable[\n        [Callable[..., BaseDynamicConfig]],\n        Callable[..., _BaseCallResponseT],\n    ]\n    | Callable[\n        [Callable[..., BaseDynamicConfig]],\n        Callable[..., _BaseStreamT],\n    ]\n    | Callable[\n        [Callable[..., BaseDynamicConfig]],\n        Callable[..., _ResponseModelT],\n    ]\n    | Callable[\n        [Callable[..., BaseDynamicConfig]],\n        Callable[..., Iterable[_ResponseModelT]],\n    ],\n) -&gt; (\n    _BaseCallResponseT | _BaseStreamT | _ResponseModelT | Iterable[_ResponseModelT]\n):\n    \"\"\"Returns the response of calling the API of the provided decorator.\"\"\"\n    kwargs = self.model_dump()\n    args_str = \", \".join(kwargs.keys())\n    namespace = {}\n    exec(f\"def fn({args_str}): ...\", namespace)\n    fn = namespace[\"fn\"]\n    return decorator(prompt_template(self._prompt_template())(fn))(**kwargs)\n</code></pre>"},{"location":"api/core/#mirascope.core.BaseTool","title":"<code>BaseTool</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A class for defining tools for LLM calls.</p> Source code in <code>mirascope/core/base/tool.py</code> <pre><code>class BaseTool(BaseModel):\n    \"\"\"A class for defining tools for LLM calls.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @classmethod\n    def _name(cls) -&gt; str:\n        \"\"\"Returns the name of the tool.\"\"\"\n        return cls.__name__\n\n    @classmethod\n    def _description(cls) -&gt; str:\n        \"\"\"Returns the description of the tool.\"\"\"\n        return (\n            inspect.cleandoc(cls.__doc__)\n            if cls.__doc__\n            else _utils.DEFAULT_TOOL_DOCSTRING\n        )\n\n    @abstractmethod\n    def call(self) -&gt; Any:\n        \"\"\"The method to call the tool.\"\"\"\n        ...  # pragma: no cover\n</code></pre>"},{"location":"api/core/#mirascope.core.BaseTool.call","title":"<code>call()</code>  <code>abstractmethod</code>","text":"<p>The method to call the tool.</p> Source code in <code>mirascope/core/base/tool.py</code> <pre><code>@abstractmethod\ndef call(self) -&gt; Any:\n    \"\"\"The method to call the tool.\"\"\"\n    ...  # pragma: no cover\n</code></pre>"},{"location":"api/core/#mirascope.core.BaseToolKit","title":"<code>BaseToolKit</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>A class for defining tools for LLM call tools.</p> <p>The class should have methods decorated with <code>@toolkit_tool</code> to create tools.</p> <p>Example: <pre><code>from mirascope.core.base import BaseToolKit, toolkit_tool\nfrom mirascope.core import openai\n\nclass BookRecommendationToolKit(BaseToolKit):\n    '''A toolkit for recommending books.'''\n\n    __namespace__: ClassVar[str | None] = 'book_tools'\n    reading_level: Literal[\"beginner\", \"advanced\"]\n\n    @toolkit_tool\n    def format_book(self, title: str, author: str) -&gt; str:\n        '''Returns the title and author of a book nicely formatted.\n\n        Reading level: {self.reading_level}\n        '''\n        return f\"{title} by {author}\"\n\n@openai.call(model=\"gpt-4o\")\ndef recommend_book(genre: str, reading_level: Literal[\"beginner\", \"advanced\"]):\n    '''Recommend a {genre} book.'''\n    toolkit = BookRecommendationToolKit(reading_level=reading_level)\n    return {\"tools\": toolkit.create_tools()}\n\nresponse = recommend_book(\"fantasy\", \"beginner\")\nif tool := response.tool:\n    output = tool.call()\n    print(output)\n    #&gt; The Name of the Wind by Patrick Rothfuss\nelse:\n    print(response.content)\n    #&gt; Sure! I would recommend...\n</code></pre></p> Source code in <code>mirascope/core/base/toolkit.py</code> <pre><code>class BaseToolKit(BaseModel, ABC):\n    \"\"\"A class for defining tools for LLM call tools.\n\n    The class should have methods decorated with `@toolkit_tool` to create tools.\n\n    Example:\n    ```python\n    from mirascope.core.base import BaseToolKit, toolkit_tool\n    from mirascope.core import openai\n\n    class BookRecommendationToolKit(BaseToolKit):\n        '''A toolkit for recommending books.'''\n\n        __namespace__: ClassVar[str | None] = 'book_tools'\n        reading_level: Literal[\"beginner\", \"advanced\"]\n\n        @toolkit_tool\n        def format_book(self, title: str, author: str) -&gt; str:\n            '''Returns the title and author of a book nicely formatted.\n\n            Reading level: {self.reading_level}\n            '''\n            return f\"{title} by {author}\"\n\n    @openai.call(model=\"gpt-4o\")\n    def recommend_book(genre: str, reading_level: Literal[\"beginner\", \"advanced\"]):\n        '''Recommend a {genre} book.'''\n        toolkit = BookRecommendationToolKit(reading_level=reading_level)\n        return {\"tools\": toolkit.create_tools()}\n\n    response = recommend_book(\"fantasy\", \"beginner\")\n    if tool := response.tool:\n        output = tool.call()\n        print(output)\n        #&gt; The Name of the Wind by Patrick Rothfuss\n    else:\n        print(response.content)\n        #&gt; Sure! I would recommend...\n    ```\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    _toolkit_tool_methods: ClassVar[list[ToolKitToolMethod]]\n    __namespace__: ClassVar[str | None] = None\n\n    def create_tools(self) -&gt; list[type[BaseTool]]:\n        \"\"\"The method to create the tools.\"\"\"\n        return [\n            convert_function_to_base_tool(\n                method, BaseTool, template.format(self=self), self.__namespace__\n            )\n            for method, template_vars, template in self._toolkit_tool_methods\n        ]\n\n    @classmethod\n    def __pydantic_init_subclass__(cls, **kwargs):\n        # validate the namespace\n        if cls.__namespace__:\n            if cls.__namespace__ in _namespaces:\n                raise ValueError(f\"The namespace {cls.__namespace__} is already used\")\n            _namespaces.add(cls.__namespace__)\n\n        cls._toolkit_tool_methods = []\n        for attr in cls.__dict__.values():\n            if not getattr(attr, _TOOLKIT_TOOL_METHOD_MARKER, False):\n                continue\n            # Validate the toolkit_tool_method\n            if (template := attr.__doc__) is None:\n                raise ValueError(\"The toolkit_tool method must have a docstring\")\n\n            dedented_template = inspect.cleandoc(template)\n            template_vars = get_template_variables(dedented_template)\n\n            for var in template_vars:\n                if not var.startswith(\"self.\"):\n                    raise ValueError(\n                        \"The toolkit_tool method must use self. prefix in template variables \"\n                        \"when creating tools dynamically\"\n                    )\n\n                self_var = var[5:]\n\n                # Expecting pydantic model fields or class attribute and property\n                if self_var in cls.model_fields or hasattr(cls, self_var):\n                    continue\n                raise ValueError(\n                    f\"The toolkit_tool method template variable {var} is not found in the class\"\n                )\n\n            cls._toolkit_tool_methods.append(\n                ToolKitToolMethod(attr, template_vars, dedented_template)\n            )\n        if not cls._toolkit_tool_methods:\n            raise ValueError(\"No toolkit_tool method found\")\n</code></pre>"},{"location":"api/core/#mirascope.core.BaseToolKit.create_tools","title":"<code>create_tools()</code>","text":"<p>The method to create the tools.</p> Source code in <code>mirascope/core/base/toolkit.py</code> <pre><code>def create_tools(self) -&gt; list[type[BaseTool]]:\n    \"\"\"The method to create the tools.\"\"\"\n    return [\n        convert_function_to_base_tool(\n            method, BaseTool, template.format(self=self), self.__namespace__\n        )\n        for method, template_vars, template in self._toolkit_tool_methods\n    ]\n</code></pre>"},{"location":"api/core/#mirascope.core.prompt_template","title":"<code>prompt_template(template)</code>","text":"<p>A decorator for setting the <code>prompt_template</code> of a <code>BasePrompt</code> or <code>call</code>.</p> Source code in <code>mirascope/core/base/prompt.py</code> <pre><code>def prompt_template(template: str):\n    \"\"\"A decorator for setting the `prompt_template` of a `BasePrompt` or `call`.\"\"\"\n\n    @overload\n    def inner(prompt: type[_BasePromptT]) -&gt; type[_BasePromptT]: ...  # pragma: no cover\n\n    @overload\n    def inner(prompt: Callable[_P, _R]) -&gt; Callable[_P, _R]: ...  # pragma: no cover\n\n    def inner(\n        prompt: type[_BasePromptT] | Callable[_P, _R],\n    ) -&gt; type[_BasePromptT] | Callable[_P, _R]:\n        prompt.__annotations__[\"prompt_template\"] = template\n        return prompt\n\n    return inner\n</code></pre>"},{"location":"api/core/anthropic/","title":"mirascope.core.anthropic","text":"<p>The Mirascope Anthropic Module.</p>"},{"location":"api/core/anthropic/#mirascope.core.anthropic.AnthropicDynamicConfig","title":"<code>AnthropicDynamicConfig = BaseDynamicConfig[MessageParam, AnthropicCallParams]</code>  <code>module-attribute</code>","text":"<p>The function return type for functions wrapped with the <code>anthropic_call</code> decorator.</p> <p>Attributes:</p> Name Type Description <code>computed_fields</code> <p>The computed fields to use in the prompt template.</p> <code>messages</code> <p>The messages to send to the Anthropic API. If provided, <code>computed_fields</code> will be ignored.</p> <code>call_params</code> <p>The call parameters to use when calling the Anthropic API. These will override any call parameters provided to the decorator.</p> <p>Example:</p> <pre><code>from mirascope.core import anthropic\n\n@anthropic.call(model=\"gpt-4o\")\ndef recommend_book(genre: str) -&gt; anthropic.AnthropicDynamicConfig:\n    \"\"\"Recommend a {capitalized_genre} book.\"\"\"\n    return {\"computed_fields\": {\"capitalized_genre\": genre.capitalize()}}\n</code></pre>"},{"location":"api/core/anthropic/#mirascope.core.anthropic.anthropic_call","title":"<code>anthropic_call = call_factory(TCallResponse=AnthropicCallResponse, TCallResponseChunk=AnthropicCallResponseChunk, TDynamicConfig=AnthropicDynamicConfig, TStream=AnthropicStream, TMessageParamType=MessageParam, TToolType=AnthropicTool, TCallParams=AnthropicCallParams, default_call_params=AnthropicCallParams(max_tokens=1000), setup_call=setup_call, get_json_output=get_json_output, handle_stream=handle_stream, handle_stream_async=handle_stream_async, calculate_cost=calculate_cost, provider='anthropic')</code>  <code>module-attribute</code>","text":"<p>A decorator for calling the Anthropic API with a typed function.</p> <p>This decorator is used to wrap a typed function that calls the Anthropic API. It parses the docstring of the wrapped function as the messages array and templates the input arguments for the function into each message's template.</p> <p>Example:</p> <pre><code>@anthropic_call(model=\"claude-3-5-sonnet-20240620\")\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\nresponse = recommend_book(\"fantasy\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>The Anthropic model to use in the API call.</p> required <code>stream</code> <p>Whether to stream the response from the API call.</p> required <code>tools</code> <p>The tools to use in the Anthropic API call.</p> required <code>**call_params</code> <p>The <code>AnthropicCallParams</code> call parameters to use in the API call.</p> required <p>Returns:</p> Type Description <p>The decorator for turning a typed function into an Anthropic API call.</p>"},{"location":"api/core/anthropic/#mirascope.core.anthropic.AnthropicCallParams","title":"<code>AnthropicCallParams</code>","text":"<p>               Bases: <code>BaseCallParams</code></p> <p>The parameters to use when calling the Anthropic API.</p> Source code in <code>mirascope/core/anthropic/call_params.py</code> <pre><code>class AnthropicCallParams(BaseCallParams):\n    \"\"\"The parameters to use when calling the Anthropic API.\"\"\"\n\n    max_tokens: Required[int]\n    tool_choice: NotRequired[ToolChoice | None]\n    metadata: NotRequired[Metadata | None]\n    stop_sequences: NotRequired[list[str] | None]\n    system: NotRequired[str | None]\n    temperature: NotRequired[float | None]\n    top_k: NotRequired[int | None]\n    top_p: NotRequired[float | None]\n    timeout: NotRequired[float | Timeout | None]\n</code></pre>"},{"location":"api/core/anthropic/#mirascope.core.anthropic.AnthropicCallResponse","title":"<code>AnthropicCallResponse</code>","text":"<p>               Bases: <code>BaseCallResponse[Message, AnthropicTool, AnthropicDynamicConfig, MessageParam, AnthropicCallParams, MessageParam]</code></p> <p>A convenience wrapper around the Anthropic <code>Message</code> response.</p> <p>When calling the Anthropic API using a function decorated with <code>anthropic_call</code>, the response will be an <code>AnthropicCallResponse</code> instance with properties that allow for more convenience access to commonly used attributes.</p> <p>Example:</p> <pre><code>from mirascope.core.anthropic import anthropic_call\n\n@anthropic_call(model=\"claude-3-5-sonnet-20240620\")\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\nresponse = recommend_book(\"fantasy\")  # response is an `AnthropicCallResponse` instance\nprint(response.content)\n#&gt; Sure! I would recommend...\n</code></pre> Source code in <code>mirascope/core/anthropic/call_response.py</code> <pre><code>class AnthropicCallResponse(\n    BaseCallResponse[\n        Message,\n        AnthropicTool,\n        AnthropicDynamicConfig,\n        MessageParam,\n        AnthropicCallParams,\n        MessageParam,\n    ]\n):\n    '''A convenience wrapper around the Anthropic `Message` response.\n\n    When calling the Anthropic API using a function decorated with `anthropic_call`, the\n    response will be an `AnthropicCallResponse` instance with properties that allow for\n    more convenience access to commonly used attributes.\n\n    Example:\n\n    ```python\n    from mirascope.core.anthropic import anthropic_call\n\n    @anthropic_call(model=\"claude-3-5-sonnet-20240620\")\n    def recommend_book(genre: str):\n        \"\"\"Recommend a {genre} book.\"\"\"\n\n    response = recommend_book(\"fantasy\")  # response is an `AnthropicCallResponse` instance\n    print(response.content)\n    #&gt; Sure! I would recommend...\n    ```\n    '''\n\n    _provider = \"anthropic\"\n\n    @computed_field\n    @property\n    def message_param(self) -&gt; MessageParam:\n        \"\"\"Returns the assistants's response as a message parameter.\"\"\"\n        return self.response.model_dump(include={\"content\", \"role\"})  # type: ignore\n\n    @property\n    def content(self) -&gt; str:\n        \"\"\"Returns the string text of the 0th text block.\"\"\"\n        block = self.response.content[0]\n        return block.text if block.type == \"text\" else \"\"\n\n    @property\n    def model(self) -&gt; str:\n        \"\"\"Returns the name of the response model.\"\"\"\n        return self.response.model\n\n    @property\n    def id(self) -&gt; str:\n        \"\"\"Returns the id of the response.\"\"\"\n        return self.response.id\n\n    @property\n    def finish_reasons(self) -&gt; list[str]:\n        \"\"\"Returns the finish reasons of the response.\"\"\"\n        return [str(self.response.stop_reason)]\n\n    @computed_field\n    @property\n    def tools(self) -&gt; list[AnthropicTool] | None:\n        \"\"\"Returns any available tool calls as their `AnthropicTool` definition.\n\n        Raises:\n            ValidationError: if a tool call doesn't match the tool's schema.\n        \"\"\"\n        if not self.tool_types:\n            return None\n\n        extracted_tools = []\n        for tool_call in self.response.content:\n            if tool_call.type != \"tool_use\":\n                continue\n            for tool_type in self.tool_types:\n                if tool_call.name == tool_type._name():\n                    tool = tool_type.from_tool_call(tool_call)\n                    extracted_tools.append(tool)\n                    break\n\n        return extracted_tools\n\n    @computed_field\n    @property\n    def tool(self) -&gt; AnthropicTool | None:\n        \"\"\"Returns the 0th tool for the 0th choice message.\n\n        Raises:\n            ValidationError: if the tool call doesn't match the tool's schema.\n        \"\"\"\n        if tools := self.tools:\n            return tools[0]\n        return None\n\n    @classmethod\n    def tool_message_params(\n        cls, tools_and_outputs: list[tuple[AnthropicTool, str]]\n    ) -&gt; list[MessageParam]:\n        \"\"\"Returns the tool message parameters for tool call results.\"\"\"\n        return [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    ToolResultBlockParam(\n                        tool_use_id=tool.tool_call.id,\n                        type=\"tool_result\",\n                        content=[{\"text\": output, \"type\": \"text\"}],\n                    )\n                    for tool, output in tools_and_outputs\n                ],\n            }\n        ]\n\n    @property\n    def usage(self) -&gt; Usage:\n        \"\"\"Returns the usage of the message.\"\"\"\n        return self.response.usage\n\n    @property\n    def input_tokens(self) -&gt; int:\n        \"\"\"Returns the number of input tokens.\"\"\"\n        return self.usage.input_tokens\n\n    @property\n    def output_tokens(self) -&gt; int:\n        \"\"\"Returns the number of output tokens.\"\"\"\n        return self.usage.output_tokens\n\n    def __str__(self) -&gt; str:\n        \"\"\"Returns the string content of the response.\"\"\"\n        return self.content\n</code></pre>"},{"location":"api/core/anthropic/#mirascope.core.anthropic.AnthropicCallResponse.content","title":"<code>content: str</code>  <code>property</code>","text":"<p>Returns the string text of the 0th text block.</p>"},{"location":"api/core/anthropic/#mirascope.core.anthropic.AnthropicCallResponse.finish_reasons","title":"<code>finish_reasons: list[str]</code>  <code>property</code>","text":"<p>Returns the finish reasons of the response.</p>"},{"location":"api/core/anthropic/#mirascope.core.anthropic.AnthropicCallResponse.id","title":"<code>id: str</code>  <code>property</code>","text":"<p>Returns the id of the response.</p>"},{"location":"api/core/anthropic/#mirascope.core.anthropic.AnthropicCallResponse.input_tokens","title":"<code>input_tokens: int</code>  <code>property</code>","text":"<p>Returns the number of input tokens.</p>"},{"location":"api/core/anthropic/#mirascope.core.anthropic.AnthropicCallResponse.message_param","title":"<code>message_param: MessageParam</code>  <code>property</code>","text":"<p>Returns the assistants's response as a message parameter.</p>"},{"location":"api/core/anthropic/#mirascope.core.anthropic.AnthropicCallResponse.model","title":"<code>model: str</code>  <code>property</code>","text":"<p>Returns the name of the response model.</p>"},{"location":"api/core/anthropic/#mirascope.core.anthropic.AnthropicCallResponse.output_tokens","title":"<code>output_tokens: int</code>  <code>property</code>","text":"<p>Returns the number of output tokens.</p>"},{"location":"api/core/anthropic/#mirascope.core.anthropic.AnthropicCallResponse.tool","title":"<code>tool: AnthropicTool | None</code>  <code>property</code>","text":"<p>Returns the 0th tool for the 0th choice message.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if the tool call doesn't match the tool's schema.</p>"},{"location":"api/core/anthropic/#mirascope.core.anthropic.AnthropicCallResponse.tools","title":"<code>tools: list[AnthropicTool] | None</code>  <code>property</code>","text":"<p>Returns any available tool calls as their <code>AnthropicTool</code> definition.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if a tool call doesn't match the tool's schema.</p>"},{"location":"api/core/anthropic/#mirascope.core.anthropic.AnthropicCallResponse.usage","title":"<code>usage: Usage</code>  <code>property</code>","text":"<p>Returns the usage of the message.</p>"},{"location":"api/core/anthropic/#mirascope.core.anthropic.AnthropicCallResponse.__str__","title":"<code>__str__()</code>","text":"<p>Returns the string content of the response.</p> Source code in <code>mirascope/core/anthropic/call_response.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Returns the string content of the response.\"\"\"\n    return self.content\n</code></pre>"},{"location":"api/core/anthropic/#mirascope.core.anthropic.AnthropicCallResponse.tool_message_params","title":"<code>tool_message_params(tools_and_outputs)</code>  <code>classmethod</code>","text":"<p>Returns the tool message parameters for tool call results.</p> Source code in <code>mirascope/core/anthropic/call_response.py</code> <pre><code>@classmethod\ndef tool_message_params(\n    cls, tools_and_outputs: list[tuple[AnthropicTool, str]]\n) -&gt; list[MessageParam]:\n    \"\"\"Returns the tool message parameters for tool call results.\"\"\"\n    return [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                ToolResultBlockParam(\n                    tool_use_id=tool.tool_call.id,\n                    type=\"tool_result\",\n                    content=[{\"text\": output, \"type\": \"text\"}],\n                )\n                for tool, output in tools_and_outputs\n            ],\n        }\n    ]\n</code></pre>"},{"location":"api/core/anthropic/#mirascope.core.anthropic.AnthropicCallResponseChunk","title":"<code>AnthropicCallResponseChunk</code>","text":"<p>               Bases: <code>BaseCallResponseChunk[MessageStreamEvent]</code></p> <p>A convenience wrapper around the Anthropic <code>ChatCompletionChunk</code> streamed chunks.</p> <p>When calling the Anthropic API using a function decorated with <code>anthropic_call</code> and <code>stream</code> set to <code>True</code>, the stream will contain <code>AnthropicResponseChunk</code> instances with properties that allow for more convenient access to commonly used attributes.</p> <p>Example:</p> <pre><code>from mirascope.core.anthropic import anthropic_call\n\n@anthropic_call(model=\"claude-3-5-sonnet-20240620\", stream=True)\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\nstream = recommend_book(\"fantasy\")  # response is an `AnthropicStream`\nfor chunk in stream:\n    print(chunk.content, end=\"\", flush=True)\n#&gt; Sure! I would recommend...\n</code></pre> Source code in <code>mirascope/core/anthropic/call_response_chunk.py</code> <pre><code>class AnthropicCallResponseChunk(BaseCallResponseChunk[MessageStreamEvent]):\n    '''A convenience wrapper around the Anthropic `ChatCompletionChunk` streamed chunks.\n\n    When calling the Anthropic API using a function decorated with `anthropic_call` and\n    `stream` set to `True`, the stream will contain `AnthropicResponseChunk` instances\n    with properties that allow for more convenient access to commonly used attributes.\n\n    Example:\n\n    ```python\n    from mirascope.core.anthropic import anthropic_call\n\n    @anthropic_call(model=\"claude-3-5-sonnet-20240620\", stream=True)\n    def recommend_book(genre: str):\n        \"\"\"Recommend a {genre} book.\"\"\"\n\n    stream = recommend_book(\"fantasy\")  # response is an `AnthropicStream`\n    for chunk in stream:\n        print(chunk.content, end=\"\", flush=True)\n    #&gt; Sure! I would recommend...\n    ```\n    '''\n\n    @property\n    def content(self) -&gt; str:\n        \"\"\"Returns the string content of the 0th message.\"\"\"\n        return (\n            self.chunk.delta.text\n            if self.chunk.type == \"content_block_delta\"\n            and self.chunk.delta.type == \"text_delta\"\n            else \"\"\n        )\n\n    @property\n    def model(self) -&gt; str | None:\n        \"\"\"Returns the name of the response model.\"\"\"\n        if isinstance(self.chunk, MessageStartEvent):\n            return self.chunk.message.model\n        return None\n\n    @property\n    def id(self) -&gt; str | None:\n        \"\"\"Returns the id of the response.\"\"\"\n        if isinstance(self.chunk, MessageStartEvent):\n            return self.chunk.message.id\n        return None\n\n    @property\n    def finish_reasons(self) -&gt; list[str] | None:\n        \"\"\"Returns the finish reason of the response.\"\"\"\n        if isinstance(self.chunk, RawMessageStartEvent):\n            return [str(self.chunk.message.stop_reason)]\n        return None\n\n    @property\n    def usage(self) -&gt; Usage | MessageDeltaUsage | None:\n        \"\"\"Returns the usage of the message.\"\"\"\n        if isinstance(self.chunk, MessageStartEvent):\n            return self.chunk.message.usage\n        elif isinstance(self.chunk, RawMessageDeltaEvent):\n            return self.chunk.usage\n        return None\n\n    @property\n    def input_tokens(self) -&gt; int | None:\n        \"\"\"Returns the number of input tokens.\"\"\"\n        if (usage := self.usage) and hasattr(usage, \"input_tokens\"):\n            return getattr(usage, \"input_tokens\")\n        return None\n\n    @property\n    def output_tokens(self) -&gt; int | None:\n        \"\"\"Returns the number of output tokens.\"\"\"\n        if self.usage:\n            return self.usage.output_tokens\n        return None\n\n    def __str__(self) -&gt; str:\n        \"\"\"Returns the string content of the chunk.\"\"\"\n        return self.content\n</code></pre>"},{"location":"api/core/anthropic/#mirascope.core.anthropic.AnthropicCallResponseChunk.content","title":"<code>content: str</code>  <code>property</code>","text":"<p>Returns the string content of the 0th message.</p>"},{"location":"api/core/anthropic/#mirascope.core.anthropic.AnthropicCallResponseChunk.finish_reasons","title":"<code>finish_reasons: list[str] | None</code>  <code>property</code>","text":"<p>Returns the finish reason of the response.</p>"},{"location":"api/core/anthropic/#mirascope.core.anthropic.AnthropicCallResponseChunk.id","title":"<code>id: str | None</code>  <code>property</code>","text":"<p>Returns the id of the response.</p>"},{"location":"api/core/anthropic/#mirascope.core.anthropic.AnthropicCallResponseChunk.input_tokens","title":"<code>input_tokens: int | None</code>  <code>property</code>","text":"<p>Returns the number of input tokens.</p>"},{"location":"api/core/anthropic/#mirascope.core.anthropic.AnthropicCallResponseChunk.model","title":"<code>model: str | None</code>  <code>property</code>","text":"<p>Returns the name of the response model.</p>"},{"location":"api/core/anthropic/#mirascope.core.anthropic.AnthropicCallResponseChunk.output_tokens","title":"<code>output_tokens: int | None</code>  <code>property</code>","text":"<p>Returns the number of output tokens.</p>"},{"location":"api/core/anthropic/#mirascope.core.anthropic.AnthropicCallResponseChunk.usage","title":"<code>usage: Usage | MessageDeltaUsage | None</code>  <code>property</code>","text":"<p>Returns the usage of the message.</p>"},{"location":"api/core/anthropic/#mirascope.core.anthropic.AnthropicCallResponseChunk.__str__","title":"<code>__str__()</code>","text":"<p>Returns the string content of the chunk.</p> Source code in <code>mirascope/core/anthropic/call_response_chunk.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Returns the string content of the chunk.\"\"\"\n    return self.content\n</code></pre>"},{"location":"api/core/anthropic/#mirascope.core.anthropic.AnthropicTool","title":"<code>AnthropicTool</code>","text":"<p>               Bases: <code>BaseTool</code></p> <p>A class for defining tools for Anthropic LLM calls.</p> Source code in <code>mirascope/core/anthropic/tool.py</code> <pre><code>class AnthropicTool(BaseTool):\n    \"\"\"A class for defining tools for Anthropic LLM calls.\"\"\"\n\n    tool_call: ToolUseBlock\n\n    @classmethod\n    def tool_schema(cls) -&gt; ToolParam:\n        \"\"\"Constructs a `ToolParam` tool schema from the `BaseModel` schema defined.\"\"\"\n        model_schema = cls.model_json_schema()\n        model_schema.pop(\"title\", None)\n        model_schema.pop(\"description\", None)\n\n        return ToolParam(\n            input_schema=model_schema,\n            name=cls._name(),\n            description=cls._description(),\n        )\n\n    @classmethod\n    def from_tool_call(cls, tool_call: ToolUseBlock) -&gt; AnthropicTool:\n        \"\"\"Constructs an `AnthropicTool` instance from a `tool_call`.\"\"\"\n        model_json = copy.deepcopy(tool_call.input)\n        model_json[\"tool_call\"] = tool_call.model_dump()  # type: ignore\n        return cls.model_validate(model_json)\n</code></pre>"},{"location":"api/core/anthropic/#mirascope.core.anthropic.AnthropicTool.from_tool_call","title":"<code>from_tool_call(tool_call)</code>  <code>classmethod</code>","text":"<p>Constructs an <code>AnthropicTool</code> instance from a <code>tool_call</code>.</p> Source code in <code>mirascope/core/anthropic/tool.py</code> <pre><code>@classmethod\ndef from_tool_call(cls, tool_call: ToolUseBlock) -&gt; AnthropicTool:\n    \"\"\"Constructs an `AnthropicTool` instance from a `tool_call`.\"\"\"\n    model_json = copy.deepcopy(tool_call.input)\n    model_json[\"tool_call\"] = tool_call.model_dump()  # type: ignore\n    return cls.model_validate(model_json)\n</code></pre>"},{"location":"api/core/anthropic/#mirascope.core.anthropic.AnthropicTool.tool_schema","title":"<code>tool_schema()</code>  <code>classmethod</code>","text":"<p>Constructs a <code>ToolParam</code> tool schema from the <code>BaseModel</code> schema defined.</p> Source code in <code>mirascope/core/anthropic/tool.py</code> <pre><code>@classmethod\ndef tool_schema(cls) -&gt; ToolParam:\n    \"\"\"Constructs a `ToolParam` tool schema from the `BaseModel` schema defined.\"\"\"\n    model_schema = cls.model_json_schema()\n    model_schema.pop(\"title\", None)\n    model_schema.pop(\"description\", None)\n\n    return ToolParam(\n        input_schema=model_schema,\n        name=cls._name(),\n        description=cls._description(),\n    )\n</code></pre>"},{"location":"api/core/anthropic/call/","title":"mirascope.core.anthropic.call","text":"<p>The <code>anthropic_call</code> decorator for functions as LLM calls.</p>"},{"location":"api/core/anthropic/call/#mirascope.core.anthropic.call.anthropic_call","title":"<code>anthropic_call = call_factory(TCallResponse=AnthropicCallResponse, TCallResponseChunk=AnthropicCallResponseChunk, TDynamicConfig=AnthropicDynamicConfig, TStream=AnthropicStream, TMessageParamType=MessageParam, TToolType=AnthropicTool, TCallParams=AnthropicCallParams, default_call_params=AnthropicCallParams(max_tokens=1000), setup_call=setup_call, get_json_output=get_json_output, handle_stream=handle_stream, handle_stream_async=handle_stream_async, calculate_cost=calculate_cost, provider='anthropic')</code>  <code>module-attribute</code>","text":"<p>A decorator for calling the Anthropic API with a typed function.</p> <p>This decorator is used to wrap a typed function that calls the Anthropic API. It parses the docstring of the wrapped function as the messages array and templates the input arguments for the function into each message's template.</p> <p>Example:</p> <pre><code>@anthropic_call(model=\"claude-3-5-sonnet-20240620\")\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\nresponse = recommend_book(\"fantasy\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>The Anthropic model to use in the API call.</p> required <code>stream</code> <p>Whether to stream the response from the API call.</p> required <code>tools</code> <p>The tools to use in the Anthropic API call.</p> required <code>**call_params</code> <p>The <code>AnthropicCallParams</code> call parameters to use in the API call.</p> required <p>Returns:</p> Type Description <p>The decorator for turning a typed function into an Anthropic API call.</p>"},{"location":"api/core/anthropic/call_params/","title":"mirascope.core.anthropic.call_params","text":"<p>The parameters to use when calling the Anthropic API.</p>"},{"location":"api/core/anthropic/call_params/#mirascope.core.anthropic.call_params.AnthropicCallParams","title":"<code>AnthropicCallParams</code>","text":"<p>               Bases: <code>BaseCallParams</code></p> <p>The parameters to use when calling the Anthropic API.</p> Source code in <code>mirascope/core/anthropic/call_params.py</code> <pre><code>class AnthropicCallParams(BaseCallParams):\n    \"\"\"The parameters to use when calling the Anthropic API.\"\"\"\n\n    max_tokens: Required[int]\n    tool_choice: NotRequired[ToolChoice | None]\n    metadata: NotRequired[Metadata | None]\n    stop_sequences: NotRequired[list[str] | None]\n    system: NotRequired[str | None]\n    temperature: NotRequired[float | None]\n    top_k: NotRequired[int | None]\n    top_p: NotRequired[float | None]\n    timeout: NotRequired[float | Timeout | None]\n</code></pre>"},{"location":"api/core/anthropic/call_response/","title":"mirascope.core.anthropic.call_response","text":"<p>This module contains the <code>AnthropicCallResponse</code> class.</p>"},{"location":"api/core/anthropic/call_response/#mirascope.core.anthropic.call_response.AnthropicCallResponse","title":"<code>AnthropicCallResponse</code>","text":"<p>               Bases: <code>BaseCallResponse[Message, AnthropicTool, AnthropicDynamicConfig, MessageParam, AnthropicCallParams, MessageParam]</code></p> <p>A convenience wrapper around the Anthropic <code>Message</code> response.</p> <p>When calling the Anthropic API using a function decorated with <code>anthropic_call</code>, the response will be an <code>AnthropicCallResponse</code> instance with properties that allow for more convenience access to commonly used attributes.</p> <p>Example:</p> <pre><code>from mirascope.core.anthropic import anthropic_call\n\n@anthropic_call(model=\"claude-3-5-sonnet-20240620\")\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\nresponse = recommend_book(\"fantasy\")  # response is an `AnthropicCallResponse` instance\nprint(response.content)\n#&gt; Sure! I would recommend...\n</code></pre> Source code in <code>mirascope/core/anthropic/call_response.py</code> <pre><code>class AnthropicCallResponse(\n    BaseCallResponse[\n        Message,\n        AnthropicTool,\n        AnthropicDynamicConfig,\n        MessageParam,\n        AnthropicCallParams,\n        MessageParam,\n    ]\n):\n    '''A convenience wrapper around the Anthropic `Message` response.\n\n    When calling the Anthropic API using a function decorated with `anthropic_call`, the\n    response will be an `AnthropicCallResponse` instance with properties that allow for\n    more convenience access to commonly used attributes.\n\n    Example:\n\n    ```python\n    from mirascope.core.anthropic import anthropic_call\n\n    @anthropic_call(model=\"claude-3-5-sonnet-20240620\")\n    def recommend_book(genre: str):\n        \"\"\"Recommend a {genre} book.\"\"\"\n\n    response = recommend_book(\"fantasy\")  # response is an `AnthropicCallResponse` instance\n    print(response.content)\n    #&gt; Sure! I would recommend...\n    ```\n    '''\n\n    _provider = \"anthropic\"\n\n    @computed_field\n    @property\n    def message_param(self) -&gt; MessageParam:\n        \"\"\"Returns the assistants's response as a message parameter.\"\"\"\n        return self.response.model_dump(include={\"content\", \"role\"})  # type: ignore\n\n    @property\n    def content(self) -&gt; str:\n        \"\"\"Returns the string text of the 0th text block.\"\"\"\n        block = self.response.content[0]\n        return block.text if block.type == \"text\" else \"\"\n\n    @property\n    def model(self) -&gt; str:\n        \"\"\"Returns the name of the response model.\"\"\"\n        return self.response.model\n\n    @property\n    def id(self) -&gt; str:\n        \"\"\"Returns the id of the response.\"\"\"\n        return self.response.id\n\n    @property\n    def finish_reasons(self) -&gt; list[str]:\n        \"\"\"Returns the finish reasons of the response.\"\"\"\n        return [str(self.response.stop_reason)]\n\n    @computed_field\n    @property\n    def tools(self) -&gt; list[AnthropicTool] | None:\n        \"\"\"Returns any available tool calls as their `AnthropicTool` definition.\n\n        Raises:\n            ValidationError: if a tool call doesn't match the tool's schema.\n        \"\"\"\n        if not self.tool_types:\n            return None\n\n        extracted_tools = []\n        for tool_call in self.response.content:\n            if tool_call.type != \"tool_use\":\n                continue\n            for tool_type in self.tool_types:\n                if tool_call.name == tool_type._name():\n                    tool = tool_type.from_tool_call(tool_call)\n                    extracted_tools.append(tool)\n                    break\n\n        return extracted_tools\n\n    @computed_field\n    @property\n    def tool(self) -&gt; AnthropicTool | None:\n        \"\"\"Returns the 0th tool for the 0th choice message.\n\n        Raises:\n            ValidationError: if the tool call doesn't match the tool's schema.\n        \"\"\"\n        if tools := self.tools:\n            return tools[0]\n        return None\n\n    @classmethod\n    def tool_message_params(\n        cls, tools_and_outputs: list[tuple[AnthropicTool, str]]\n    ) -&gt; list[MessageParam]:\n        \"\"\"Returns the tool message parameters for tool call results.\"\"\"\n        return [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    ToolResultBlockParam(\n                        tool_use_id=tool.tool_call.id,\n                        type=\"tool_result\",\n                        content=[{\"text\": output, \"type\": \"text\"}],\n                    )\n                    for tool, output in tools_and_outputs\n                ],\n            }\n        ]\n\n    @property\n    def usage(self) -&gt; Usage:\n        \"\"\"Returns the usage of the message.\"\"\"\n        return self.response.usage\n\n    @property\n    def input_tokens(self) -&gt; int:\n        \"\"\"Returns the number of input tokens.\"\"\"\n        return self.usage.input_tokens\n\n    @property\n    def output_tokens(self) -&gt; int:\n        \"\"\"Returns the number of output tokens.\"\"\"\n        return self.usage.output_tokens\n\n    def __str__(self) -&gt; str:\n        \"\"\"Returns the string content of the response.\"\"\"\n        return self.content\n</code></pre>"},{"location":"api/core/anthropic/call_response/#mirascope.core.anthropic.call_response.AnthropicCallResponse.content","title":"<code>content: str</code>  <code>property</code>","text":"<p>Returns the string text of the 0th text block.</p>"},{"location":"api/core/anthropic/call_response/#mirascope.core.anthropic.call_response.AnthropicCallResponse.finish_reasons","title":"<code>finish_reasons: list[str]</code>  <code>property</code>","text":"<p>Returns the finish reasons of the response.</p>"},{"location":"api/core/anthropic/call_response/#mirascope.core.anthropic.call_response.AnthropicCallResponse.id","title":"<code>id: str</code>  <code>property</code>","text":"<p>Returns the id of the response.</p>"},{"location":"api/core/anthropic/call_response/#mirascope.core.anthropic.call_response.AnthropicCallResponse.input_tokens","title":"<code>input_tokens: int</code>  <code>property</code>","text":"<p>Returns the number of input tokens.</p>"},{"location":"api/core/anthropic/call_response/#mirascope.core.anthropic.call_response.AnthropicCallResponse.message_param","title":"<code>message_param: MessageParam</code>  <code>property</code>","text":"<p>Returns the assistants's response as a message parameter.</p>"},{"location":"api/core/anthropic/call_response/#mirascope.core.anthropic.call_response.AnthropicCallResponse.model","title":"<code>model: str</code>  <code>property</code>","text":"<p>Returns the name of the response model.</p>"},{"location":"api/core/anthropic/call_response/#mirascope.core.anthropic.call_response.AnthropicCallResponse.output_tokens","title":"<code>output_tokens: int</code>  <code>property</code>","text":"<p>Returns the number of output tokens.</p>"},{"location":"api/core/anthropic/call_response/#mirascope.core.anthropic.call_response.AnthropicCallResponse.tool","title":"<code>tool: AnthropicTool | None</code>  <code>property</code>","text":"<p>Returns the 0th tool for the 0th choice message.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if the tool call doesn't match the tool's schema.</p>"},{"location":"api/core/anthropic/call_response/#mirascope.core.anthropic.call_response.AnthropicCallResponse.tools","title":"<code>tools: list[AnthropicTool] | None</code>  <code>property</code>","text":"<p>Returns any available tool calls as their <code>AnthropicTool</code> definition.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if a tool call doesn't match the tool's schema.</p>"},{"location":"api/core/anthropic/call_response/#mirascope.core.anthropic.call_response.AnthropicCallResponse.usage","title":"<code>usage: Usage</code>  <code>property</code>","text":"<p>Returns the usage of the message.</p>"},{"location":"api/core/anthropic/call_response/#mirascope.core.anthropic.call_response.AnthropicCallResponse.__str__","title":"<code>__str__()</code>","text":"<p>Returns the string content of the response.</p> Source code in <code>mirascope/core/anthropic/call_response.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Returns the string content of the response.\"\"\"\n    return self.content\n</code></pre>"},{"location":"api/core/anthropic/call_response/#mirascope.core.anthropic.call_response.AnthropicCallResponse.tool_message_params","title":"<code>tool_message_params(tools_and_outputs)</code>  <code>classmethod</code>","text":"<p>Returns the tool message parameters for tool call results.</p> Source code in <code>mirascope/core/anthropic/call_response.py</code> <pre><code>@classmethod\ndef tool_message_params(\n    cls, tools_and_outputs: list[tuple[AnthropicTool, str]]\n) -&gt; list[MessageParam]:\n    \"\"\"Returns the tool message parameters for tool call results.\"\"\"\n    return [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                ToolResultBlockParam(\n                    tool_use_id=tool.tool_call.id,\n                    type=\"tool_result\",\n                    content=[{\"text\": output, \"type\": \"text\"}],\n                )\n                for tool, output in tools_and_outputs\n            ],\n        }\n    ]\n</code></pre>"},{"location":"api/core/anthropic/call_response_chunk/","title":"mirascope.core.anthropic.call_response_chunk","text":"<p>This module contains the <code>AnthropicCallResponseChunk</code> class.</p>"},{"location":"api/core/anthropic/call_response_chunk/#mirascope.core.anthropic.call_response_chunk.AnthropicCallResponseChunk","title":"<code>AnthropicCallResponseChunk</code>","text":"<p>               Bases: <code>BaseCallResponseChunk[MessageStreamEvent]</code></p> <p>A convenience wrapper around the Anthropic <code>ChatCompletionChunk</code> streamed chunks.</p> <p>When calling the Anthropic API using a function decorated with <code>anthropic_call</code> and <code>stream</code> set to <code>True</code>, the stream will contain <code>AnthropicResponseChunk</code> instances with properties that allow for more convenient access to commonly used attributes.</p> <p>Example:</p> <pre><code>from mirascope.core.anthropic import anthropic_call\n\n@anthropic_call(model=\"claude-3-5-sonnet-20240620\", stream=True)\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\nstream = recommend_book(\"fantasy\")  # response is an `AnthropicStream`\nfor chunk in stream:\n    print(chunk.content, end=\"\", flush=True)\n#&gt; Sure! I would recommend...\n</code></pre> Source code in <code>mirascope/core/anthropic/call_response_chunk.py</code> <pre><code>class AnthropicCallResponseChunk(BaseCallResponseChunk[MessageStreamEvent]):\n    '''A convenience wrapper around the Anthropic `ChatCompletionChunk` streamed chunks.\n\n    When calling the Anthropic API using a function decorated with `anthropic_call` and\n    `stream` set to `True`, the stream will contain `AnthropicResponseChunk` instances\n    with properties that allow for more convenient access to commonly used attributes.\n\n    Example:\n\n    ```python\n    from mirascope.core.anthropic import anthropic_call\n\n    @anthropic_call(model=\"claude-3-5-sonnet-20240620\", stream=True)\n    def recommend_book(genre: str):\n        \"\"\"Recommend a {genre} book.\"\"\"\n\n    stream = recommend_book(\"fantasy\")  # response is an `AnthropicStream`\n    for chunk in stream:\n        print(chunk.content, end=\"\", flush=True)\n    #&gt; Sure! I would recommend...\n    ```\n    '''\n\n    @property\n    def content(self) -&gt; str:\n        \"\"\"Returns the string content of the 0th message.\"\"\"\n        return (\n            self.chunk.delta.text\n            if self.chunk.type == \"content_block_delta\"\n            and self.chunk.delta.type == \"text_delta\"\n            else \"\"\n        )\n\n    @property\n    def model(self) -&gt; str | None:\n        \"\"\"Returns the name of the response model.\"\"\"\n        if isinstance(self.chunk, MessageStartEvent):\n            return self.chunk.message.model\n        return None\n\n    @property\n    def id(self) -&gt; str | None:\n        \"\"\"Returns the id of the response.\"\"\"\n        if isinstance(self.chunk, MessageStartEvent):\n            return self.chunk.message.id\n        return None\n\n    @property\n    def finish_reasons(self) -&gt; list[str] | None:\n        \"\"\"Returns the finish reason of the response.\"\"\"\n        if isinstance(self.chunk, RawMessageStartEvent):\n            return [str(self.chunk.message.stop_reason)]\n        return None\n\n    @property\n    def usage(self) -&gt; Usage | MessageDeltaUsage | None:\n        \"\"\"Returns the usage of the message.\"\"\"\n        if isinstance(self.chunk, MessageStartEvent):\n            return self.chunk.message.usage\n        elif isinstance(self.chunk, RawMessageDeltaEvent):\n            return self.chunk.usage\n        return None\n\n    @property\n    def input_tokens(self) -&gt; int | None:\n        \"\"\"Returns the number of input tokens.\"\"\"\n        if (usage := self.usage) and hasattr(usage, \"input_tokens\"):\n            return getattr(usage, \"input_tokens\")\n        return None\n\n    @property\n    def output_tokens(self) -&gt; int | None:\n        \"\"\"Returns the number of output tokens.\"\"\"\n        if self.usage:\n            return self.usage.output_tokens\n        return None\n\n    def __str__(self) -&gt; str:\n        \"\"\"Returns the string content of the chunk.\"\"\"\n        return self.content\n</code></pre>"},{"location":"api/core/anthropic/call_response_chunk/#mirascope.core.anthropic.call_response_chunk.AnthropicCallResponseChunk.content","title":"<code>content: str</code>  <code>property</code>","text":"<p>Returns the string content of the 0th message.</p>"},{"location":"api/core/anthropic/call_response_chunk/#mirascope.core.anthropic.call_response_chunk.AnthropicCallResponseChunk.finish_reasons","title":"<code>finish_reasons: list[str] | None</code>  <code>property</code>","text":"<p>Returns the finish reason of the response.</p>"},{"location":"api/core/anthropic/call_response_chunk/#mirascope.core.anthropic.call_response_chunk.AnthropicCallResponseChunk.id","title":"<code>id: str | None</code>  <code>property</code>","text":"<p>Returns the id of the response.</p>"},{"location":"api/core/anthropic/call_response_chunk/#mirascope.core.anthropic.call_response_chunk.AnthropicCallResponseChunk.input_tokens","title":"<code>input_tokens: int | None</code>  <code>property</code>","text":"<p>Returns the number of input tokens.</p>"},{"location":"api/core/anthropic/call_response_chunk/#mirascope.core.anthropic.call_response_chunk.AnthropicCallResponseChunk.model","title":"<code>model: str | None</code>  <code>property</code>","text":"<p>Returns the name of the response model.</p>"},{"location":"api/core/anthropic/call_response_chunk/#mirascope.core.anthropic.call_response_chunk.AnthropicCallResponseChunk.output_tokens","title":"<code>output_tokens: int | None</code>  <code>property</code>","text":"<p>Returns the number of output tokens.</p>"},{"location":"api/core/anthropic/call_response_chunk/#mirascope.core.anthropic.call_response_chunk.AnthropicCallResponseChunk.usage","title":"<code>usage: Usage | MessageDeltaUsage | None</code>  <code>property</code>","text":"<p>Returns the usage of the message.</p>"},{"location":"api/core/anthropic/call_response_chunk/#mirascope.core.anthropic.call_response_chunk.AnthropicCallResponseChunk.__str__","title":"<code>__str__()</code>","text":"<p>Returns the string content of the chunk.</p> Source code in <code>mirascope/core/anthropic/call_response_chunk.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Returns the string content of the chunk.\"\"\"\n    return self.content\n</code></pre>"},{"location":"api/core/anthropic/dynamic_config/","title":"mirascope.core.anthropic.dynamic_config","text":"<p>This module defines the function return type for functions as LLM calls.</p>"},{"location":"api/core/anthropic/dynamic_config/#mirascope.core.anthropic.dynamic_config.AnthropicDynamicConfig","title":"<code>AnthropicDynamicConfig = BaseDynamicConfig[MessageParam, AnthropicCallParams]</code>  <code>module-attribute</code>","text":"<p>The function return type for functions wrapped with the <code>anthropic_call</code> decorator.</p> <p>Attributes:</p> Name Type Description <code>computed_fields</code> <p>The computed fields to use in the prompt template.</p> <code>messages</code> <p>The messages to send to the Anthropic API. If provided, <code>computed_fields</code> will be ignored.</p> <code>call_params</code> <p>The call parameters to use when calling the Anthropic API. These will override any call parameters provided to the decorator.</p> <p>Example:</p> <pre><code>from mirascope.core import anthropic\n\n@anthropic.call(model=\"gpt-4o\")\ndef recommend_book(genre: str) -&gt; anthropic.AnthropicDynamicConfig:\n    \"\"\"Recommend a {capitalized_genre} book.\"\"\"\n    return {\"computed_fields\": {\"capitalized_genre\": genre.capitalize()}}\n</code></pre>"},{"location":"api/core/anthropic/tool/","title":"mirascope.core.anthropic.tool","text":"<p>The <code>OpenAITool</code> class for easy tool usage with OpenAI LLM calls.</p>"},{"location":"api/core/anthropic/tool/#mirascope.core.anthropic.tool.AnthropicTool","title":"<code>AnthropicTool</code>","text":"<p>               Bases: <code>BaseTool</code></p> <p>A class for defining tools for Anthropic LLM calls.</p> Source code in <code>mirascope/core/anthropic/tool.py</code> <pre><code>class AnthropicTool(BaseTool):\n    \"\"\"A class for defining tools for Anthropic LLM calls.\"\"\"\n\n    tool_call: ToolUseBlock\n\n    @classmethod\n    def tool_schema(cls) -&gt; ToolParam:\n        \"\"\"Constructs a `ToolParam` tool schema from the `BaseModel` schema defined.\"\"\"\n        model_schema = cls.model_json_schema()\n        model_schema.pop(\"title\", None)\n        model_schema.pop(\"description\", None)\n\n        return ToolParam(\n            input_schema=model_schema,\n            name=cls._name(),\n            description=cls._description(),\n        )\n\n    @classmethod\n    def from_tool_call(cls, tool_call: ToolUseBlock) -&gt; AnthropicTool:\n        \"\"\"Constructs an `AnthropicTool` instance from a `tool_call`.\"\"\"\n        model_json = copy.deepcopy(tool_call.input)\n        model_json[\"tool_call\"] = tool_call.model_dump()  # type: ignore\n        return cls.model_validate(model_json)\n</code></pre>"},{"location":"api/core/anthropic/tool/#mirascope.core.anthropic.tool.AnthropicTool.from_tool_call","title":"<code>from_tool_call(tool_call)</code>  <code>classmethod</code>","text":"<p>Constructs an <code>AnthropicTool</code> instance from a <code>tool_call</code>.</p> Source code in <code>mirascope/core/anthropic/tool.py</code> <pre><code>@classmethod\ndef from_tool_call(cls, tool_call: ToolUseBlock) -&gt; AnthropicTool:\n    \"\"\"Constructs an `AnthropicTool` instance from a `tool_call`.\"\"\"\n    model_json = copy.deepcopy(tool_call.input)\n    model_json[\"tool_call\"] = tool_call.model_dump()  # type: ignore\n    return cls.model_validate(model_json)\n</code></pre>"},{"location":"api/core/anthropic/tool/#mirascope.core.anthropic.tool.AnthropicTool.tool_schema","title":"<code>tool_schema()</code>  <code>classmethod</code>","text":"<p>Constructs a <code>ToolParam</code> tool schema from the <code>BaseModel</code> schema defined.</p> Source code in <code>mirascope/core/anthropic/tool.py</code> <pre><code>@classmethod\ndef tool_schema(cls) -&gt; ToolParam:\n    \"\"\"Constructs a `ToolParam` tool schema from the `BaseModel` schema defined.\"\"\"\n    model_schema = cls.model_json_schema()\n    model_schema.pop(\"title\", None)\n    model_schema.pop(\"description\", None)\n\n    return ToolParam(\n        input_schema=model_schema,\n        name=cls._name(),\n        description=cls._description(),\n    )\n</code></pre>"},{"location":"api/core/base/","title":"mirascope.core.base","text":"<p>Mirascope Base Classes.</p>"},{"location":"api/core/base/#mirascope.core.base.BaseDynamicConfig","title":"<code>BaseDynamicConfig = DynamicConfigBase | DynamicConfigMessages[_MessageParamT] | DynamicConfigCallParams[_CallParamsT] | DynamicConfigFull[_MessageParamT, _CallParamsT] | None</code>  <code>module-attribute</code>","text":"<p>The base type in a function as an LLM call to return for dynamic configuration.</p>"},{"location":"api/core/base/#mirascope.core.base.BaseCallResponse","title":"<code>BaseCallResponse</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[_ResponseT, _BaseToolT, _BaseDynamicConfigT, _MessageParamT, _CallParamsT, _UserMessageParamT]</code>, <code>ABC</code></p> <p>A base abstract interface for LLM call responses.</p> <p>Attributes:</p> Name Type Description <code>response</code> <code>_ResponseT</code> <p>The original response from whichever model response this wraps.</p> <code>user_message_param</code> <code>_UserMessageParamT | None</code> <p>The most recent message if it was a user message. Otherwise <code>None</code>.</p> <code>tool_types</code> <code>list[type[_BaseToolT]] | None</code> <p>The tool types sent in the LLM call.</p> <code>start_time</code> <code>float</code> <p>The start time of the completion in ms.</p> <code>end_time</code> <code>float</code> <p>The end time of the completion in ms.</p> <code>cost</code> <code>float | None</code> <p>The cost of the completion in dollars.</p> Source code in <code>mirascope/core/base/call_response.py</code> <pre><code>class BaseCallResponse(\n    BaseModel,\n    Generic[\n        _ResponseT,\n        _BaseToolT,\n        _BaseDynamicConfigT,\n        _MessageParamT,\n        _CallParamsT,\n        _UserMessageParamT,\n    ],\n    ABC,\n):\n    \"\"\"A base abstract interface for LLM call responses.\n\n    Attributes:\n        response: The original response from whichever model response this wraps.\n        user_message_param: The most recent message if it was a user message. Otherwise\n            `None`.\n        tool_types: The tool types sent in the LLM call.\n        start_time: The start time of the completion in ms.\n        end_time: The end time of the completion in ms.\n        cost: The cost of the completion in dollars.\n    \"\"\"\n\n    metadata: Metadata\n    response: _ResponseT\n    tool_types: list[type[_BaseToolT]] | None = None\n    prompt_template: str | None\n    fn_args: dict[str, Any]\n    dynamic_config: _BaseDynamicConfigT\n    messages: SkipValidation[list[_MessageParamT]]\n    call_params: SkipValidation[_CallParamsT]\n    user_message_param: _UserMessageParamT | None = None\n    start_time: float\n    end_time: float\n    cost: float | None = None\n\n    _provider: ClassVar[str] = \"NO PROVIDER\"\n\n    model_config = ConfigDict(extra=\"allow\", arbitrary_types_allowed=True)\n\n    @computed_field\n    @property\n    @abstractmethod\n    def message_param(self) -&gt; Any:\n        \"\"\"Returns the assistant's response as a message parameter.\"\"\"\n        ...  # pragma: no cover\n\n    @computed_field\n    @property\n    @abstractmethod\n    def tools(self) -&gt; list[_BaseToolT] | None:\n        \"\"\"Returns the tools for the 0th choice message.\"\"\"\n        ...  # pragma: no cover\n\n    @property\n    @abstractmethod\n    def tool(self) -&gt; _BaseToolT | None:\n        \"\"\"Returns the 0th tool for the 0th choice message.\"\"\"\n        ...  # pragma: no cover\n\n    @classmethod\n    @abstractmethod\n    def tool_message_params(\n        cls, tools_and_outputs: list[tuple[_BaseToolT, Any]]\n    ) -&gt; list[Any]:\n        \"\"\"Returns the tool message parameters for tool call results.\"\"\"\n        ...  # pragma: no cover\n\n    @property\n    @abstractmethod\n    def content(self) -&gt; str:\n        \"\"\"Should return the string content of the response.\n\n        If there are multiple choices in a response, this method should select the 0th\n        choice and return it's string content.\n\n        If there is no string content (e.g. when using tools), this method must return\n        the empty string.\n        \"\"\"\n        ...  # pragma: no cover\n\n    @property\n    @abstractmethod\n    def finish_reasons(self) -&gt; list[str] | None:\n        \"\"\"Should return the finish reasons of the response.\n\n        If there is no finish reason, this method must return None.\n        \"\"\"\n        ...  # pragma: no cover\n\n    @property\n    @abstractmethod\n    def model(self) -&gt; str | None:\n        \"\"\"Should return the name of the response model.\"\"\"\n        ...  # pragma: no cover\n\n    @property\n    @abstractmethod\n    def id(self) -&gt; str | None:\n        \"\"\"Should return the id of the response.\"\"\"\n        ...  # pragma: no cover\n\n    @property\n    @abstractmethod\n    def usage(self) -&gt; Any:\n        \"\"\"Should return the usage of the response.\n\n        If there is no usage, this method must return None.\n        \"\"\"\n        ...  # pragma: no cover\n\n    @property\n    @abstractmethod\n    def input_tokens(self) -&gt; int | float | None:\n        \"\"\"Should return the number of input tokens.\n\n        If there is no input_tokens, this method must return None.\n        \"\"\"\n        ...  # pragma: no cover\n\n    @property\n    @abstractmethod\n    def output_tokens(self) -&gt; int | float | None:\n        \"\"\"Should return the number of output tokens.\n\n        If there is no output_tokens, this method must return None.\n        \"\"\"\n        ...  # pragma: no cover\n\n    @field_serializer(\"tool_types\")\n    def serialize_tool_types(self, tool_types: list[type[_BaseToolT]], _info):\n        return [{\"type\": \"function\", \"name\": tool._name()} for tool in tool_types or []]\n\n    def __str__(self) -&gt; str:\n        \"\"\"Returns the string content of the response.\"\"\"\n        return self.content\n</code></pre>"},{"location":"api/core/base/#mirascope.core.base.BaseCallResponse.content","title":"<code>content: str</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Should return the string content of the response.</p> <p>If there are multiple choices in a response, this method should select the 0th choice and return it's string content.</p> <p>If there is no string content (e.g. when using tools), this method must return the empty string.</p>"},{"location":"api/core/base/#mirascope.core.base.BaseCallResponse.finish_reasons","title":"<code>finish_reasons: list[str] | None</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Should return the finish reasons of the response.</p> <p>If there is no finish reason, this method must return None.</p>"},{"location":"api/core/base/#mirascope.core.base.BaseCallResponse.id","title":"<code>id: str | None</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Should return the id of the response.</p>"},{"location":"api/core/base/#mirascope.core.base.BaseCallResponse.input_tokens","title":"<code>input_tokens: int | float | None</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Should return the number of input tokens.</p> <p>If there is no input_tokens, this method must return None.</p>"},{"location":"api/core/base/#mirascope.core.base.BaseCallResponse.message_param","title":"<code>message_param: Any</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Returns the assistant's response as a message parameter.</p>"},{"location":"api/core/base/#mirascope.core.base.BaseCallResponse.model","title":"<code>model: str | None</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Should return the name of the response model.</p>"},{"location":"api/core/base/#mirascope.core.base.BaseCallResponse.output_tokens","title":"<code>output_tokens: int | float | None</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Should return the number of output tokens.</p> <p>If there is no output_tokens, this method must return None.</p>"},{"location":"api/core/base/#mirascope.core.base.BaseCallResponse.tool","title":"<code>tool: _BaseToolT | None</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Returns the 0th tool for the 0th choice message.</p>"},{"location":"api/core/base/#mirascope.core.base.BaseCallResponse.tools","title":"<code>tools: list[_BaseToolT] | None</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Returns the tools for the 0th choice message.</p>"},{"location":"api/core/base/#mirascope.core.base.BaseCallResponse.usage","title":"<code>usage: Any</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Should return the usage of the response.</p> <p>If there is no usage, this method must return None.</p>"},{"location":"api/core/base/#mirascope.core.base.BaseCallResponse.__str__","title":"<code>__str__()</code>","text":"<p>Returns the string content of the response.</p> Source code in <code>mirascope/core/base/call_response.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Returns the string content of the response.\"\"\"\n    return self.content\n</code></pre>"},{"location":"api/core/base/#mirascope.core.base.BaseCallResponse.tool_message_params","title":"<code>tool_message_params(tools_and_outputs)</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Returns the tool message parameters for tool call results.</p> Source code in <code>mirascope/core/base/call_response.py</code> <pre><code>@classmethod\n@abstractmethod\ndef tool_message_params(\n    cls, tools_and_outputs: list[tuple[_BaseToolT, Any]]\n) -&gt; list[Any]:\n    \"\"\"Returns the tool message parameters for tool call results.\"\"\"\n    ...  # pragma: no cover\n</code></pre>"},{"location":"api/core/base/#mirascope.core.base.BaseCallResponseChunk","title":"<code>BaseCallResponseChunk</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[_ChunkT]</code>, <code>ABC</code></p> <p>A base abstract interface for LLM streaming response chunks.</p> <p>Attributes:</p> Name Type Description <code>chunk</code> <code>_ChunkT</code> <p>The original response chunk from whichever model response this wraps.</p> <code>tool_types</code> <code>_ChunkT</code> <p>The tool types sent in the LLM call if any.</p> <code>user_message_param</code> <code>_ChunkT</code> <p>The most recent message if it was a user message. Otherwise <code>None</code>.</p> <code>cost</code> <code>_ChunkT</code> <p>The cost of the completion in dollars.</p> Source code in <code>mirascope/core/base/call_response_chunk.py</code> <pre><code>class BaseCallResponseChunk(BaseModel, Generic[_ChunkT], ABC):\n    \"\"\"A base abstract interface for LLM streaming response chunks.\n\n    Attributes:\n        chunk: The original response chunk from whichever model response this wraps.\n        tool_types: The tool types sent in the LLM call if any.\n        user_message_param: The most recent message if it was a user message. Otherwise\n            `None`.\n        cost: The cost of the completion in dollars.\n    \"\"\"\n\n    chunk: _ChunkT\n\n    model_config = ConfigDict(extra=\"allow\", arbitrary_types_allowed=True)\n\n    @property\n    @abstractmethod\n    def content(self) -&gt; str:\n        \"\"\"Should return the string content of the response chunk.\n\n        If there are multiple choices in a chunk, this method should select the 0th\n        choice and return it's string content.\n\n        If there is no string content (e.g. when using tools), this method must return\n        the empty string.\n        \"\"\"\n        ...  # pragma: no cover\n\n    @property\n    @abstractmethod\n    def model(self) -&gt; str | None:\n        \"\"\"Should return the name of the response model.\"\"\"\n        ...  # pragma: no cover\n\n    @property\n    @abstractmethod\n    def id(self) -&gt; str | None:\n        \"\"\"Should return the id of the response.\"\"\"\n        ...  # pragma: no cover\n\n    @property\n    @abstractmethod\n    def finish_reasons(self) -&gt; list[str] | None:\n        \"\"\"Should return the finish reasons of the response.\n\n        If there is no finish reason, this method must return None.\n        \"\"\"\n        ...  # pragma: no cover\n\n    @property\n    @abstractmethod\n    def usage(self) -&gt; Any:\n        \"\"\"Should return the usage of the response.\n\n        If there is no usage, this method must return None.\n        \"\"\"\n        ...  # pragma: no cover\n\n    @property\n    @abstractmethod\n    def input_tokens(self) -&gt; int | float | None:\n        \"\"\"Should return the number of input tokens.\n\n        If there is no input_tokens, this method must return None.\n        \"\"\"\n        ...  # pragma: no cover\n\n    @property\n    @abstractmethod\n    def output_tokens(self) -&gt; int | float | None:\n        \"\"\"Should return the number of output tokens.\n\n        If there is no output_tokens, this method must return None.\n        \"\"\"\n        ...  # pragma: no cover\n\n    def __str__(self) -&gt; str:\n        \"\"\"Returns the string content of the chunk.\"\"\"\n        return self.content\n</code></pre>"},{"location":"api/core/base/#mirascope.core.base.BaseCallResponseChunk.content","title":"<code>content: str</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Should return the string content of the response chunk.</p> <p>If there are multiple choices in a chunk, this method should select the 0th choice and return it's string content.</p> <p>If there is no string content (e.g. when using tools), this method must return the empty string.</p>"},{"location":"api/core/base/#mirascope.core.base.BaseCallResponseChunk.finish_reasons","title":"<code>finish_reasons: list[str] | None</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Should return the finish reasons of the response.</p> <p>If there is no finish reason, this method must return None.</p>"},{"location":"api/core/base/#mirascope.core.base.BaseCallResponseChunk.id","title":"<code>id: str | None</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Should return the id of the response.</p>"},{"location":"api/core/base/#mirascope.core.base.BaseCallResponseChunk.input_tokens","title":"<code>input_tokens: int | float | None</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Should return the number of input tokens.</p> <p>If there is no input_tokens, this method must return None.</p>"},{"location":"api/core/base/#mirascope.core.base.BaseCallResponseChunk.model","title":"<code>model: str | None</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Should return the name of the response model.</p>"},{"location":"api/core/base/#mirascope.core.base.BaseCallResponseChunk.output_tokens","title":"<code>output_tokens: int | float | None</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Should return the number of output tokens.</p> <p>If there is no output_tokens, this method must return None.</p>"},{"location":"api/core/base/#mirascope.core.base.BaseCallResponseChunk.usage","title":"<code>usage: Any</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Should return the usage of the response.</p> <p>If there is no usage, this method must return None.</p>"},{"location":"api/core/base/#mirascope.core.base.BaseCallResponseChunk.__str__","title":"<code>__str__()</code>","text":"<p>Returns the string content of the chunk.</p> Source code in <code>mirascope/core/base/call_response_chunk.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Returns the string content of the chunk.\"\"\"\n    return self.content\n</code></pre>"},{"location":"api/core/base/#mirascope.core.base.BaseMessageParam","title":"<code>BaseMessageParam</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>A base class for message parameters.</p> <p>Available roles: <code>system</code>, <code>user</code>, <code>assistant</code>, <code>tool</code>.</p> Source code in <code>mirascope/core/base/message_param.py</code> <pre><code>class BaseMessageParam(TypedDict):\n    \"\"\"A base class for message parameters.\n\n    Available roles: `system`, `user`, `assistant`, `tool`.\n    \"\"\"\n\n    role: Literal[\"system\", \"user\", \"assistant\", \"tool\"]\n    content: str\n</code></pre>"},{"location":"api/core/base/#mirascope.core.base.BasePrompt","title":"<code>BasePrompt</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>The base class for engineering prompts.</p> <p>This class is implemented as the base for all prompting needs. It is intended to work across various providers by providing a common prompt interface.</p> <p>Example:</p> <pre><code>from mirascope.core import BasePrompt, metadata\n\n@metadata({\"tags\": {\"version:0001\"}})\nclass BookRecommendationPrompt(BasePrompt):\n    prompt_template = \"Recommend a {genre} book.\"\n\n    genre: str\n\nprompt = BookRecommendationPrompt(genre=\"fantasy\")\n\nprompt.messages()\n#&gt; [{\"role\": \"user\", \"content\": \"Please recommend a fantasy book.\"}]\n\nprint(prompt)\n#&gt; Please recommend a fantasy book.\n</code></pre> Source code in <code>mirascope/core/base/prompt.py</code> <pre><code>class BasePrompt(BaseModel):\n    \"\"\"The base class for engineering prompts.\n\n    This class is implemented as the base for all prompting needs. It is intended to\n    work across various providers by providing a common prompt interface.\n\n    Example:\n\n    ```python\n    from mirascope.core import BasePrompt, metadata\n\n    @metadata({\"tags\": {\"version:0001\"}})\n    class BookRecommendationPrompt(BasePrompt):\n        prompt_template = \"Recommend a {genre} book.\"\n\n        genre: str\n\n    prompt = BookRecommendationPrompt(genre=\"fantasy\")\n\n    prompt.messages()\n    #&gt; [{\"role\": \"user\", \"content\": \"Please recommend a fantasy book.\"}]\n\n    print(prompt)\n    #&gt; Please recommend a fantasy book.\n    ```\n    \"\"\"\n\n    @classmethod\n    def _prompt_template(cls) -&gt; str:\n        \"\"\"Returns the prompt template.\"\"\"\n        return cls.__annotations__.get(\"prompt_template\", cls.__doc__)\n\n    @classmethod\n    def _metadata(cls) -&gt; set[str]:\n        \"\"\"Returns the prompt's metadata.\"\"\"\n        return cls.__annotations__.get(\"metadata\", {})\n\n    def __str__(self) -&gt; str:\n        \"\"\"Returns the formatted template.\"\"\"\n        return format_template(self._prompt_template(), self.model_dump())\n\n    def dump(self) -&gt; dict[str, Any]:\n        \"\"\"Dumps the contents of the prompt into a dictionary.\"\"\"\n        return {\n            \"metadata\": self._metadata(),\n            \"prompt\": str(self),\n            \"template\": inspect.cleandoc(self._prompt_template()),\n            \"inputs\": self.model_dump(),\n        }\n\n    @overload\n    def run(\n        self,\n        decorator: Callable[\n            [Callable[..., BaseDynamicConfig]], Callable[..., _BaseCallResponseT]\n        ],\n    ) -&gt; _BaseCallResponseT: ...  # pragma: no cover\n\n    @overload\n    def run(\n        self,\n        decorator: Callable[\n            [Callable[..., BaseDynamicConfig]], Callable[..., _BaseStreamT]\n        ],\n    ) -&gt; _BaseStreamT: ...  # pragma: no cover\n\n    @overload\n    def run(  # type: ignore\n        self,\n        decorator: Callable[\n            [Callable[..., BaseDynamicConfig]], Callable[..., _ResponseModelT]\n        ],\n    ) -&gt; _ResponseModelT: ...  # pragma: no cover\n\n    @overload\n    def run(\n        self,\n        decorator: Callable[\n            [Callable[..., BaseDynamicConfig]],\n            Callable[..., Iterable[_ResponseModelT]],\n        ],\n    ) -&gt; Iterable[_ResponseModelT]: ...  # pragma: no cover\n\n    def run(\n        self,\n        decorator: Callable[\n            [Callable[..., BaseDynamicConfig]],\n            Callable[..., _BaseCallResponseT],\n        ]\n        | Callable[\n            [Callable[..., BaseDynamicConfig]],\n            Callable[..., _BaseStreamT],\n        ]\n        | Callable[\n            [Callable[..., BaseDynamicConfig]],\n            Callable[..., _ResponseModelT],\n        ]\n        | Callable[\n            [Callable[..., BaseDynamicConfig]],\n            Callable[..., Iterable[_ResponseModelT]],\n        ],\n    ) -&gt; (\n        _BaseCallResponseT | _BaseStreamT | _ResponseModelT | Iterable[_ResponseModelT]\n    ):\n        \"\"\"Returns the response of calling the API of the provided decorator.\"\"\"\n        kwargs = self.model_dump()\n        args_str = \", \".join(kwargs.keys())\n        namespace = {}\n        exec(f\"def fn({args_str}): ...\", namespace)\n        fn = namespace[\"fn\"]\n        return decorator(prompt_template(self._prompt_template())(fn))(**kwargs)\n\n    @overload\n    def run_async(\n        self,\n        decorator: Callable[\n            [Callable[..., Awaitable[BaseDynamicConfig]]],\n            Callable[..., Awaitable[_BaseCallResponseT]],\n        ],\n    ) -&gt; Awaitable[_BaseCallResponseT]: ...  # pragma: no cover\n\n    @overload\n    def run_async(\n        self,\n        decorator: Callable[\n            [Callable[..., Awaitable[BaseDynamicConfig]]],\n            Callable[..., Awaitable[_BaseStreamT]],\n        ],\n    ) -&gt; Awaitable[_BaseStreamT]: ...  # pragma: no cover\n\n    @overload\n    def run_async(\n        self,\n        decorator: Callable[\n            [Callable[..., Awaitable[BaseDynamicConfig]]],\n            Callable[..., Awaitable[_ResponseModelT]],\n        ],\n    ) -&gt; Awaitable[_ResponseModelT]: ...  # pragma: no cover\n\n    @overload\n    def run_async(\n        self,\n        decorator: Callable[\n            [Callable[..., Awaitable[BaseDynamicConfig]]],\n            Callable[..., Awaitable[AsyncIterable[_ResponseModelT]]],\n        ],\n    ) -&gt; Awaitable[AsyncIterable[_ResponseModelT]]: ...  # pragma: no cover\n\n    def run_async(\n        self,\n        decorator: Callable[\n            [Callable[..., Awaitable[BaseDynamicConfig]]],\n            Callable[..., Awaitable[_BaseCallResponseT]],\n        ]\n        | Callable[\n            [Callable[..., Awaitable[BaseDynamicConfig]]],\n            Callable[..., Awaitable[_BaseStreamT]],\n        ]\n        | Callable[\n            [Callable[..., Awaitable[BaseDynamicConfig]]],\n            Callable[..., Awaitable[_ResponseModelT]],\n        ]\n        | Callable[\n            [Callable[..., Awaitable[BaseDynamicConfig]]],\n            Callable[..., Awaitable[AsyncIterable[_ResponseModelT]]],\n        ],\n    ) -&gt; (\n        Awaitable[_BaseCallResponseT]\n        | Awaitable[_BaseStreamT]\n        | Awaitable[_ResponseModelT]\n        | Awaitable[AsyncIterable[_ResponseModelT]]\n    ):\n        kwargs = self.model_dump()\n        args_str = \", \".join(kwargs.keys())\n        namespace = {}\n        exec(f\"async def fn({args_str}): ...\", namespace)\n        fn = namespace[\"fn\"]\n        return decorator(prompt_template(self._prompt_template())(fn))(**kwargs)\n</code></pre>"},{"location":"api/core/base/#mirascope.core.base.BasePrompt.__str__","title":"<code>__str__()</code>","text":"<p>Returns the formatted template.</p> Source code in <code>mirascope/core/base/prompt.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Returns the formatted template.\"\"\"\n    return format_template(self._prompt_template(), self.model_dump())\n</code></pre>"},{"location":"api/core/base/#mirascope.core.base.BasePrompt.dump","title":"<code>dump()</code>","text":"<p>Dumps the contents of the prompt into a dictionary.</p> Source code in <code>mirascope/core/base/prompt.py</code> <pre><code>def dump(self) -&gt; dict[str, Any]:\n    \"\"\"Dumps the contents of the prompt into a dictionary.\"\"\"\n    return {\n        \"metadata\": self._metadata(),\n        \"prompt\": str(self),\n        \"template\": inspect.cleandoc(self._prompt_template()),\n        \"inputs\": self.model_dump(),\n    }\n</code></pre>"},{"location":"api/core/base/#mirascope.core.base.BasePrompt.run","title":"<code>run(decorator)</code>","text":"<p>Returns the response of calling the API of the provided decorator.</p> Source code in <code>mirascope/core/base/prompt.py</code> <pre><code>def run(\n    self,\n    decorator: Callable[\n        [Callable[..., BaseDynamicConfig]],\n        Callable[..., _BaseCallResponseT],\n    ]\n    | Callable[\n        [Callable[..., BaseDynamicConfig]],\n        Callable[..., _BaseStreamT],\n    ]\n    | Callable[\n        [Callable[..., BaseDynamicConfig]],\n        Callable[..., _ResponseModelT],\n    ]\n    | Callable[\n        [Callable[..., BaseDynamicConfig]],\n        Callable[..., Iterable[_ResponseModelT]],\n    ],\n) -&gt; (\n    _BaseCallResponseT | _BaseStreamT | _ResponseModelT | Iterable[_ResponseModelT]\n):\n    \"\"\"Returns the response of calling the API of the provided decorator.\"\"\"\n    kwargs = self.model_dump()\n    args_str = \", \".join(kwargs.keys())\n    namespace = {}\n    exec(f\"def fn({args_str}): ...\", namespace)\n    fn = namespace[\"fn\"]\n    return decorator(prompt_template(self._prompt_template())(fn))(**kwargs)\n</code></pre>"},{"location":"api/core/base/#mirascope.core.base.BaseTool","title":"<code>BaseTool</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A class for defining tools for LLM calls.</p> Source code in <code>mirascope/core/base/tool.py</code> <pre><code>class BaseTool(BaseModel):\n    \"\"\"A class for defining tools for LLM calls.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @classmethod\n    def _name(cls) -&gt; str:\n        \"\"\"Returns the name of the tool.\"\"\"\n        return cls.__name__\n\n    @classmethod\n    def _description(cls) -&gt; str:\n        \"\"\"Returns the description of the tool.\"\"\"\n        return (\n            inspect.cleandoc(cls.__doc__)\n            if cls.__doc__\n            else _utils.DEFAULT_TOOL_DOCSTRING\n        )\n\n    @abstractmethod\n    def call(self) -&gt; Any:\n        \"\"\"The method to call the tool.\"\"\"\n        ...  # pragma: no cover\n</code></pre>"},{"location":"api/core/base/#mirascope.core.base.BaseTool.call","title":"<code>call()</code>  <code>abstractmethod</code>","text":"<p>The method to call the tool.</p> Source code in <code>mirascope/core/base/tool.py</code> <pre><code>@abstractmethod\ndef call(self) -&gt; Any:\n    \"\"\"The method to call the tool.\"\"\"\n    ...  # pragma: no cover\n</code></pre>"},{"location":"api/core/base/#mirascope.core.base.BaseToolKit","title":"<code>BaseToolKit</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>A class for defining tools for LLM call tools.</p> <p>The class should have methods decorated with <code>@toolkit_tool</code> to create tools.</p> <p>Example: <pre><code>from mirascope.core.base import BaseToolKit, toolkit_tool\nfrom mirascope.core import openai\n\nclass BookRecommendationToolKit(BaseToolKit):\n    '''A toolkit for recommending books.'''\n\n    __namespace__: ClassVar[str | None] = 'book_tools'\n    reading_level: Literal[\"beginner\", \"advanced\"]\n\n    @toolkit_tool\n    def format_book(self, title: str, author: str) -&gt; str:\n        '''Returns the title and author of a book nicely formatted.\n\n        Reading level: {self.reading_level}\n        '''\n        return f\"{title} by {author}\"\n\n@openai.call(model=\"gpt-4o\")\ndef recommend_book(genre: str, reading_level: Literal[\"beginner\", \"advanced\"]):\n    '''Recommend a {genre} book.'''\n    toolkit = BookRecommendationToolKit(reading_level=reading_level)\n    return {\"tools\": toolkit.create_tools()}\n\nresponse = recommend_book(\"fantasy\", \"beginner\")\nif tool := response.tool:\n    output = tool.call()\n    print(output)\n    #&gt; The Name of the Wind by Patrick Rothfuss\nelse:\n    print(response.content)\n    #&gt; Sure! I would recommend...\n</code></pre></p> Source code in <code>mirascope/core/base/toolkit.py</code> <pre><code>class BaseToolKit(BaseModel, ABC):\n    \"\"\"A class for defining tools for LLM call tools.\n\n    The class should have methods decorated with `@toolkit_tool` to create tools.\n\n    Example:\n    ```python\n    from mirascope.core.base import BaseToolKit, toolkit_tool\n    from mirascope.core import openai\n\n    class BookRecommendationToolKit(BaseToolKit):\n        '''A toolkit for recommending books.'''\n\n        __namespace__: ClassVar[str | None] = 'book_tools'\n        reading_level: Literal[\"beginner\", \"advanced\"]\n\n        @toolkit_tool\n        def format_book(self, title: str, author: str) -&gt; str:\n            '''Returns the title and author of a book nicely formatted.\n\n            Reading level: {self.reading_level}\n            '''\n            return f\"{title} by {author}\"\n\n    @openai.call(model=\"gpt-4o\")\n    def recommend_book(genre: str, reading_level: Literal[\"beginner\", \"advanced\"]):\n        '''Recommend a {genre} book.'''\n        toolkit = BookRecommendationToolKit(reading_level=reading_level)\n        return {\"tools\": toolkit.create_tools()}\n\n    response = recommend_book(\"fantasy\", \"beginner\")\n    if tool := response.tool:\n        output = tool.call()\n        print(output)\n        #&gt; The Name of the Wind by Patrick Rothfuss\n    else:\n        print(response.content)\n        #&gt; Sure! I would recommend...\n    ```\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    _toolkit_tool_methods: ClassVar[list[ToolKitToolMethod]]\n    __namespace__: ClassVar[str | None] = None\n\n    def create_tools(self) -&gt; list[type[BaseTool]]:\n        \"\"\"The method to create the tools.\"\"\"\n        return [\n            convert_function_to_base_tool(\n                method, BaseTool, template.format(self=self), self.__namespace__\n            )\n            for method, template_vars, template in self._toolkit_tool_methods\n        ]\n\n    @classmethod\n    def __pydantic_init_subclass__(cls, **kwargs):\n        # validate the namespace\n        if cls.__namespace__:\n            if cls.__namespace__ in _namespaces:\n                raise ValueError(f\"The namespace {cls.__namespace__} is already used\")\n            _namespaces.add(cls.__namespace__)\n\n        cls._toolkit_tool_methods = []\n        for attr in cls.__dict__.values():\n            if not getattr(attr, _TOOLKIT_TOOL_METHOD_MARKER, False):\n                continue\n            # Validate the toolkit_tool_method\n            if (template := attr.__doc__) is None:\n                raise ValueError(\"The toolkit_tool method must have a docstring\")\n\n            dedented_template = inspect.cleandoc(template)\n            template_vars = get_template_variables(dedented_template)\n\n            for var in template_vars:\n                if not var.startswith(\"self.\"):\n                    raise ValueError(\n                        \"The toolkit_tool method must use self. prefix in template variables \"\n                        \"when creating tools dynamically\"\n                    )\n\n                self_var = var[5:]\n\n                # Expecting pydantic model fields or class attribute and property\n                if self_var in cls.model_fields or hasattr(cls, self_var):\n                    continue\n                raise ValueError(\n                    f\"The toolkit_tool method template variable {var} is not found in the class\"\n                )\n\n            cls._toolkit_tool_methods.append(\n                ToolKitToolMethod(attr, template_vars, dedented_template)\n            )\n        if not cls._toolkit_tool_methods:\n            raise ValueError(\"No toolkit_tool method found\")\n</code></pre>"},{"location":"api/core/base/#mirascope.core.base.BaseToolKit.create_tools","title":"<code>create_tools()</code>","text":"<p>The method to create the tools.</p> Source code in <code>mirascope/core/base/toolkit.py</code> <pre><code>def create_tools(self) -&gt; list[type[BaseTool]]:\n    \"\"\"The method to create the tools.\"\"\"\n    return [\n        convert_function_to_base_tool(\n            method, BaseTool, template.format(self=self), self.__namespace__\n        )\n        for method, template_vars, template in self._toolkit_tool_methods\n    ]\n</code></pre>"},{"location":"api/core/base/#mirascope.core.base.prompt_template","title":"<code>prompt_template(template)</code>","text":"<p>A decorator for setting the <code>prompt_template</code> of a <code>BasePrompt</code> or <code>call</code>.</p> Source code in <code>mirascope/core/base/prompt.py</code> <pre><code>def prompt_template(template: str):\n    \"\"\"A decorator for setting the `prompt_template` of a `BasePrompt` or `call`.\"\"\"\n\n    @overload\n    def inner(prompt: type[_BasePromptT]) -&gt; type[_BasePromptT]: ...  # pragma: no cover\n\n    @overload\n    def inner(prompt: Callable[_P, _R]) -&gt; Callable[_P, _R]: ...  # pragma: no cover\n\n    def inner(\n        prompt: type[_BasePromptT] | Callable[_P, _R],\n    ) -&gt; type[_BasePromptT] | Callable[_P, _R]:\n        prompt.__annotations__[\"prompt_template\"] = template\n        return prompt\n\n    return inner\n</code></pre>"},{"location":"api/core/base/call_factory/","title":"mirascope.core.base.call_factory","text":"<p>The <code>call_factory</code> method for generating provider specific call decorators.</p>"},{"location":"api/core/base/call_params/","title":"mirascope.core.base.call_params","text":"<p>This module contains the type definition for the base call parameters.</p>"},{"location":"api/core/base/call_response/","title":"mirascope.core.base.call_response","text":"<p>This module contains the base call response class.</p>"},{"location":"api/core/base/call_response/#mirascope.core.base.call_response.BaseCallResponse","title":"<code>BaseCallResponse</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[_ResponseT, _BaseToolT, _BaseDynamicConfigT, _MessageParamT, _CallParamsT, _UserMessageParamT]</code>, <code>ABC</code></p> <p>A base abstract interface for LLM call responses.</p> <p>Attributes:</p> Name Type Description <code>response</code> <code>_ResponseT</code> <p>The original response from whichever model response this wraps.</p> <code>user_message_param</code> <code>_UserMessageParamT | None</code> <p>The most recent message if it was a user message. Otherwise <code>None</code>.</p> <code>tool_types</code> <code>list[type[_BaseToolT]] | None</code> <p>The tool types sent in the LLM call.</p> <code>start_time</code> <code>float</code> <p>The start time of the completion in ms.</p> <code>end_time</code> <code>float</code> <p>The end time of the completion in ms.</p> <code>cost</code> <code>float | None</code> <p>The cost of the completion in dollars.</p> Source code in <code>mirascope/core/base/call_response.py</code> <pre><code>class BaseCallResponse(\n    BaseModel,\n    Generic[\n        _ResponseT,\n        _BaseToolT,\n        _BaseDynamicConfigT,\n        _MessageParamT,\n        _CallParamsT,\n        _UserMessageParamT,\n    ],\n    ABC,\n):\n    \"\"\"A base abstract interface for LLM call responses.\n\n    Attributes:\n        response: The original response from whichever model response this wraps.\n        user_message_param: The most recent message if it was a user message. Otherwise\n            `None`.\n        tool_types: The tool types sent in the LLM call.\n        start_time: The start time of the completion in ms.\n        end_time: The end time of the completion in ms.\n        cost: The cost of the completion in dollars.\n    \"\"\"\n\n    metadata: Metadata\n    response: _ResponseT\n    tool_types: list[type[_BaseToolT]] | None = None\n    prompt_template: str | None\n    fn_args: dict[str, Any]\n    dynamic_config: _BaseDynamicConfigT\n    messages: SkipValidation[list[_MessageParamT]]\n    call_params: SkipValidation[_CallParamsT]\n    user_message_param: _UserMessageParamT | None = None\n    start_time: float\n    end_time: float\n    cost: float | None = None\n\n    _provider: ClassVar[str] = \"NO PROVIDER\"\n\n    model_config = ConfigDict(extra=\"allow\", arbitrary_types_allowed=True)\n\n    @computed_field\n    @property\n    @abstractmethod\n    def message_param(self) -&gt; Any:\n        \"\"\"Returns the assistant's response as a message parameter.\"\"\"\n        ...  # pragma: no cover\n\n    @computed_field\n    @property\n    @abstractmethod\n    def tools(self) -&gt; list[_BaseToolT] | None:\n        \"\"\"Returns the tools for the 0th choice message.\"\"\"\n        ...  # pragma: no cover\n\n    @property\n    @abstractmethod\n    def tool(self) -&gt; _BaseToolT | None:\n        \"\"\"Returns the 0th tool for the 0th choice message.\"\"\"\n        ...  # pragma: no cover\n\n    @classmethod\n    @abstractmethod\n    def tool_message_params(\n        cls, tools_and_outputs: list[tuple[_BaseToolT, Any]]\n    ) -&gt; list[Any]:\n        \"\"\"Returns the tool message parameters for tool call results.\"\"\"\n        ...  # pragma: no cover\n\n    @property\n    @abstractmethod\n    def content(self) -&gt; str:\n        \"\"\"Should return the string content of the response.\n\n        If there are multiple choices in a response, this method should select the 0th\n        choice and return it's string content.\n\n        If there is no string content (e.g. when using tools), this method must return\n        the empty string.\n        \"\"\"\n        ...  # pragma: no cover\n\n    @property\n    @abstractmethod\n    def finish_reasons(self) -&gt; list[str] | None:\n        \"\"\"Should return the finish reasons of the response.\n\n        If there is no finish reason, this method must return None.\n        \"\"\"\n        ...  # pragma: no cover\n\n    @property\n    @abstractmethod\n    def model(self) -&gt; str | None:\n        \"\"\"Should return the name of the response model.\"\"\"\n        ...  # pragma: no cover\n\n    @property\n    @abstractmethod\n    def id(self) -&gt; str | None:\n        \"\"\"Should return the id of the response.\"\"\"\n        ...  # pragma: no cover\n\n    @property\n    @abstractmethod\n    def usage(self) -&gt; Any:\n        \"\"\"Should return the usage of the response.\n\n        If there is no usage, this method must return None.\n        \"\"\"\n        ...  # pragma: no cover\n\n    @property\n    @abstractmethod\n    def input_tokens(self) -&gt; int | float | None:\n        \"\"\"Should return the number of input tokens.\n\n        If there is no input_tokens, this method must return None.\n        \"\"\"\n        ...  # pragma: no cover\n\n    @property\n    @abstractmethod\n    def output_tokens(self) -&gt; int | float | None:\n        \"\"\"Should return the number of output tokens.\n\n        If there is no output_tokens, this method must return None.\n        \"\"\"\n        ...  # pragma: no cover\n\n    @field_serializer(\"tool_types\")\n    def serialize_tool_types(self, tool_types: list[type[_BaseToolT]], _info):\n        return [{\"type\": \"function\", \"name\": tool._name()} for tool in tool_types or []]\n\n    def __str__(self) -&gt; str:\n        \"\"\"Returns the string content of the response.\"\"\"\n        return self.content\n</code></pre>"},{"location":"api/core/base/call_response/#mirascope.core.base.call_response.BaseCallResponse.content","title":"<code>content: str</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Should return the string content of the response.</p> <p>If there are multiple choices in a response, this method should select the 0th choice and return it's string content.</p> <p>If there is no string content (e.g. when using tools), this method must return the empty string.</p>"},{"location":"api/core/base/call_response/#mirascope.core.base.call_response.BaseCallResponse.finish_reasons","title":"<code>finish_reasons: list[str] | None</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Should return the finish reasons of the response.</p> <p>If there is no finish reason, this method must return None.</p>"},{"location":"api/core/base/call_response/#mirascope.core.base.call_response.BaseCallResponse.id","title":"<code>id: str | None</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Should return the id of the response.</p>"},{"location":"api/core/base/call_response/#mirascope.core.base.call_response.BaseCallResponse.input_tokens","title":"<code>input_tokens: int | float | None</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Should return the number of input tokens.</p> <p>If there is no input_tokens, this method must return None.</p>"},{"location":"api/core/base/call_response/#mirascope.core.base.call_response.BaseCallResponse.message_param","title":"<code>message_param: Any</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Returns the assistant's response as a message parameter.</p>"},{"location":"api/core/base/call_response/#mirascope.core.base.call_response.BaseCallResponse.model","title":"<code>model: str | None</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Should return the name of the response model.</p>"},{"location":"api/core/base/call_response/#mirascope.core.base.call_response.BaseCallResponse.output_tokens","title":"<code>output_tokens: int | float | None</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Should return the number of output tokens.</p> <p>If there is no output_tokens, this method must return None.</p>"},{"location":"api/core/base/call_response/#mirascope.core.base.call_response.BaseCallResponse.tool","title":"<code>tool: _BaseToolT | None</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Returns the 0th tool for the 0th choice message.</p>"},{"location":"api/core/base/call_response/#mirascope.core.base.call_response.BaseCallResponse.tools","title":"<code>tools: list[_BaseToolT] | None</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Returns the tools for the 0th choice message.</p>"},{"location":"api/core/base/call_response/#mirascope.core.base.call_response.BaseCallResponse.usage","title":"<code>usage: Any</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Should return the usage of the response.</p> <p>If there is no usage, this method must return None.</p>"},{"location":"api/core/base/call_response/#mirascope.core.base.call_response.BaseCallResponse.__str__","title":"<code>__str__()</code>","text":"<p>Returns the string content of the response.</p> Source code in <code>mirascope/core/base/call_response.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Returns the string content of the response.\"\"\"\n    return self.content\n</code></pre>"},{"location":"api/core/base/call_response/#mirascope.core.base.call_response.BaseCallResponse.tool_message_params","title":"<code>tool_message_params(tools_and_outputs)</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Returns the tool message parameters for tool call results.</p> Source code in <code>mirascope/core/base/call_response.py</code> <pre><code>@classmethod\n@abstractmethod\ndef tool_message_params(\n    cls, tools_and_outputs: list[tuple[_BaseToolT, Any]]\n) -&gt; list[Any]:\n    \"\"\"Returns the tool message parameters for tool call results.\"\"\"\n    ...  # pragma: no cover\n</code></pre>"},{"location":"api/core/base/call_response_chunk/","title":"mirascope.core.base.call_response_chunk","text":"<p>This module contains the <code>BaseCallResponseChunk</code> class.</p>"},{"location":"api/core/base/call_response_chunk/#mirascope.core.base.call_response_chunk.BaseCallResponseChunk","title":"<code>BaseCallResponseChunk</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[_ChunkT]</code>, <code>ABC</code></p> <p>A base abstract interface for LLM streaming response chunks.</p> <p>Attributes:</p> Name Type Description <code>chunk</code> <code>_ChunkT</code> <p>The original response chunk from whichever model response this wraps.</p> <code>tool_types</code> <code>_ChunkT</code> <p>The tool types sent in the LLM call if any.</p> <code>user_message_param</code> <code>_ChunkT</code> <p>The most recent message if it was a user message. Otherwise <code>None</code>.</p> <code>cost</code> <code>_ChunkT</code> <p>The cost of the completion in dollars.</p> Source code in <code>mirascope/core/base/call_response_chunk.py</code> <pre><code>class BaseCallResponseChunk(BaseModel, Generic[_ChunkT], ABC):\n    \"\"\"A base abstract interface for LLM streaming response chunks.\n\n    Attributes:\n        chunk: The original response chunk from whichever model response this wraps.\n        tool_types: The tool types sent in the LLM call if any.\n        user_message_param: The most recent message if it was a user message. Otherwise\n            `None`.\n        cost: The cost of the completion in dollars.\n    \"\"\"\n\n    chunk: _ChunkT\n\n    model_config = ConfigDict(extra=\"allow\", arbitrary_types_allowed=True)\n\n    @property\n    @abstractmethod\n    def content(self) -&gt; str:\n        \"\"\"Should return the string content of the response chunk.\n\n        If there are multiple choices in a chunk, this method should select the 0th\n        choice and return it's string content.\n\n        If there is no string content (e.g. when using tools), this method must return\n        the empty string.\n        \"\"\"\n        ...  # pragma: no cover\n\n    @property\n    @abstractmethod\n    def model(self) -&gt; str | None:\n        \"\"\"Should return the name of the response model.\"\"\"\n        ...  # pragma: no cover\n\n    @property\n    @abstractmethod\n    def id(self) -&gt; str | None:\n        \"\"\"Should return the id of the response.\"\"\"\n        ...  # pragma: no cover\n\n    @property\n    @abstractmethod\n    def finish_reasons(self) -&gt; list[str] | None:\n        \"\"\"Should return the finish reasons of the response.\n\n        If there is no finish reason, this method must return None.\n        \"\"\"\n        ...  # pragma: no cover\n\n    @property\n    @abstractmethod\n    def usage(self) -&gt; Any:\n        \"\"\"Should return the usage of the response.\n\n        If there is no usage, this method must return None.\n        \"\"\"\n        ...  # pragma: no cover\n\n    @property\n    @abstractmethod\n    def input_tokens(self) -&gt; int | float | None:\n        \"\"\"Should return the number of input tokens.\n\n        If there is no input_tokens, this method must return None.\n        \"\"\"\n        ...  # pragma: no cover\n\n    @property\n    @abstractmethod\n    def output_tokens(self) -&gt; int | float | None:\n        \"\"\"Should return the number of output tokens.\n\n        If there is no output_tokens, this method must return None.\n        \"\"\"\n        ...  # pragma: no cover\n\n    def __str__(self) -&gt; str:\n        \"\"\"Returns the string content of the chunk.\"\"\"\n        return self.content\n</code></pre>"},{"location":"api/core/base/call_response_chunk/#mirascope.core.base.call_response_chunk.BaseCallResponseChunk.content","title":"<code>content: str</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Should return the string content of the response chunk.</p> <p>If there are multiple choices in a chunk, this method should select the 0th choice and return it's string content.</p> <p>If there is no string content (e.g. when using tools), this method must return the empty string.</p>"},{"location":"api/core/base/call_response_chunk/#mirascope.core.base.call_response_chunk.BaseCallResponseChunk.finish_reasons","title":"<code>finish_reasons: list[str] | None</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Should return the finish reasons of the response.</p> <p>If there is no finish reason, this method must return None.</p>"},{"location":"api/core/base/call_response_chunk/#mirascope.core.base.call_response_chunk.BaseCallResponseChunk.id","title":"<code>id: str | None</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Should return the id of the response.</p>"},{"location":"api/core/base/call_response_chunk/#mirascope.core.base.call_response_chunk.BaseCallResponseChunk.input_tokens","title":"<code>input_tokens: int | float | None</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Should return the number of input tokens.</p> <p>If there is no input_tokens, this method must return None.</p>"},{"location":"api/core/base/call_response_chunk/#mirascope.core.base.call_response_chunk.BaseCallResponseChunk.model","title":"<code>model: str | None</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Should return the name of the response model.</p>"},{"location":"api/core/base/call_response_chunk/#mirascope.core.base.call_response_chunk.BaseCallResponseChunk.output_tokens","title":"<code>output_tokens: int | float | None</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Should return the number of output tokens.</p> <p>If there is no output_tokens, this method must return None.</p>"},{"location":"api/core/base/call_response_chunk/#mirascope.core.base.call_response_chunk.BaseCallResponseChunk.usage","title":"<code>usage: Any</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Should return the usage of the response.</p> <p>If there is no usage, this method must return None.</p>"},{"location":"api/core/base/call_response_chunk/#mirascope.core.base.call_response_chunk.BaseCallResponseChunk.__str__","title":"<code>__str__()</code>","text":"<p>Returns the string content of the chunk.</p> Source code in <code>mirascope/core/base/call_response_chunk.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Returns the string content of the chunk.\"\"\"\n    return self.content\n</code></pre>"},{"location":"api/core/base/dynamic_config/","title":"mirascope.core.base.dynamic_config","text":"<p>The base type in a function as an LLM call to return for dynamic configuration.</p>"},{"location":"api/core/base/dynamic_config/#mirascope.core.base.dynamic_config.BaseDynamicConfig","title":"<code>BaseDynamicConfig = DynamicConfigBase | DynamicConfigMessages[_MessageParamT] | DynamicConfigCallParams[_CallParamsT] | DynamicConfigFull[_MessageParamT, _CallParamsT] | None</code>  <code>module-attribute</code>","text":"<p>The base type in a function as an LLM call to return for dynamic configuration.</p>"},{"location":"api/core/base/message_param/","title":"mirascope.core.base.message_param","text":"<p>This module contains the base class for message parameters.</p>"},{"location":"api/core/base/message_param/#mirascope.core.base.message_param.BaseMessageParam","title":"<code>BaseMessageParam</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>A base class for message parameters.</p> <p>Available roles: <code>system</code>, <code>user</code>, <code>assistant</code>, <code>tool</code>.</p> Source code in <code>mirascope/core/base/message_param.py</code> <pre><code>class BaseMessageParam(TypedDict):\n    \"\"\"A base class for message parameters.\n\n    Available roles: `system`, `user`, `assistant`, `tool`.\n    \"\"\"\n\n    role: Literal[\"system\", \"user\", \"assistant\", \"tool\"]\n    content: str\n</code></pre>"},{"location":"api/core/base/metadata/","title":"mirascope.core.base.metadata","text":"<p>The <code>Metadata</code> typed dictionary for including general metadata.</p>"},{"location":"api/core/base/metadata/#mirascope.core.base.metadata.Metadata","title":"<code>Metadata</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>The <code>Metadata</code> typed dictionary for including general metadata.</p> Source code in <code>mirascope/core/base/metadata.py</code> <pre><code>class Metadata(TypedDict, total=False):\n    \"\"\"The `Metadata` typed dictionary for including general metadata.\"\"\"\n\n    tags: NotRequired[set[str]]\n</code></pre>"},{"location":"api/core/base/prompt/","title":"mirascope.core.base.prompt","text":"<p>The <code>BasePrompt</code> class for better prompt engineering.</p>"},{"location":"api/core/base/prompt/#mirascope.core.base.prompt.BasePrompt","title":"<code>BasePrompt</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>The base class for engineering prompts.</p> <p>This class is implemented as the base for all prompting needs. It is intended to work across various providers by providing a common prompt interface.</p> <p>Example:</p> <pre><code>from mirascope.core import BasePrompt, metadata\n\n@metadata({\"tags\": {\"version:0001\"}})\nclass BookRecommendationPrompt(BasePrompt):\n    prompt_template = \"Recommend a {genre} book.\"\n\n    genre: str\n\nprompt = BookRecommendationPrompt(genre=\"fantasy\")\n\nprompt.messages()\n#&gt; [{\"role\": \"user\", \"content\": \"Please recommend a fantasy book.\"}]\n\nprint(prompt)\n#&gt; Please recommend a fantasy book.\n</code></pre> Source code in <code>mirascope/core/base/prompt.py</code> <pre><code>class BasePrompt(BaseModel):\n    \"\"\"The base class for engineering prompts.\n\n    This class is implemented as the base for all prompting needs. It is intended to\n    work across various providers by providing a common prompt interface.\n\n    Example:\n\n    ```python\n    from mirascope.core import BasePrompt, metadata\n\n    @metadata({\"tags\": {\"version:0001\"}})\n    class BookRecommendationPrompt(BasePrompt):\n        prompt_template = \"Recommend a {genre} book.\"\n\n        genre: str\n\n    prompt = BookRecommendationPrompt(genre=\"fantasy\")\n\n    prompt.messages()\n    #&gt; [{\"role\": \"user\", \"content\": \"Please recommend a fantasy book.\"}]\n\n    print(prompt)\n    #&gt; Please recommend a fantasy book.\n    ```\n    \"\"\"\n\n    @classmethod\n    def _prompt_template(cls) -&gt; str:\n        \"\"\"Returns the prompt template.\"\"\"\n        return cls.__annotations__.get(\"prompt_template\", cls.__doc__)\n\n    @classmethod\n    def _metadata(cls) -&gt; set[str]:\n        \"\"\"Returns the prompt's metadata.\"\"\"\n        return cls.__annotations__.get(\"metadata\", {})\n\n    def __str__(self) -&gt; str:\n        \"\"\"Returns the formatted template.\"\"\"\n        return format_template(self._prompt_template(), self.model_dump())\n\n    def dump(self) -&gt; dict[str, Any]:\n        \"\"\"Dumps the contents of the prompt into a dictionary.\"\"\"\n        return {\n            \"metadata\": self._metadata(),\n            \"prompt\": str(self),\n            \"template\": inspect.cleandoc(self._prompt_template()),\n            \"inputs\": self.model_dump(),\n        }\n\n    @overload\n    def run(\n        self,\n        decorator: Callable[\n            [Callable[..., BaseDynamicConfig]], Callable[..., _BaseCallResponseT]\n        ],\n    ) -&gt; _BaseCallResponseT: ...  # pragma: no cover\n\n    @overload\n    def run(\n        self,\n        decorator: Callable[\n            [Callable[..., BaseDynamicConfig]], Callable[..., _BaseStreamT]\n        ],\n    ) -&gt; _BaseStreamT: ...  # pragma: no cover\n\n    @overload\n    def run(  # type: ignore\n        self,\n        decorator: Callable[\n            [Callable[..., BaseDynamicConfig]], Callable[..., _ResponseModelT]\n        ],\n    ) -&gt; _ResponseModelT: ...  # pragma: no cover\n\n    @overload\n    def run(\n        self,\n        decorator: Callable[\n            [Callable[..., BaseDynamicConfig]],\n            Callable[..., Iterable[_ResponseModelT]],\n        ],\n    ) -&gt; Iterable[_ResponseModelT]: ...  # pragma: no cover\n\n    def run(\n        self,\n        decorator: Callable[\n            [Callable[..., BaseDynamicConfig]],\n            Callable[..., _BaseCallResponseT],\n        ]\n        | Callable[\n            [Callable[..., BaseDynamicConfig]],\n            Callable[..., _BaseStreamT],\n        ]\n        | Callable[\n            [Callable[..., BaseDynamicConfig]],\n            Callable[..., _ResponseModelT],\n        ]\n        | Callable[\n            [Callable[..., BaseDynamicConfig]],\n            Callable[..., Iterable[_ResponseModelT]],\n        ],\n    ) -&gt; (\n        _BaseCallResponseT | _BaseStreamT | _ResponseModelT | Iterable[_ResponseModelT]\n    ):\n        \"\"\"Returns the response of calling the API of the provided decorator.\"\"\"\n        kwargs = self.model_dump()\n        args_str = \", \".join(kwargs.keys())\n        namespace = {}\n        exec(f\"def fn({args_str}): ...\", namespace)\n        fn = namespace[\"fn\"]\n        return decorator(prompt_template(self._prompt_template())(fn))(**kwargs)\n\n    @overload\n    def run_async(\n        self,\n        decorator: Callable[\n            [Callable[..., Awaitable[BaseDynamicConfig]]],\n            Callable[..., Awaitable[_BaseCallResponseT]],\n        ],\n    ) -&gt; Awaitable[_BaseCallResponseT]: ...  # pragma: no cover\n\n    @overload\n    def run_async(\n        self,\n        decorator: Callable[\n            [Callable[..., Awaitable[BaseDynamicConfig]]],\n            Callable[..., Awaitable[_BaseStreamT]],\n        ],\n    ) -&gt; Awaitable[_BaseStreamT]: ...  # pragma: no cover\n\n    @overload\n    def run_async(\n        self,\n        decorator: Callable[\n            [Callable[..., Awaitable[BaseDynamicConfig]]],\n            Callable[..., Awaitable[_ResponseModelT]],\n        ],\n    ) -&gt; Awaitable[_ResponseModelT]: ...  # pragma: no cover\n\n    @overload\n    def run_async(\n        self,\n        decorator: Callable[\n            [Callable[..., Awaitable[BaseDynamicConfig]]],\n            Callable[..., Awaitable[AsyncIterable[_ResponseModelT]]],\n        ],\n    ) -&gt; Awaitable[AsyncIterable[_ResponseModelT]]: ...  # pragma: no cover\n\n    def run_async(\n        self,\n        decorator: Callable[\n            [Callable[..., Awaitable[BaseDynamicConfig]]],\n            Callable[..., Awaitable[_BaseCallResponseT]],\n        ]\n        | Callable[\n            [Callable[..., Awaitable[BaseDynamicConfig]]],\n            Callable[..., Awaitable[_BaseStreamT]],\n        ]\n        | Callable[\n            [Callable[..., Awaitable[BaseDynamicConfig]]],\n            Callable[..., Awaitable[_ResponseModelT]],\n        ]\n        | Callable[\n            [Callable[..., Awaitable[BaseDynamicConfig]]],\n            Callable[..., Awaitable[AsyncIterable[_ResponseModelT]]],\n        ],\n    ) -&gt; (\n        Awaitable[_BaseCallResponseT]\n        | Awaitable[_BaseStreamT]\n        | Awaitable[_ResponseModelT]\n        | Awaitable[AsyncIterable[_ResponseModelT]]\n    ):\n        kwargs = self.model_dump()\n        args_str = \", \".join(kwargs.keys())\n        namespace = {}\n        exec(f\"async def fn({args_str}): ...\", namespace)\n        fn = namespace[\"fn\"]\n        return decorator(prompt_template(self._prompt_template())(fn))(**kwargs)\n</code></pre>"},{"location":"api/core/base/prompt/#mirascope.core.base.prompt.BasePrompt.__str__","title":"<code>__str__()</code>","text":"<p>Returns the formatted template.</p> Source code in <code>mirascope/core/base/prompt.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Returns the formatted template.\"\"\"\n    return format_template(self._prompt_template(), self.model_dump())\n</code></pre>"},{"location":"api/core/base/prompt/#mirascope.core.base.prompt.BasePrompt.dump","title":"<code>dump()</code>","text":"<p>Dumps the contents of the prompt into a dictionary.</p> Source code in <code>mirascope/core/base/prompt.py</code> <pre><code>def dump(self) -&gt; dict[str, Any]:\n    \"\"\"Dumps the contents of the prompt into a dictionary.\"\"\"\n    return {\n        \"metadata\": self._metadata(),\n        \"prompt\": str(self),\n        \"template\": inspect.cleandoc(self._prompt_template()),\n        \"inputs\": self.model_dump(),\n    }\n</code></pre>"},{"location":"api/core/base/prompt/#mirascope.core.base.prompt.BasePrompt.run","title":"<code>run(decorator)</code>","text":"<p>Returns the response of calling the API of the provided decorator.</p> Source code in <code>mirascope/core/base/prompt.py</code> <pre><code>def run(\n    self,\n    decorator: Callable[\n        [Callable[..., BaseDynamicConfig]],\n        Callable[..., _BaseCallResponseT],\n    ]\n    | Callable[\n        [Callable[..., BaseDynamicConfig]],\n        Callable[..., _BaseStreamT],\n    ]\n    | Callable[\n        [Callable[..., BaseDynamicConfig]],\n        Callable[..., _ResponseModelT],\n    ]\n    | Callable[\n        [Callable[..., BaseDynamicConfig]],\n        Callable[..., Iterable[_ResponseModelT]],\n    ],\n) -&gt; (\n    _BaseCallResponseT | _BaseStreamT | _ResponseModelT | Iterable[_ResponseModelT]\n):\n    \"\"\"Returns the response of calling the API of the provided decorator.\"\"\"\n    kwargs = self.model_dump()\n    args_str = \", \".join(kwargs.keys())\n    namespace = {}\n    exec(f\"def fn({args_str}): ...\", namespace)\n    fn = namespace[\"fn\"]\n    return decorator(prompt_template(self._prompt_template())(fn))(**kwargs)\n</code></pre>"},{"location":"api/core/base/prompt/#mirascope.core.base.prompt.metadata","title":"<code>metadata(metadata)</code>","text":"<p>A decorator for adding metadata to a <code>BasePrompt</code> or <code>call</code>.</p> <p>Adding this decorator to a <code>BasePrompt</code> or <code>call</code> updates the <code>metadata</code> annotation to the given value. This is useful for adding metadata to a <code>BasePrompt</code> or <code>call</code> that can be used for logging or filtering.</p> <p>Example:</p> <pre><code>from mirascope.core import BasePrompt, metadata\n\n@metadata({\"tags\": {\"version:0001\", \"books\"}})\nclass BookRecommendationPrompt(BasePrompt):\n    prompt_template = \"Recommend a {genre} book.\"\n\n    genre: str\n\nprompt = BookRecommendationPrompt(genre=\"fantasy\")\n\nprint(prompt.dump()[\"metadata\"])\n#&gt; {\"metadata\": {\"version:0001\", \"books\"}}\n</code></pre> <p>Returns:</p> Type Description <p>Decorator function that updates the <code>metadata</code> attribute of the decorated input.</p> Source code in <code>mirascope/core/base/prompt.py</code> <pre><code>def metadata(metadata: Metadata):\n    \"\"\"A decorator for adding metadata to a `BasePrompt` or `call`.\n\n    Adding this decorator to a `BasePrompt` or `call` updates the `metadata` annotation\n    to the given value. This is useful for adding metadata to a `BasePrompt` or `call`\n    that can be used for logging or filtering.\n\n    Example:\n\n    ```python\n    from mirascope.core import BasePrompt, metadata\n\n    @metadata({\"tags\": {\"version:0001\", \"books\"}})\n    class BookRecommendationPrompt(BasePrompt):\n        prompt_template = \"Recommend a {genre} book.\"\n\n        genre: str\n\n    prompt = BookRecommendationPrompt(genre=\"fantasy\")\n\n    print(prompt.dump()[\"metadata\"])\n    #&gt; {\"metadata\": {\"version:0001\", \"books\"}}\n    ```\n\n    Returns:\n        Decorator function that updates the `metadata` attribute of the decorated input.\n    \"\"\"\n\n    @overload\n    def inner(prompt: type[_BasePromptT]) -&gt; type[_BasePromptT]: ...  # pragma: no cover\n\n    @overload\n    def inner(prompt: Callable[_P, _R]) -&gt; Callable[_P, _R]: ...  # pragma: no cover\n\n    def inner(\n        prompt: type[_BasePromptT] | Callable[_P, _R],\n    ) -&gt; type[_BasePromptT] | Callable[_P, _R]:\n        \"\"\"Updates the `metadata` class attribute to the given value.\"\"\"\n        prompt.__annotations__[\"metadata\"] = metadata\n        return prompt\n\n    return inner\n</code></pre>"},{"location":"api/core/base/prompt/#mirascope.core.base.prompt.prompt_template","title":"<code>prompt_template(template)</code>","text":"<p>A decorator for setting the <code>prompt_template</code> of a <code>BasePrompt</code> or <code>call</code>.</p> Source code in <code>mirascope/core/base/prompt.py</code> <pre><code>def prompt_template(template: str):\n    \"\"\"A decorator for setting the `prompt_template` of a `BasePrompt` or `call`.\"\"\"\n\n    @overload\n    def inner(prompt: type[_BasePromptT]) -&gt; type[_BasePromptT]: ...  # pragma: no cover\n\n    @overload\n    def inner(prompt: Callable[_P, _R]) -&gt; Callable[_P, _R]: ...  # pragma: no cover\n\n    def inner(\n        prompt: type[_BasePromptT] | Callable[_P, _R],\n    ) -&gt; type[_BasePromptT] | Callable[_P, _R]:\n        prompt.__annotations__[\"prompt_template\"] = template\n        return prompt\n\n    return inner\n</code></pre>"},{"location":"api/core/base/tool/","title":"mirascope.core.base.tool","text":"<p>This module defines the base class for tools used in LLM calls.</p>"},{"location":"api/core/base/tool/#mirascope.core.base.tool.BaseTool","title":"<code>BaseTool</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A class for defining tools for LLM calls.</p> Source code in <code>mirascope/core/base/tool.py</code> <pre><code>class BaseTool(BaseModel):\n    \"\"\"A class for defining tools for LLM calls.\"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @classmethod\n    def _name(cls) -&gt; str:\n        \"\"\"Returns the name of the tool.\"\"\"\n        return cls.__name__\n\n    @classmethod\n    def _description(cls) -&gt; str:\n        \"\"\"Returns the description of the tool.\"\"\"\n        return (\n            inspect.cleandoc(cls.__doc__)\n            if cls.__doc__\n            else _utils.DEFAULT_TOOL_DOCSTRING\n        )\n\n    @abstractmethod\n    def call(self) -&gt; Any:\n        \"\"\"The method to call the tool.\"\"\"\n        ...  # pragma: no cover\n</code></pre>"},{"location":"api/core/base/tool/#mirascope.core.base.tool.BaseTool.call","title":"<code>call()</code>  <code>abstractmethod</code>","text":"<p>The method to call the tool.</p> Source code in <code>mirascope/core/base/tool.py</code> <pre><code>@abstractmethod\ndef call(self) -&gt; Any:\n    \"\"\"The method to call the tool.\"\"\"\n    ...  # pragma: no cover\n</code></pre>"},{"location":"api/core/base/toolkit/","title":"mirascope.core.base.toolkit","text":"<p>The module for defining the toolkit class for LLM call tools.</p>"},{"location":"api/core/base/toolkit/#mirascope.core.base.toolkit.BaseToolKit","title":"<code>BaseToolKit</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>A class for defining tools for LLM call tools.</p> <p>The class should have methods decorated with <code>@toolkit_tool</code> to create tools.</p> <p>Example: <pre><code>from mirascope.core.base import BaseToolKit, toolkit_tool\nfrom mirascope.core import openai\n\nclass BookRecommendationToolKit(BaseToolKit):\n    '''A toolkit for recommending books.'''\n\n    __namespace__: ClassVar[str | None] = 'book_tools'\n    reading_level: Literal[\"beginner\", \"advanced\"]\n\n    @toolkit_tool\n    def format_book(self, title: str, author: str) -&gt; str:\n        '''Returns the title and author of a book nicely formatted.\n\n        Reading level: {self.reading_level}\n        '''\n        return f\"{title} by {author}\"\n\n@openai.call(model=\"gpt-4o\")\ndef recommend_book(genre: str, reading_level: Literal[\"beginner\", \"advanced\"]):\n    '''Recommend a {genre} book.'''\n    toolkit = BookRecommendationToolKit(reading_level=reading_level)\n    return {\"tools\": toolkit.create_tools()}\n\nresponse = recommend_book(\"fantasy\", \"beginner\")\nif tool := response.tool:\n    output = tool.call()\n    print(output)\n    #&gt; The Name of the Wind by Patrick Rothfuss\nelse:\n    print(response.content)\n    #&gt; Sure! I would recommend...\n</code></pre></p> Source code in <code>mirascope/core/base/toolkit.py</code> <pre><code>class BaseToolKit(BaseModel, ABC):\n    \"\"\"A class for defining tools for LLM call tools.\n\n    The class should have methods decorated with `@toolkit_tool` to create tools.\n\n    Example:\n    ```python\n    from mirascope.core.base import BaseToolKit, toolkit_tool\n    from mirascope.core import openai\n\n    class BookRecommendationToolKit(BaseToolKit):\n        '''A toolkit for recommending books.'''\n\n        __namespace__: ClassVar[str | None] = 'book_tools'\n        reading_level: Literal[\"beginner\", \"advanced\"]\n\n        @toolkit_tool\n        def format_book(self, title: str, author: str) -&gt; str:\n            '''Returns the title and author of a book nicely formatted.\n\n            Reading level: {self.reading_level}\n            '''\n            return f\"{title} by {author}\"\n\n    @openai.call(model=\"gpt-4o\")\n    def recommend_book(genre: str, reading_level: Literal[\"beginner\", \"advanced\"]):\n        '''Recommend a {genre} book.'''\n        toolkit = BookRecommendationToolKit(reading_level=reading_level)\n        return {\"tools\": toolkit.create_tools()}\n\n    response = recommend_book(\"fantasy\", \"beginner\")\n    if tool := response.tool:\n        output = tool.call()\n        print(output)\n        #&gt; The Name of the Wind by Patrick Rothfuss\n    else:\n        print(response.content)\n        #&gt; Sure! I would recommend...\n    ```\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n    _toolkit_tool_methods: ClassVar[list[ToolKitToolMethod]]\n    __namespace__: ClassVar[str | None] = None\n\n    def create_tools(self) -&gt; list[type[BaseTool]]:\n        \"\"\"The method to create the tools.\"\"\"\n        return [\n            convert_function_to_base_tool(\n                method, BaseTool, template.format(self=self), self.__namespace__\n            )\n            for method, template_vars, template in self._toolkit_tool_methods\n        ]\n\n    @classmethod\n    def __pydantic_init_subclass__(cls, **kwargs):\n        # validate the namespace\n        if cls.__namespace__:\n            if cls.__namespace__ in _namespaces:\n                raise ValueError(f\"The namespace {cls.__namespace__} is already used\")\n            _namespaces.add(cls.__namespace__)\n\n        cls._toolkit_tool_methods = []\n        for attr in cls.__dict__.values():\n            if not getattr(attr, _TOOLKIT_TOOL_METHOD_MARKER, False):\n                continue\n            # Validate the toolkit_tool_method\n            if (template := attr.__doc__) is None:\n                raise ValueError(\"The toolkit_tool method must have a docstring\")\n\n            dedented_template = inspect.cleandoc(template)\n            template_vars = get_template_variables(dedented_template)\n\n            for var in template_vars:\n                if not var.startswith(\"self.\"):\n                    raise ValueError(\n                        \"The toolkit_tool method must use self. prefix in template variables \"\n                        \"when creating tools dynamically\"\n                    )\n\n                self_var = var[5:]\n\n                # Expecting pydantic model fields or class attribute and property\n                if self_var in cls.model_fields or hasattr(cls, self_var):\n                    continue\n                raise ValueError(\n                    f\"The toolkit_tool method template variable {var} is not found in the class\"\n                )\n\n            cls._toolkit_tool_methods.append(\n                ToolKitToolMethod(attr, template_vars, dedented_template)\n            )\n        if not cls._toolkit_tool_methods:\n            raise ValueError(\"No toolkit_tool method found\")\n</code></pre>"},{"location":"api/core/base/toolkit/#mirascope.core.base.toolkit.BaseToolKit.create_tools","title":"<code>create_tools()</code>","text":"<p>The method to create the tools.</p> Source code in <code>mirascope/core/base/toolkit.py</code> <pre><code>def create_tools(self) -&gt; list[type[BaseTool]]:\n    \"\"\"The method to create the tools.\"\"\"\n    return [\n        convert_function_to_base_tool(\n            method, BaseTool, template.format(self=self), self.__namespace__\n        )\n        for method, template_vars, template in self._toolkit_tool_methods\n    ]\n</code></pre>"},{"location":"api/core/cohere/","title":"mirascope.core.cohere","text":"<p>The Mirascope Cohere Module.</p>"},{"location":"api/core/cohere/#mirascope.core.cohere.CohereDynamicConfig","title":"<code>CohereDynamicConfig = BaseDynamicConfig[BaseMessageParam, CohereCallParams]</code>  <code>module-attribute</code>","text":"<p>The function return type for functions wrapped with the <code>cohere_call</code> decorator.</p> <p>Attributes:</p> Name Type Description <code>computed_fields</code> <p>The computed fields to use in the prompt template.</p> <code>messages</code> <p>The messages to send to the Cohere API. If provided, <code>computed_fields</code> will be ignored.</p> <code>call_params</code> <p>The call parameters to use when calling the Cohere API. These will override any call parameters provided to the decorator.</p> <p>Example:</p> <pre><code>from mirascope.core.cohere import CohereDynamicConfig, cohere_call\n\n@cohere_call(model=\"gpt-4o\")\ndef recommend_book(genre: str) -&gt; CohereDynamicConfig:\n    \"\"\"Recommend a {capitalized_genre} book.\"\"\"\n    reeturn {\"computed_fields\": {\"capitalized_genre\": genre.capitalize()}}\n</code></pre>"},{"location":"api/core/cohere/#mirascope.core.cohere.cohere_call","title":"<code>cohere_call = call_factory(TCallResponse=CohereCallResponse, TCallResponseChunk=CohereCallResponseChunk, TDynamicConfig=CohereDynamicConfig, TMessageParamType=BaseMessageParam, TToolType=CohereTool, TStream=CohereStream, TCallParams=CohereCallParams, default_call_params=CohereCallParams(), setup_call=setup_call, get_json_output=get_json_output, handle_stream=handle_stream, handle_stream_async=handle_stream_async, calculate_cost=calculate_cost, provider='cohere')</code>  <code>module-attribute</code>","text":"<p>A decorator for calling the Cohere API with a typed function.</p> <p>This decorator is used to wrap a typed function that calls the Cohere API. It parses the docstring of the wrapped function as the messages array and templates the input arguments for the function into each message's template.</p> <p>Example:</p> <pre><code>@cohere_call(model=\"gpt-4o\")\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\nresponse = recommend_book(\"fantasy\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>The Cohere model to use in the API call.</p> required <code>stream</code> <p>Whether to stream the response from the API call.</p> required <code>tools</code> <p>The tools to use in the Cohere API call.</p> required <code>**call_params</code> <p>The <code>CohereCallParams</code> call parameters to use in the API call.</p> required <p>Returns:</p> Type Description <p>The decorator for turning a typed function into an Cohere API call.</p>"},{"location":"api/core/cohere/#mirascope.core.cohere.CohereCallParams","title":"<code>CohereCallParams</code>","text":"<p>               Bases: <code>BaseCallParams</code></p> <p>The parameters to use when calling the Cohere API.</p> Source code in <code>mirascope/core/cohere/call_params.py</code> <pre><code>class CohereCallParams(BaseCallParams):\n    \"\"\"The parameters to use when calling the Cohere API.\"\"\"\n\n    preamble: NotRequired[str | None]\n    chat_history: NotRequired[Sequence[ChatMessage] | None]\n    conversation_id: NotRequired[str | None]\n    prompt_truncation: NotRequired[ChatRequestPromptTruncation | None]\n    connectors: NotRequired[Sequence[ChatConnector] | None]\n    search_queries_only: NotRequired[bool | None]\n    documents: NotRequired[Sequence[ChatDocument] | None]\n    temperature: NotRequired[float | None]\n    max_tokens: NotRequired[int | None]\n    max_input_tokens: NotRequired[int | None]\n    k: NotRequired[int | None]\n    p: NotRequired[float | None]\n    seed: NotRequired[int | None]\n    stop_sequences: NotRequired[Sequence[str] | None]\n    frequency_penalty: NotRequired[float | None]\n    presence_penalty: NotRequired[float | None]\n    raw_prompting: NotRequired[bool | None]\n    tool_results: NotRequired[Sequence[ToolResult] | None]\n</code></pre>"},{"location":"api/core/cohere/#mirascope.core.cohere.CohereCallResponse","title":"<code>CohereCallResponse</code>","text":"<p>               Bases: <code>BaseCallResponse[SkipValidation[NonStreamedChatResponse], CohereTool, CohereDynamicConfig, SkipValidation[ChatMessage], CohereCallParams, SkipValidation[ChatMessage]]</code></p> <p>A convenience wrapper around the Cohere <code>ChatCompletion</code> response.</p> <p>When calling the Cohere API using a function decorated with <code>cohere_call</code>, the response will be an <code>CohereCallResponse</code> instance with properties that allow for more convenience access to commonly used attributes.</p> <p>Example:</p> <pre><code>from mirascope.core.cohere import cohere_call\n\n@cohere_call(model=\"gpt-4o\")\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\nresponse = recommend_book(\"fantasy\")  # response is an `CohereCallResponse` instance\nprint(response.content)\n#&gt; Sure! I would recommend...\n</code></pre> Source code in <code>mirascope/core/cohere/call_response.py</code> <pre><code>class CohereCallResponse(\n    BaseCallResponse[\n        SkipValidation[NonStreamedChatResponse],\n        CohereTool,\n        CohereDynamicConfig,\n        SkipValidation[ChatMessage],\n        CohereCallParams,\n        SkipValidation[ChatMessage],\n    ]\n):\n    '''A convenience wrapper around the Cohere `ChatCompletion` response.\n\n    When calling the Cohere API using a function decorated with `cohere_call`, the\n    response will be an `CohereCallResponse` instance with properties that allow for\n    more convenience access to commonly used attributes.\n\n    Example:\n\n    ```python\n    from mirascope.core.cohere import cohere_call\n\n    @cohere_call(model=\"gpt-4o\")\n    def recommend_book(genre: str):\n        \"\"\"Recommend a {genre} book.\"\"\"\n\n    response = recommend_book(\"fantasy\")  # response is an `CohereCallResponse` instance\n    print(response.content)\n    #&gt; Sure! I would recommend...\n    ```\n    '''\n\n    _provider = \"cohere\"\n\n    @computed_field\n    @property\n    def message_param(self) -&gt; ChatMessage:\n        \"\"\"Returns the assistant's response as a message parameter.\"\"\"\n        return ChatMessage(\n            message=self.response.text,\n            tool_calls=self.response.tool_calls,\n            role=\"assistant\",  # type: ignore\n        )\n\n    @property\n    def content(self) -&gt; str:\n        \"\"\"Returns the content of the chat completion for the 0th choice.\"\"\"\n        return self.response.text\n\n    @property\n    def model(self) -&gt; str | None:\n        \"\"\"Returns the name of the response model.\n\n        Cohere does not return model, so we return None\n        \"\"\"\n        return None\n\n    @property\n    def id(self) -&gt; str | None:\n        \"\"\"Returns the id of the response.\"\"\"\n        return self.response.generation_id\n\n    @property\n    def finish_reasons(self) -&gt; list[str] | None:\n        \"\"\"Returns the finish reasons of the response.\"\"\"\n        return [str(self.response.finish_reason)]\n\n    @property\n    def search_queries(self) -&gt; list[ChatSearchQuery] | None:\n        \"\"\"Returns the search queries for the 0th choice message.\"\"\"\n        return self.response.search_queries\n\n    @property\n    def search_results(self) -&gt; list[ChatSearchResult] | None:\n        \"\"\"Returns the search results for the 0th choice message.\"\"\"\n        return self.response.search_results\n\n    @property\n    def documents(self) -&gt; list[ChatDocument] | None:\n        \"\"\"Returns the documents for the 0th choice message.\"\"\"\n        return self.response.documents\n\n    @property\n    def citations(self) -&gt; list[ChatCitation] | None:\n        \"\"\"Returns the citations for the 0th choice message.\"\"\"\n        return self.response.citations\n\n    @property\n    def tool_calls(self) -&gt; list[ToolCall] | None:\n        \"\"\"Returns the tool calls for the 0th choice message.\"\"\"\n        return self.response.tool_calls\n\n    @computed_field\n    @property\n    def tools(self) -&gt; list[CohereTool] | None:\n        \"\"\"Returns the tools for the 0th choice message.\n\n        Raises:\n            ValidationError: if a tool call doesn't match the tool's schema.\n        \"\"\"\n        if not self.tool_types or not self.tool_calls:\n            return None\n\n        extracted_tools = []\n        for tool_call in self.tool_calls:\n            for tool_type in self.tool_types:\n                if tool_call.name == tool_type._name():\n                    extracted_tools.append(tool_type.from_tool_call(tool_call))\n                    break\n\n        return extracted_tools\n\n    @computed_field\n    @property\n    def tool(self) -&gt; CohereTool | None:\n        \"\"\"Returns the 0th tool for the 0th choice message.\n\n        Raises:\n            ValidationError: if the tool call doesn't match the tool's schema.\n        \"\"\"\n        tools = self.tools\n        if tools:\n            return tools[0]\n        return None\n\n    @classmethod\n    def tool_message_params(\n        cls, tools_and_outputs: list[tuple[CohereTool, list[dict[str, Any]]]]\n    ) -&gt; list[ToolResult]:\n        \"\"\"Returns the tool message parameters for tool call results.\"\"\"\n        return [\n            {\"call\": tool.tool_call, \"outputs\": output}  # type: ignore\n            for tool, output in tools_and_outputs\n        ]\n\n    @property\n    def usage(self) -&gt; ApiMetaBilledUnits | None:\n        \"\"\"Returns the usage of the response.\"\"\"\n        if self.response.meta:\n            return self.response.meta.billed_units\n        return None\n\n    @property\n    def input_tokens(self) -&gt; float | None:\n        \"\"\"Returns the number of input tokens.\"\"\"\n        if self.usage:\n            return self.usage.input_tokens\n        return None\n\n    @property\n    def output_tokens(self) -&gt; float | None:\n        \"\"\"Returns the number of output tokens.\"\"\"\n        if self.usage:\n            return self.usage.output_tokens\n        return None\n</code></pre>"},{"location":"api/core/cohere/#mirascope.core.cohere.CohereCallResponse.citations","title":"<code>citations: list[ChatCitation] | None</code>  <code>property</code>","text":"<p>Returns the citations for the 0th choice message.</p>"},{"location":"api/core/cohere/#mirascope.core.cohere.CohereCallResponse.content","title":"<code>content: str</code>  <code>property</code>","text":"<p>Returns the content of the chat completion for the 0th choice.</p>"},{"location":"api/core/cohere/#mirascope.core.cohere.CohereCallResponse.documents","title":"<code>documents: list[ChatDocument] | None</code>  <code>property</code>","text":"<p>Returns the documents for the 0th choice message.</p>"},{"location":"api/core/cohere/#mirascope.core.cohere.CohereCallResponse.finish_reasons","title":"<code>finish_reasons: list[str] | None</code>  <code>property</code>","text":"<p>Returns the finish reasons of the response.</p>"},{"location":"api/core/cohere/#mirascope.core.cohere.CohereCallResponse.id","title":"<code>id: str | None</code>  <code>property</code>","text":"<p>Returns the id of the response.</p>"},{"location":"api/core/cohere/#mirascope.core.cohere.CohereCallResponse.input_tokens","title":"<code>input_tokens: float | None</code>  <code>property</code>","text":"<p>Returns the number of input tokens.</p>"},{"location":"api/core/cohere/#mirascope.core.cohere.CohereCallResponse.message_param","title":"<code>message_param: ChatMessage</code>  <code>property</code>","text":"<p>Returns the assistant's response as a message parameter.</p>"},{"location":"api/core/cohere/#mirascope.core.cohere.CohereCallResponse.model","title":"<code>model: str | None</code>  <code>property</code>","text":"<p>Returns the name of the response model.</p> <p>Cohere does not return model, so we return None</p>"},{"location":"api/core/cohere/#mirascope.core.cohere.CohereCallResponse.output_tokens","title":"<code>output_tokens: float | None</code>  <code>property</code>","text":"<p>Returns the number of output tokens.</p>"},{"location":"api/core/cohere/#mirascope.core.cohere.CohereCallResponse.search_queries","title":"<code>search_queries: list[ChatSearchQuery] | None</code>  <code>property</code>","text":"<p>Returns the search queries for the 0th choice message.</p>"},{"location":"api/core/cohere/#mirascope.core.cohere.CohereCallResponse.search_results","title":"<code>search_results: list[ChatSearchResult] | None</code>  <code>property</code>","text":"<p>Returns the search results for the 0th choice message.</p>"},{"location":"api/core/cohere/#mirascope.core.cohere.CohereCallResponse.tool","title":"<code>tool: CohereTool | None</code>  <code>property</code>","text":"<p>Returns the 0th tool for the 0th choice message.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if the tool call doesn't match the tool's schema.</p>"},{"location":"api/core/cohere/#mirascope.core.cohere.CohereCallResponse.tool_calls","title":"<code>tool_calls: list[ToolCall] | None</code>  <code>property</code>","text":"<p>Returns the tool calls for the 0th choice message.</p>"},{"location":"api/core/cohere/#mirascope.core.cohere.CohereCallResponse.tools","title":"<code>tools: list[CohereTool] | None</code>  <code>property</code>","text":"<p>Returns the tools for the 0th choice message.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if a tool call doesn't match the tool's schema.</p>"},{"location":"api/core/cohere/#mirascope.core.cohere.CohereCallResponse.usage","title":"<code>usage: ApiMetaBilledUnits | None</code>  <code>property</code>","text":"<p>Returns the usage of the response.</p>"},{"location":"api/core/cohere/#mirascope.core.cohere.CohereCallResponse.tool_message_params","title":"<code>tool_message_params(tools_and_outputs)</code>  <code>classmethod</code>","text":"<p>Returns the tool message parameters for tool call results.</p> Source code in <code>mirascope/core/cohere/call_response.py</code> <pre><code>@classmethod\ndef tool_message_params(\n    cls, tools_and_outputs: list[tuple[CohereTool, list[dict[str, Any]]]]\n) -&gt; list[ToolResult]:\n    \"\"\"Returns the tool message parameters for tool call results.\"\"\"\n    return [\n        {\"call\": tool.tool_call, \"outputs\": output}  # type: ignore\n        for tool, output in tools_and_outputs\n    ]\n</code></pre>"},{"location":"api/core/cohere/#mirascope.core.cohere.CohereCallResponseChunk","title":"<code>CohereCallResponseChunk</code>","text":"<p>               Bases: <code>BaseCallResponseChunk[SkipValidation[StreamedChatResponse]]</code></p> <p>A convenience wrapper around the Cohere <code>ChatCompletionChunk</code> streamed chunks.</p> <p>When calling the Cohere API using a function decorated with <code>cohere_call</code> and <code>stream</code> set to <code>True</code>, the stream will contain <code>CohereResponseChunk</code> instances with properties that allow for more convenient access to commonly used attributes.</p> <p>Example:</p> <pre><code>from mirascope.core.cohere import cohere_call\n\n@cohere_call(model=\"gpt-4o\", stream=True)\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\nstream = recommend_book(\"fantasy\")  # response is an `CohereStream`\nfor chunk in stream:\n    print(chunk.content, end=\"\", flush=True)\n#&gt; Sure! I would recommend...\n</code></pre> Source code in <code>mirascope/core/cohere/call_response_chunk.py</code> <pre><code>class CohereCallResponseChunk(\n    BaseCallResponseChunk[SkipValidation[StreamedChatResponse]]\n):\n    '''A convenience wrapper around the Cohere `ChatCompletionChunk` streamed chunks.\n\n    When calling the Cohere API using a function decorated with `cohere_call` and\n    `stream` set to `True`, the stream will contain `CohereResponseChunk` instances with\n    properties that allow for more convenient access to commonly used attributes.\n\n    Example:\n\n    ```python\n    from mirascope.core.cohere import cohere_call\n\n    @cohere_call(model=\"gpt-4o\", stream=True)\n    def recommend_book(genre: str):\n        \"\"\"Recommend a {genre} book.\"\"\"\n\n    stream = recommend_book(\"fantasy\")  # response is an `CohereStream`\n    for chunk in stream:\n        print(chunk.content, end=\"\", flush=True)\n    #&gt; Sure! I would recommend...\n    ```\n    '''\n\n    @property\n    def event_type(\n        self,\n    ) -&gt; Literal[\n        \"stream-start\",\n        \"search-queries-generation\",\n        \"search-results\",\n        \"text-generation\",\n        \"citation-generation\",\n        \"tool-calls-generation\",\n        \"stream-end\",\n        \"tool-calls-chunk\",\n    ]:\n        \"\"\"Returns the type of the chunk.\"\"\"\n        return self.chunk.event_type\n\n    @property\n    def content(self) -&gt; str:\n        \"\"\"Returns the content for the 0th choice delta.\"\"\"\n        if isinstance(self.chunk, StreamedChatResponse_TextGeneration):\n            return self.chunk.text\n        return \"\"\n\n    @property\n    def search_queries(self) -&gt; list[ChatSearchQuery] | None:\n        \"\"\"Returns the search queries for search-query event type else None.\"\"\"\n        if isinstance(self.chunk, StreamedChatResponse_SearchQueriesGeneration):\n            return self.chunk.search_queries  # type: ignore\n        return None\n\n    @property\n    def search_results(self) -&gt; list[ChatSearchResult] | None:\n        \"\"\"Returns the search results for search-results event type else None.\"\"\"\n        if isinstance(self.chunk, StreamedChatResponse_SearchResults):\n            return self.chunk.search_results\n        return None\n\n    @property\n    def documents(self) -&gt; list[ChatDocument] | None:\n        \"\"\"Returns the documents for search-results event type else None.\"\"\"\n        if isinstance(self.chunk, StreamedChatResponse_SearchResults):\n            return self.chunk.documents\n        return None\n\n    @property\n    def citations(self) -&gt; list[ChatCitation] | None:\n        \"\"\"Returns the citations for citation-generation event type else None.\"\"\"\n        if isinstance(self.chunk, StreamedChatResponse_CitationGeneration):\n            return self.chunk.citations\n        return None\n\n    @property\n    def model(self) -&gt; str | None:\n        \"\"\"Returns the name of the response model.\n\n        Cohere does not return model, so we return None\n        \"\"\"\n        return None\n\n    @property\n    def id(self) -&gt; str | None:\n        \"\"\"Returns the id of the response.\"\"\"\n        if isinstance(self.chunk, StreamedChatResponse_StreamStart):\n            return self.chunk.generation_id\n        return None\n\n    @property\n    def finish_reasons(self) -&gt; list[str] | None:\n        \"\"\"Returns the finish reasons of the response.\"\"\"\n        if isinstance(self.chunk, StreamedChatResponse_StreamEnd):\n            return [str(self.chunk.finish_reason)]\n        return None\n\n    @property\n    def response(self) -&gt; NonStreamedChatResponse | None:\n        \"\"\"Returns the full response for the stream-end event type else None.\"\"\"\n        if isinstance(self.chunk, StreamedChatResponse_StreamEnd):\n            return self.chunk.response\n        return None\n\n    @property\n    def tool_calls(self) -&gt; list[ToolCall] | None:\n        \"\"\"Returns the partial tool calls for the 0th choice message.\"\"\"\n        if isinstance(self.chunk, StreamedChatResponse_ToolCallsGeneration):\n            return self.chunk.tool_calls\n        return None\n\n    @property\n    def usage(self) -&gt; ApiMetaBilledUnits | None:\n        \"\"\"Returns the usage of the response.\"\"\"\n        if isinstance(self.chunk, StreamedChatResponse_StreamEnd):\n            if self.chunk.response.meta:\n                return self.chunk.response.meta.billed_units\n        return None\n\n    @property\n    def input_tokens(self) -&gt; float | None:\n        \"\"\"Returns the number of input tokens.\"\"\"\n        if self.usage:\n            return self.usage.input_tokens\n        return None\n\n    @property\n    def output_tokens(self) -&gt; float | None:\n        \"\"\"Returns the number of output tokens.\"\"\"\n        if self.usage:\n            return self.usage.output_tokens\n        return None\n</code></pre>"},{"location":"api/core/cohere/#mirascope.core.cohere.CohereCallResponseChunk.citations","title":"<code>citations: list[ChatCitation] | None</code>  <code>property</code>","text":"<p>Returns the citations for citation-generation event type else None.</p>"},{"location":"api/core/cohere/#mirascope.core.cohere.CohereCallResponseChunk.content","title":"<code>content: str</code>  <code>property</code>","text":"<p>Returns the content for the 0th choice delta.</p>"},{"location":"api/core/cohere/#mirascope.core.cohere.CohereCallResponseChunk.documents","title":"<code>documents: list[ChatDocument] | None</code>  <code>property</code>","text":"<p>Returns the documents for search-results event type else None.</p>"},{"location":"api/core/cohere/#mirascope.core.cohere.CohereCallResponseChunk.event_type","title":"<code>event_type: Literal['stream-start', 'search-queries-generation', 'search-results', 'text-generation', 'citation-generation', 'tool-calls-generation', 'stream-end', 'tool-calls-chunk']</code>  <code>property</code>","text":"<p>Returns the type of the chunk.</p>"},{"location":"api/core/cohere/#mirascope.core.cohere.CohereCallResponseChunk.finish_reasons","title":"<code>finish_reasons: list[str] | None</code>  <code>property</code>","text":"<p>Returns the finish reasons of the response.</p>"},{"location":"api/core/cohere/#mirascope.core.cohere.CohereCallResponseChunk.id","title":"<code>id: str | None</code>  <code>property</code>","text":"<p>Returns the id of the response.</p>"},{"location":"api/core/cohere/#mirascope.core.cohere.CohereCallResponseChunk.input_tokens","title":"<code>input_tokens: float | None</code>  <code>property</code>","text":"<p>Returns the number of input tokens.</p>"},{"location":"api/core/cohere/#mirascope.core.cohere.CohereCallResponseChunk.model","title":"<code>model: str | None</code>  <code>property</code>","text":"<p>Returns the name of the response model.</p> <p>Cohere does not return model, so we return None</p>"},{"location":"api/core/cohere/#mirascope.core.cohere.CohereCallResponseChunk.output_tokens","title":"<code>output_tokens: float | None</code>  <code>property</code>","text":"<p>Returns the number of output tokens.</p>"},{"location":"api/core/cohere/#mirascope.core.cohere.CohereCallResponseChunk.response","title":"<code>response: NonStreamedChatResponse | None</code>  <code>property</code>","text":"<p>Returns the full response for the stream-end event type else None.</p>"},{"location":"api/core/cohere/#mirascope.core.cohere.CohereCallResponseChunk.search_queries","title":"<code>search_queries: list[ChatSearchQuery] | None</code>  <code>property</code>","text":"<p>Returns the search queries for search-query event type else None.</p>"},{"location":"api/core/cohere/#mirascope.core.cohere.CohereCallResponseChunk.search_results","title":"<code>search_results: list[ChatSearchResult] | None</code>  <code>property</code>","text":"<p>Returns the search results for search-results event type else None.</p>"},{"location":"api/core/cohere/#mirascope.core.cohere.CohereCallResponseChunk.tool_calls","title":"<code>tool_calls: list[ToolCall] | None</code>  <code>property</code>","text":"<p>Returns the partial tool calls for the 0th choice message.</p>"},{"location":"api/core/cohere/#mirascope.core.cohere.CohereCallResponseChunk.usage","title":"<code>usage: ApiMetaBilledUnits | None</code>  <code>property</code>","text":"<p>Returns the usage of the response.</p>"},{"location":"api/core/cohere/#mirascope.core.cohere.CohereTool","title":"<code>CohereTool</code>","text":"<p>               Bases: <code>BaseTool</code></p> <p>A class for defining tools for Cohere LLM calls.</p> Source code in <code>mirascope/core/cohere/tool.py</code> <pre><code>class CohereTool(BaseTool):\n    \"\"\"A class for defining tools for Cohere LLM calls.\"\"\"\n\n    tool_call: SkipJsonSchema[SkipValidation[ToolCall]]\n\n    @classmethod\n    def tool_schema(cls) -&gt; Tool:\n        \"\"\"Constructs a JSON Schema tool schema from the `BaseModel` schema defined.\"\"\"\n        model_schema = cls.model_json_schema()\n        model_schema.pop(\"title\", None)\n        model_schema.pop(\"description\", None)\n        parameter_definitions = None\n        if \"parameters\" in model_schema:\n            if \"$defs\" in model_schema[\"parameters\"]:\n                raise ValueError(\n                    \"Unfortunately Cohere's chat API cannot handle nested structures \"\n                    \"with $defs.\"\n                )\n            parameter_definitions = {\n                prop: ToolParameterDefinitionsValue(\n                    description=prop_schema[\"description\"]\n                    if \"description\" in prop_schema\n                    else None,\n                    type=prop_schema[\"type\"],\n                    required=\"required\" in model_schema[\"parameters\"]\n                    and prop in model_schema[\"parameters\"][\"required\"],\n                )\n                for prop, prop_schema in model_schema[\"parameters\"][\n                    \"properties\"\n                ].items()\n            }\n        return Tool(\n            name=cls._name(),\n            description=cls._description(),\n            parameter_definitions=parameter_definitions,\n        )\n\n    @classmethod\n    def from_tool_call(cls, tool_call: ToolCall) -&gt; CohereTool:\n        \"\"\"Constructs an `CohereTool` instance from a `tool_call`.\"\"\"\n        model_json = tool_call.parameters\n        model_json[\"tool_call\"] = tool_call.model_dump()\n        return cls.model_validate(model_json)\n</code></pre>"},{"location":"api/core/cohere/#mirascope.core.cohere.CohereTool.from_tool_call","title":"<code>from_tool_call(tool_call)</code>  <code>classmethod</code>","text":"<p>Constructs an <code>CohereTool</code> instance from a <code>tool_call</code>.</p> Source code in <code>mirascope/core/cohere/tool.py</code> <pre><code>@classmethod\ndef from_tool_call(cls, tool_call: ToolCall) -&gt; CohereTool:\n    \"\"\"Constructs an `CohereTool` instance from a `tool_call`.\"\"\"\n    model_json = tool_call.parameters\n    model_json[\"tool_call\"] = tool_call.model_dump()\n    return cls.model_validate(model_json)\n</code></pre>"},{"location":"api/core/cohere/#mirascope.core.cohere.CohereTool.tool_schema","title":"<code>tool_schema()</code>  <code>classmethod</code>","text":"<p>Constructs a JSON Schema tool schema from the <code>BaseModel</code> schema defined.</p> Source code in <code>mirascope/core/cohere/tool.py</code> <pre><code>@classmethod\ndef tool_schema(cls) -&gt; Tool:\n    \"\"\"Constructs a JSON Schema tool schema from the `BaseModel` schema defined.\"\"\"\n    model_schema = cls.model_json_schema()\n    model_schema.pop(\"title\", None)\n    model_schema.pop(\"description\", None)\n    parameter_definitions = None\n    if \"parameters\" in model_schema:\n        if \"$defs\" in model_schema[\"parameters\"]:\n            raise ValueError(\n                \"Unfortunately Cohere's chat API cannot handle nested structures \"\n                \"with $defs.\"\n            )\n        parameter_definitions = {\n            prop: ToolParameterDefinitionsValue(\n                description=prop_schema[\"description\"]\n                if \"description\" in prop_schema\n                else None,\n                type=prop_schema[\"type\"],\n                required=\"required\" in model_schema[\"parameters\"]\n                and prop in model_schema[\"parameters\"][\"required\"],\n            )\n            for prop, prop_schema in model_schema[\"parameters\"][\n                \"properties\"\n            ].items()\n        }\n    return Tool(\n        name=cls._name(),\n        description=cls._description(),\n        parameter_definitions=parameter_definitions,\n    )\n</code></pre>"},{"location":"api/core/cohere/call/","title":"mirascope.core.cohere.call","text":"<p>The <code>cohere_call</code> decorator for functions as LLM calls.</p>"},{"location":"api/core/cohere/call/#mirascope.core.cohere.call.cohere_call","title":"<code>cohere_call = call_factory(TCallResponse=CohereCallResponse, TCallResponseChunk=CohereCallResponseChunk, TDynamicConfig=CohereDynamicConfig, TMessageParamType=BaseMessageParam, TToolType=CohereTool, TStream=CohereStream, TCallParams=CohereCallParams, default_call_params=CohereCallParams(), setup_call=setup_call, get_json_output=get_json_output, handle_stream=handle_stream, handle_stream_async=handle_stream_async, calculate_cost=calculate_cost, provider='cohere')</code>  <code>module-attribute</code>","text":"<p>A decorator for calling the Cohere API with a typed function.</p> <p>This decorator is used to wrap a typed function that calls the Cohere API. It parses the docstring of the wrapped function as the messages array and templates the input arguments for the function into each message's template.</p> <p>Example:</p> <pre><code>@cohere_call(model=\"gpt-4o\")\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\nresponse = recommend_book(\"fantasy\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>The Cohere model to use in the API call.</p> required <code>stream</code> <p>Whether to stream the response from the API call.</p> required <code>tools</code> <p>The tools to use in the Cohere API call.</p> required <code>**call_params</code> <p>The <code>CohereCallParams</code> call parameters to use in the API call.</p> required <p>Returns:</p> Type Description <p>The decorator for turning a typed function into an Cohere API call.</p>"},{"location":"api/core/cohere/call_params/","title":"mirascope.core.cohere.call_params","text":"<p>The parameters to use when calling the Cohere API.</p>"},{"location":"api/core/cohere/call_params/#mirascope.core.cohere.call_params.CohereCallParams","title":"<code>CohereCallParams</code>","text":"<p>               Bases: <code>BaseCallParams</code></p> <p>The parameters to use when calling the Cohere API.</p> Source code in <code>mirascope/core/cohere/call_params.py</code> <pre><code>class CohereCallParams(BaseCallParams):\n    \"\"\"The parameters to use when calling the Cohere API.\"\"\"\n\n    preamble: NotRequired[str | None]\n    chat_history: NotRequired[Sequence[ChatMessage] | None]\n    conversation_id: NotRequired[str | None]\n    prompt_truncation: NotRequired[ChatRequestPromptTruncation | None]\n    connectors: NotRequired[Sequence[ChatConnector] | None]\n    search_queries_only: NotRequired[bool | None]\n    documents: NotRequired[Sequence[ChatDocument] | None]\n    temperature: NotRequired[float | None]\n    max_tokens: NotRequired[int | None]\n    max_input_tokens: NotRequired[int | None]\n    k: NotRequired[int | None]\n    p: NotRequired[float | None]\n    seed: NotRequired[int | None]\n    stop_sequences: NotRequired[Sequence[str] | None]\n    frequency_penalty: NotRequired[float | None]\n    presence_penalty: NotRequired[float | None]\n    raw_prompting: NotRequired[bool | None]\n    tool_results: NotRequired[Sequence[ToolResult] | None]\n</code></pre>"},{"location":"api/core/cohere/call_response/","title":"mirascope.core.cohere.call_response","text":"<p>This module contains the <code>CohereCallResponse</code> class.</p>"},{"location":"api/core/cohere/call_response/#mirascope.core.cohere.call_response.CohereCallResponse","title":"<code>CohereCallResponse</code>","text":"<p>               Bases: <code>BaseCallResponse[SkipValidation[NonStreamedChatResponse], CohereTool, CohereDynamicConfig, SkipValidation[ChatMessage], CohereCallParams, SkipValidation[ChatMessage]]</code></p> <p>A convenience wrapper around the Cohere <code>ChatCompletion</code> response.</p> <p>When calling the Cohere API using a function decorated with <code>cohere_call</code>, the response will be an <code>CohereCallResponse</code> instance with properties that allow for more convenience access to commonly used attributes.</p> <p>Example:</p> <pre><code>from mirascope.core.cohere import cohere_call\n\n@cohere_call(model=\"gpt-4o\")\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\nresponse = recommend_book(\"fantasy\")  # response is an `CohereCallResponse` instance\nprint(response.content)\n#&gt; Sure! I would recommend...\n</code></pre> Source code in <code>mirascope/core/cohere/call_response.py</code> <pre><code>class CohereCallResponse(\n    BaseCallResponse[\n        SkipValidation[NonStreamedChatResponse],\n        CohereTool,\n        CohereDynamicConfig,\n        SkipValidation[ChatMessage],\n        CohereCallParams,\n        SkipValidation[ChatMessage],\n    ]\n):\n    '''A convenience wrapper around the Cohere `ChatCompletion` response.\n\n    When calling the Cohere API using a function decorated with `cohere_call`, the\n    response will be an `CohereCallResponse` instance with properties that allow for\n    more convenience access to commonly used attributes.\n\n    Example:\n\n    ```python\n    from mirascope.core.cohere import cohere_call\n\n    @cohere_call(model=\"gpt-4o\")\n    def recommend_book(genre: str):\n        \"\"\"Recommend a {genre} book.\"\"\"\n\n    response = recommend_book(\"fantasy\")  # response is an `CohereCallResponse` instance\n    print(response.content)\n    #&gt; Sure! I would recommend...\n    ```\n    '''\n\n    _provider = \"cohere\"\n\n    @computed_field\n    @property\n    def message_param(self) -&gt; ChatMessage:\n        \"\"\"Returns the assistant's response as a message parameter.\"\"\"\n        return ChatMessage(\n            message=self.response.text,\n            tool_calls=self.response.tool_calls,\n            role=\"assistant\",  # type: ignore\n        )\n\n    @property\n    def content(self) -&gt; str:\n        \"\"\"Returns the content of the chat completion for the 0th choice.\"\"\"\n        return self.response.text\n\n    @property\n    def model(self) -&gt; str | None:\n        \"\"\"Returns the name of the response model.\n\n        Cohere does not return model, so we return None\n        \"\"\"\n        return None\n\n    @property\n    def id(self) -&gt; str | None:\n        \"\"\"Returns the id of the response.\"\"\"\n        return self.response.generation_id\n\n    @property\n    def finish_reasons(self) -&gt; list[str] | None:\n        \"\"\"Returns the finish reasons of the response.\"\"\"\n        return [str(self.response.finish_reason)]\n\n    @property\n    def search_queries(self) -&gt; list[ChatSearchQuery] | None:\n        \"\"\"Returns the search queries for the 0th choice message.\"\"\"\n        return self.response.search_queries\n\n    @property\n    def search_results(self) -&gt; list[ChatSearchResult] | None:\n        \"\"\"Returns the search results for the 0th choice message.\"\"\"\n        return self.response.search_results\n\n    @property\n    def documents(self) -&gt; list[ChatDocument] | None:\n        \"\"\"Returns the documents for the 0th choice message.\"\"\"\n        return self.response.documents\n\n    @property\n    def citations(self) -&gt; list[ChatCitation] | None:\n        \"\"\"Returns the citations for the 0th choice message.\"\"\"\n        return self.response.citations\n\n    @property\n    def tool_calls(self) -&gt; list[ToolCall] | None:\n        \"\"\"Returns the tool calls for the 0th choice message.\"\"\"\n        return self.response.tool_calls\n\n    @computed_field\n    @property\n    def tools(self) -&gt; list[CohereTool] | None:\n        \"\"\"Returns the tools for the 0th choice message.\n\n        Raises:\n            ValidationError: if a tool call doesn't match the tool's schema.\n        \"\"\"\n        if not self.tool_types or not self.tool_calls:\n            return None\n\n        extracted_tools = []\n        for tool_call in self.tool_calls:\n            for tool_type in self.tool_types:\n                if tool_call.name == tool_type._name():\n                    extracted_tools.append(tool_type.from_tool_call(tool_call))\n                    break\n\n        return extracted_tools\n\n    @computed_field\n    @property\n    def tool(self) -&gt; CohereTool | None:\n        \"\"\"Returns the 0th tool for the 0th choice message.\n\n        Raises:\n            ValidationError: if the tool call doesn't match the tool's schema.\n        \"\"\"\n        tools = self.tools\n        if tools:\n            return tools[0]\n        return None\n\n    @classmethod\n    def tool_message_params(\n        cls, tools_and_outputs: list[tuple[CohereTool, list[dict[str, Any]]]]\n    ) -&gt; list[ToolResult]:\n        \"\"\"Returns the tool message parameters for tool call results.\"\"\"\n        return [\n            {\"call\": tool.tool_call, \"outputs\": output}  # type: ignore\n            for tool, output in tools_and_outputs\n        ]\n\n    @property\n    def usage(self) -&gt; ApiMetaBilledUnits | None:\n        \"\"\"Returns the usage of the response.\"\"\"\n        if self.response.meta:\n            return self.response.meta.billed_units\n        return None\n\n    @property\n    def input_tokens(self) -&gt; float | None:\n        \"\"\"Returns the number of input tokens.\"\"\"\n        if self.usage:\n            return self.usage.input_tokens\n        return None\n\n    @property\n    def output_tokens(self) -&gt; float | None:\n        \"\"\"Returns the number of output tokens.\"\"\"\n        if self.usage:\n            return self.usage.output_tokens\n        return None\n</code></pre>"},{"location":"api/core/cohere/call_response/#mirascope.core.cohere.call_response.CohereCallResponse.citations","title":"<code>citations: list[ChatCitation] | None</code>  <code>property</code>","text":"<p>Returns the citations for the 0th choice message.</p>"},{"location":"api/core/cohere/call_response/#mirascope.core.cohere.call_response.CohereCallResponse.content","title":"<code>content: str</code>  <code>property</code>","text":"<p>Returns the content of the chat completion for the 0th choice.</p>"},{"location":"api/core/cohere/call_response/#mirascope.core.cohere.call_response.CohereCallResponse.documents","title":"<code>documents: list[ChatDocument] | None</code>  <code>property</code>","text":"<p>Returns the documents for the 0th choice message.</p>"},{"location":"api/core/cohere/call_response/#mirascope.core.cohere.call_response.CohereCallResponse.finish_reasons","title":"<code>finish_reasons: list[str] | None</code>  <code>property</code>","text":"<p>Returns the finish reasons of the response.</p>"},{"location":"api/core/cohere/call_response/#mirascope.core.cohere.call_response.CohereCallResponse.id","title":"<code>id: str | None</code>  <code>property</code>","text":"<p>Returns the id of the response.</p>"},{"location":"api/core/cohere/call_response/#mirascope.core.cohere.call_response.CohereCallResponse.input_tokens","title":"<code>input_tokens: float | None</code>  <code>property</code>","text":"<p>Returns the number of input tokens.</p>"},{"location":"api/core/cohere/call_response/#mirascope.core.cohere.call_response.CohereCallResponse.message_param","title":"<code>message_param: ChatMessage</code>  <code>property</code>","text":"<p>Returns the assistant's response as a message parameter.</p>"},{"location":"api/core/cohere/call_response/#mirascope.core.cohere.call_response.CohereCallResponse.model","title":"<code>model: str | None</code>  <code>property</code>","text":"<p>Returns the name of the response model.</p> <p>Cohere does not return model, so we return None</p>"},{"location":"api/core/cohere/call_response/#mirascope.core.cohere.call_response.CohereCallResponse.output_tokens","title":"<code>output_tokens: float | None</code>  <code>property</code>","text":"<p>Returns the number of output tokens.</p>"},{"location":"api/core/cohere/call_response/#mirascope.core.cohere.call_response.CohereCallResponse.search_queries","title":"<code>search_queries: list[ChatSearchQuery] | None</code>  <code>property</code>","text":"<p>Returns the search queries for the 0th choice message.</p>"},{"location":"api/core/cohere/call_response/#mirascope.core.cohere.call_response.CohereCallResponse.search_results","title":"<code>search_results: list[ChatSearchResult] | None</code>  <code>property</code>","text":"<p>Returns the search results for the 0th choice message.</p>"},{"location":"api/core/cohere/call_response/#mirascope.core.cohere.call_response.CohereCallResponse.tool","title":"<code>tool: CohereTool | None</code>  <code>property</code>","text":"<p>Returns the 0th tool for the 0th choice message.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if the tool call doesn't match the tool's schema.</p>"},{"location":"api/core/cohere/call_response/#mirascope.core.cohere.call_response.CohereCallResponse.tool_calls","title":"<code>tool_calls: list[ToolCall] | None</code>  <code>property</code>","text":"<p>Returns the tool calls for the 0th choice message.</p>"},{"location":"api/core/cohere/call_response/#mirascope.core.cohere.call_response.CohereCallResponse.tools","title":"<code>tools: list[CohereTool] | None</code>  <code>property</code>","text":"<p>Returns the tools for the 0th choice message.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if a tool call doesn't match the tool's schema.</p>"},{"location":"api/core/cohere/call_response/#mirascope.core.cohere.call_response.CohereCallResponse.usage","title":"<code>usage: ApiMetaBilledUnits | None</code>  <code>property</code>","text":"<p>Returns the usage of the response.</p>"},{"location":"api/core/cohere/call_response/#mirascope.core.cohere.call_response.CohereCallResponse.tool_message_params","title":"<code>tool_message_params(tools_and_outputs)</code>  <code>classmethod</code>","text":"<p>Returns the tool message parameters for tool call results.</p> Source code in <code>mirascope/core/cohere/call_response.py</code> <pre><code>@classmethod\ndef tool_message_params(\n    cls, tools_and_outputs: list[tuple[CohereTool, list[dict[str, Any]]]]\n) -&gt; list[ToolResult]:\n    \"\"\"Returns the tool message parameters for tool call results.\"\"\"\n    return [\n        {\"call\": tool.tool_call, \"outputs\": output}  # type: ignore\n        for tool, output in tools_and_outputs\n    ]\n</code></pre>"},{"location":"api/core/cohere/call_response_chunk/","title":"mirascope.core.cohere.call_response_chunk","text":"<p>This module contains the <code>CohereCallResponseChunk</code> class.</p>"},{"location":"api/core/cohere/call_response_chunk/#mirascope.core.cohere.call_response_chunk.CohereCallResponseChunk","title":"<code>CohereCallResponseChunk</code>","text":"<p>               Bases: <code>BaseCallResponseChunk[SkipValidation[StreamedChatResponse]]</code></p> <p>A convenience wrapper around the Cohere <code>ChatCompletionChunk</code> streamed chunks.</p> <p>When calling the Cohere API using a function decorated with <code>cohere_call</code> and <code>stream</code> set to <code>True</code>, the stream will contain <code>CohereResponseChunk</code> instances with properties that allow for more convenient access to commonly used attributes.</p> <p>Example:</p> <pre><code>from mirascope.core.cohere import cohere_call\n\n@cohere_call(model=\"gpt-4o\", stream=True)\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\nstream = recommend_book(\"fantasy\")  # response is an `CohereStream`\nfor chunk in stream:\n    print(chunk.content, end=\"\", flush=True)\n#&gt; Sure! I would recommend...\n</code></pre> Source code in <code>mirascope/core/cohere/call_response_chunk.py</code> <pre><code>class CohereCallResponseChunk(\n    BaseCallResponseChunk[SkipValidation[StreamedChatResponse]]\n):\n    '''A convenience wrapper around the Cohere `ChatCompletionChunk` streamed chunks.\n\n    When calling the Cohere API using a function decorated with `cohere_call` and\n    `stream` set to `True`, the stream will contain `CohereResponseChunk` instances with\n    properties that allow for more convenient access to commonly used attributes.\n\n    Example:\n\n    ```python\n    from mirascope.core.cohere import cohere_call\n\n    @cohere_call(model=\"gpt-4o\", stream=True)\n    def recommend_book(genre: str):\n        \"\"\"Recommend a {genre} book.\"\"\"\n\n    stream = recommend_book(\"fantasy\")  # response is an `CohereStream`\n    for chunk in stream:\n        print(chunk.content, end=\"\", flush=True)\n    #&gt; Sure! I would recommend...\n    ```\n    '''\n\n    @property\n    def event_type(\n        self,\n    ) -&gt; Literal[\n        \"stream-start\",\n        \"search-queries-generation\",\n        \"search-results\",\n        \"text-generation\",\n        \"citation-generation\",\n        \"tool-calls-generation\",\n        \"stream-end\",\n        \"tool-calls-chunk\",\n    ]:\n        \"\"\"Returns the type of the chunk.\"\"\"\n        return self.chunk.event_type\n\n    @property\n    def content(self) -&gt; str:\n        \"\"\"Returns the content for the 0th choice delta.\"\"\"\n        if isinstance(self.chunk, StreamedChatResponse_TextGeneration):\n            return self.chunk.text\n        return \"\"\n\n    @property\n    def search_queries(self) -&gt; list[ChatSearchQuery] | None:\n        \"\"\"Returns the search queries for search-query event type else None.\"\"\"\n        if isinstance(self.chunk, StreamedChatResponse_SearchQueriesGeneration):\n            return self.chunk.search_queries  # type: ignore\n        return None\n\n    @property\n    def search_results(self) -&gt; list[ChatSearchResult] | None:\n        \"\"\"Returns the search results for search-results event type else None.\"\"\"\n        if isinstance(self.chunk, StreamedChatResponse_SearchResults):\n            return self.chunk.search_results\n        return None\n\n    @property\n    def documents(self) -&gt; list[ChatDocument] | None:\n        \"\"\"Returns the documents for search-results event type else None.\"\"\"\n        if isinstance(self.chunk, StreamedChatResponse_SearchResults):\n            return self.chunk.documents\n        return None\n\n    @property\n    def citations(self) -&gt; list[ChatCitation] | None:\n        \"\"\"Returns the citations for citation-generation event type else None.\"\"\"\n        if isinstance(self.chunk, StreamedChatResponse_CitationGeneration):\n            return self.chunk.citations\n        return None\n\n    @property\n    def model(self) -&gt; str | None:\n        \"\"\"Returns the name of the response model.\n\n        Cohere does not return model, so we return None\n        \"\"\"\n        return None\n\n    @property\n    def id(self) -&gt; str | None:\n        \"\"\"Returns the id of the response.\"\"\"\n        if isinstance(self.chunk, StreamedChatResponse_StreamStart):\n            return self.chunk.generation_id\n        return None\n\n    @property\n    def finish_reasons(self) -&gt; list[str] | None:\n        \"\"\"Returns the finish reasons of the response.\"\"\"\n        if isinstance(self.chunk, StreamedChatResponse_StreamEnd):\n            return [str(self.chunk.finish_reason)]\n        return None\n\n    @property\n    def response(self) -&gt; NonStreamedChatResponse | None:\n        \"\"\"Returns the full response for the stream-end event type else None.\"\"\"\n        if isinstance(self.chunk, StreamedChatResponse_StreamEnd):\n            return self.chunk.response\n        return None\n\n    @property\n    def tool_calls(self) -&gt; list[ToolCall] | None:\n        \"\"\"Returns the partial tool calls for the 0th choice message.\"\"\"\n        if isinstance(self.chunk, StreamedChatResponse_ToolCallsGeneration):\n            return self.chunk.tool_calls\n        return None\n\n    @property\n    def usage(self) -&gt; ApiMetaBilledUnits | None:\n        \"\"\"Returns the usage of the response.\"\"\"\n        if isinstance(self.chunk, StreamedChatResponse_StreamEnd):\n            if self.chunk.response.meta:\n                return self.chunk.response.meta.billed_units\n        return None\n\n    @property\n    def input_tokens(self) -&gt; float | None:\n        \"\"\"Returns the number of input tokens.\"\"\"\n        if self.usage:\n            return self.usage.input_tokens\n        return None\n\n    @property\n    def output_tokens(self) -&gt; float | None:\n        \"\"\"Returns the number of output tokens.\"\"\"\n        if self.usage:\n            return self.usage.output_tokens\n        return None\n</code></pre>"},{"location":"api/core/cohere/call_response_chunk/#mirascope.core.cohere.call_response_chunk.CohereCallResponseChunk.citations","title":"<code>citations: list[ChatCitation] | None</code>  <code>property</code>","text":"<p>Returns the citations for citation-generation event type else None.</p>"},{"location":"api/core/cohere/call_response_chunk/#mirascope.core.cohere.call_response_chunk.CohereCallResponseChunk.content","title":"<code>content: str</code>  <code>property</code>","text":"<p>Returns the content for the 0th choice delta.</p>"},{"location":"api/core/cohere/call_response_chunk/#mirascope.core.cohere.call_response_chunk.CohereCallResponseChunk.documents","title":"<code>documents: list[ChatDocument] | None</code>  <code>property</code>","text":"<p>Returns the documents for search-results event type else None.</p>"},{"location":"api/core/cohere/call_response_chunk/#mirascope.core.cohere.call_response_chunk.CohereCallResponseChunk.event_type","title":"<code>event_type: Literal['stream-start', 'search-queries-generation', 'search-results', 'text-generation', 'citation-generation', 'tool-calls-generation', 'stream-end', 'tool-calls-chunk']</code>  <code>property</code>","text":"<p>Returns the type of the chunk.</p>"},{"location":"api/core/cohere/call_response_chunk/#mirascope.core.cohere.call_response_chunk.CohereCallResponseChunk.finish_reasons","title":"<code>finish_reasons: list[str] | None</code>  <code>property</code>","text":"<p>Returns the finish reasons of the response.</p>"},{"location":"api/core/cohere/call_response_chunk/#mirascope.core.cohere.call_response_chunk.CohereCallResponseChunk.id","title":"<code>id: str | None</code>  <code>property</code>","text":"<p>Returns the id of the response.</p>"},{"location":"api/core/cohere/call_response_chunk/#mirascope.core.cohere.call_response_chunk.CohereCallResponseChunk.input_tokens","title":"<code>input_tokens: float | None</code>  <code>property</code>","text":"<p>Returns the number of input tokens.</p>"},{"location":"api/core/cohere/call_response_chunk/#mirascope.core.cohere.call_response_chunk.CohereCallResponseChunk.model","title":"<code>model: str | None</code>  <code>property</code>","text":"<p>Returns the name of the response model.</p> <p>Cohere does not return model, so we return None</p>"},{"location":"api/core/cohere/call_response_chunk/#mirascope.core.cohere.call_response_chunk.CohereCallResponseChunk.output_tokens","title":"<code>output_tokens: float | None</code>  <code>property</code>","text":"<p>Returns the number of output tokens.</p>"},{"location":"api/core/cohere/call_response_chunk/#mirascope.core.cohere.call_response_chunk.CohereCallResponseChunk.response","title":"<code>response: NonStreamedChatResponse | None</code>  <code>property</code>","text":"<p>Returns the full response for the stream-end event type else None.</p>"},{"location":"api/core/cohere/call_response_chunk/#mirascope.core.cohere.call_response_chunk.CohereCallResponseChunk.search_queries","title":"<code>search_queries: list[ChatSearchQuery] | None</code>  <code>property</code>","text":"<p>Returns the search queries for search-query event type else None.</p>"},{"location":"api/core/cohere/call_response_chunk/#mirascope.core.cohere.call_response_chunk.CohereCallResponseChunk.search_results","title":"<code>search_results: list[ChatSearchResult] | None</code>  <code>property</code>","text":"<p>Returns the search results for search-results event type else None.</p>"},{"location":"api/core/cohere/call_response_chunk/#mirascope.core.cohere.call_response_chunk.CohereCallResponseChunk.tool_calls","title":"<code>tool_calls: list[ToolCall] | None</code>  <code>property</code>","text":"<p>Returns the partial tool calls for the 0th choice message.</p>"},{"location":"api/core/cohere/call_response_chunk/#mirascope.core.cohere.call_response_chunk.CohereCallResponseChunk.usage","title":"<code>usage: ApiMetaBilledUnits | None</code>  <code>property</code>","text":"<p>Returns the usage of the response.</p>"},{"location":"api/core/cohere/dynamic_config/","title":"mirascope.core.cohere.dynamic_config","text":"<p>This module defines the function return type for functions as LLM calls.</p>"},{"location":"api/core/cohere/dynamic_config/#mirascope.core.cohere.dynamic_config.CohereDynamicConfig","title":"<code>CohereDynamicConfig = BaseDynamicConfig[BaseMessageParam, CohereCallParams]</code>  <code>module-attribute</code>","text":"<p>The function return type for functions wrapped with the <code>cohere_call</code> decorator.</p> <p>Attributes:</p> Name Type Description <code>computed_fields</code> <p>The computed fields to use in the prompt template.</p> <code>messages</code> <p>The messages to send to the Cohere API. If provided, <code>computed_fields</code> will be ignored.</p> <code>call_params</code> <p>The call parameters to use when calling the Cohere API. These will override any call parameters provided to the decorator.</p> <p>Example:</p> <pre><code>from mirascope.core.cohere import CohereDynamicConfig, cohere_call\n\n@cohere_call(model=\"gpt-4o\")\ndef recommend_book(genre: str) -&gt; CohereDynamicConfig:\n    \"\"\"Recommend a {capitalized_genre} book.\"\"\"\n    reeturn {\"computed_fields\": {\"capitalized_genre\": genre.capitalize()}}\n</code></pre>"},{"location":"api/core/cohere/tool/","title":"mirascope.core.cohere.tool","text":"<p>The <code>CohereTool</code> class for easy tool usage with Cohere LLM calls.</p>"},{"location":"api/core/cohere/tool/#mirascope.core.cohere.tool.CohereTool","title":"<code>CohereTool</code>","text":"<p>               Bases: <code>BaseTool</code></p> <p>A class for defining tools for Cohere LLM calls.</p> Source code in <code>mirascope/core/cohere/tool.py</code> <pre><code>class CohereTool(BaseTool):\n    \"\"\"A class for defining tools for Cohere LLM calls.\"\"\"\n\n    tool_call: SkipJsonSchema[SkipValidation[ToolCall]]\n\n    @classmethod\n    def tool_schema(cls) -&gt; Tool:\n        \"\"\"Constructs a JSON Schema tool schema from the `BaseModel` schema defined.\"\"\"\n        model_schema = cls.model_json_schema()\n        model_schema.pop(\"title\", None)\n        model_schema.pop(\"description\", None)\n        parameter_definitions = None\n        if \"parameters\" in model_schema:\n            if \"$defs\" in model_schema[\"parameters\"]:\n                raise ValueError(\n                    \"Unfortunately Cohere's chat API cannot handle nested structures \"\n                    \"with $defs.\"\n                )\n            parameter_definitions = {\n                prop: ToolParameterDefinitionsValue(\n                    description=prop_schema[\"description\"]\n                    if \"description\" in prop_schema\n                    else None,\n                    type=prop_schema[\"type\"],\n                    required=\"required\" in model_schema[\"parameters\"]\n                    and prop in model_schema[\"parameters\"][\"required\"],\n                )\n                for prop, prop_schema in model_schema[\"parameters\"][\n                    \"properties\"\n                ].items()\n            }\n        return Tool(\n            name=cls._name(),\n            description=cls._description(),\n            parameter_definitions=parameter_definitions,\n        )\n\n    @classmethod\n    def from_tool_call(cls, tool_call: ToolCall) -&gt; CohereTool:\n        \"\"\"Constructs an `CohereTool` instance from a `tool_call`.\"\"\"\n        model_json = tool_call.parameters\n        model_json[\"tool_call\"] = tool_call.model_dump()\n        return cls.model_validate(model_json)\n</code></pre>"},{"location":"api/core/cohere/tool/#mirascope.core.cohere.tool.CohereTool.from_tool_call","title":"<code>from_tool_call(tool_call)</code>  <code>classmethod</code>","text":"<p>Constructs an <code>CohereTool</code> instance from a <code>tool_call</code>.</p> Source code in <code>mirascope/core/cohere/tool.py</code> <pre><code>@classmethod\ndef from_tool_call(cls, tool_call: ToolCall) -&gt; CohereTool:\n    \"\"\"Constructs an `CohereTool` instance from a `tool_call`.\"\"\"\n    model_json = tool_call.parameters\n    model_json[\"tool_call\"] = tool_call.model_dump()\n    return cls.model_validate(model_json)\n</code></pre>"},{"location":"api/core/cohere/tool/#mirascope.core.cohere.tool.CohereTool.tool_schema","title":"<code>tool_schema()</code>  <code>classmethod</code>","text":"<p>Constructs a JSON Schema tool schema from the <code>BaseModel</code> schema defined.</p> Source code in <code>mirascope/core/cohere/tool.py</code> <pre><code>@classmethod\ndef tool_schema(cls) -&gt; Tool:\n    \"\"\"Constructs a JSON Schema tool schema from the `BaseModel` schema defined.\"\"\"\n    model_schema = cls.model_json_schema()\n    model_schema.pop(\"title\", None)\n    model_schema.pop(\"description\", None)\n    parameter_definitions = None\n    if \"parameters\" in model_schema:\n        if \"$defs\" in model_schema[\"parameters\"]:\n            raise ValueError(\n                \"Unfortunately Cohere's chat API cannot handle nested structures \"\n                \"with $defs.\"\n            )\n        parameter_definitions = {\n            prop: ToolParameterDefinitionsValue(\n                description=prop_schema[\"description\"]\n                if \"description\" in prop_schema\n                else None,\n                type=prop_schema[\"type\"],\n                required=\"required\" in model_schema[\"parameters\"]\n                and prop in model_schema[\"parameters\"][\"required\"],\n            )\n            for prop, prop_schema in model_schema[\"parameters\"][\n                \"properties\"\n            ].items()\n        }\n    return Tool(\n        name=cls._name(),\n        description=cls._description(),\n        parameter_definitions=parameter_definitions,\n    )\n</code></pre>"},{"location":"api/core/gemini/","title":"mirascope.core.gemini","text":"<p>The Mirascope Gemini Module.</p>"},{"location":"api/core/gemini/#mirascope.core.gemini.GeminiDynamicConfig","title":"<code>GeminiDynamicConfig = BaseDynamicConfig[ContentsType, GeminiCallParams]</code>  <code>module-attribute</code>","text":"<p>The function return type for functions wrapped with the <code>gemini_call</code> decorator.</p> <p>Attributes:</p> Name Type Description <code>computed_fields</code> <p>The computed fields to use in the prompt template.</p> <code>messages</code> <p>The messages to send to the Gemini API. If provided, <code>computed_fields</code> will be ignored.</p> <code>call_params</code> <p>The call parameters to use when calling the Gemini API. These will override any call parameters provided to the decorator.</p> <p>Example:</p> <pre><code>from mirascope.core.gemini import GeminiDynamicConfig, gemini_call\n\n@gemini_call(model=\"gemini-pro\")\ndef recommend_book(genre: str) -&gt; GeminiDynamicConfig:\n    \"\"\"Recommend a {capitalized_genre} book.\"\"\"\n    reeturn {\"computed_fields\": {\"capitalized_genre\": genre.capitalize()}}\n</code></pre>"},{"location":"api/core/gemini/#mirascope.core.gemini.gemini_call","title":"<code>gemini_call = call_factory(TCallResponse=GeminiCallResponse, TCallResponseChunk=GeminiCallResponseChunk, TDynamicConfig=GeminiDynamicConfig, TStream=GeminiStream, TMessageParamType=ContentDict, TToolType=GeminiTool, TCallParams=GeminiCallParams, default_call_params=GeminiCallParams(), setup_call=setup_call, get_json_output=get_json_output, handle_stream=handle_stream, handle_stream_async=handle_stream_async, calculate_cost=calculate_cost, provider='gemini')</code>  <code>module-attribute</code>","text":"<p>A decorator for calling the Gemini API with a typed function.</p> <p>This decorator is used to wrap a typed function that calls the Gemini API. It parses the docstring of the wrapped function as the messages array and templates the input arguments for the function into each message's template.</p> <p>Example:</p> <pre><code>@gemini_call(model=\"gemini-1.5-pro\")\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\nresponse = recommend_book(\"fantasy\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>The Gemini model to use in the API call.</p> required <code>stream</code> <p>Whether to stream the response from the API call.</p> required <code>tools</code> <p>The tools to use in the Gemini API call.</p> required <code>**call_params</code> <p>The <code>GeminiCallParams</code> call parameters to use in the API call.</p> required <p>Returns:</p> Type Description <p>The decorator for turning a typed function into an Gemini API call.</p>"},{"location":"api/core/gemini/#mirascope.core.gemini.GeminiCallParams","title":"<code>GeminiCallParams</code>","text":"<p>               Bases: <code>BaseCallParams</code></p> <p>The parameters to use when calling the Gemini API.</p> Source code in <code>mirascope/core/gemini/call_params.py</code> <pre><code>class GeminiCallParams(BaseCallParams):\n    \"\"\"The parameters to use when calling the Gemini API.\"\"\"\n\n    generation_config: NotRequired[GenerationConfig]\n    safety_settings: NotRequired[SafetySettingOptions]\n    request_options: NotRequired[RequestOptions]\n    tool_config: NotRequired[ToolConfigType]\n</code></pre>"},{"location":"api/core/gemini/#mirascope.core.gemini.GeminiCallResponse","title":"<code>GeminiCallResponse</code>","text":"<p>               Bases: <code>BaseCallResponse[GenerateContentResponse | AsyncGenerateContentResponse, GeminiTool, GeminiDynamicConfig, ContentsType, GeminiCallParams, ContentDict]</code></p> <p>Convenience wrapper around Gemini's <code>GenerateContentResponse</code>.</p> <p>When using Mirascope's convenience wrappers to interact with Gemini models via <code>GeminiCall</code>, responses using <code>GeminiCall.call()</code> will return a <code>GeminiCallResponse</code>, whereby the implemented properties allow for simpler syntax and a convenient developer experience.</p> <p>Example:</p> <pre><code>from mirascope.core.gemini import gemini_call\n\n@gemini_call(model=\"gemini-1.5-pro\")\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\nresponse = recommend_book(\"fantasy\")  # response is an `GeminiCallResponse` instance\nprint(response.content)\n#&gt; Sure! I would recommend...\n</code></pre> Source code in <code>mirascope/core/gemini/call_response.py</code> <pre><code>class GeminiCallResponse(\n    BaseCallResponse[\n        GenerateContentResponse | AsyncGenerateContentResponse,\n        GeminiTool,\n        GeminiDynamicConfig,\n        ContentsType,\n        GeminiCallParams,\n        ContentDict,\n    ]\n):\n    '''Convenience wrapper around Gemini's `GenerateContentResponse`.\n\n    When using Mirascope's convenience wrappers to interact with Gemini models via\n    `GeminiCall`, responses using `GeminiCall.call()` will return a\n    `GeminiCallResponse`, whereby the implemented properties allow for simpler syntax\n    and a convenient developer experience.\n\n    Example:\n\n    ```python\n    from mirascope.core.gemini import gemini_call\n\n    @gemini_call(model=\"gemini-1.5-pro\")\n    def recommend_book(genre: str):\n        \"\"\"Recommend a {genre} book.\"\"\"\n\n    response = recommend_book(\"fantasy\")  # response is an `GeminiCallResponse` instance\n    print(response.content)\n    #&gt; Sure! I would recommend...\n    ```\n    '''\n\n    _provider = \"gemini\"\n\n    @computed_field\n    @property\n    def message_param(self) -&gt; ContentDict:\n        \"\"\"Returns the models's response as a message parameter.\"\"\"\n        return {\"role\": \"model\", \"parts\": self.response.parts}  # type: ignore\n\n    @computed_field\n    @property\n    def tools(self) -&gt; list[GeminiTool] | None:\n        \"\"\"Returns the list of tools for the 0th candidate's 0th content part.\"\"\"\n        if self.tool_types is None:\n            return None\n\n        if self.finish_reasons[0] != \"STOP\":\n            raise RuntimeError(\n                \"Generation stopped before the stop sequence. \"\n                \"This is likely due to a limit on output tokens that is too low. \"\n                \"Note that this could also indicate no tool is beind called, so we \"\n                \"recommend that you check the output of the call to confirm.\"\n                f\"Finish Reason: {self.finish_reasons[0]}\"\n            )\n\n        extracted_tools = []\n        for tool_call in self.tool_calls:\n            for tool_type in self.tool_types:\n                if tool_call.name == tool_type._name():\n                    extracted_tools.append(tool_type.from_tool_call(tool_call))\n                    break\n\n        return extracted_tools\n\n    @computed_field\n    @property\n    def tool(self) -&gt; GeminiTool | None:\n        \"\"\"Returns the 0th tool for the 0th candidate's 0th content part.\n\n        Raises:\n            ValidationError: if the tool call doesn't match the tool's schema.\n        \"\"\"\n        tools = self.tools\n        if tools:\n            return tools[0]\n        return None\n\n    @classmethod\n    def tool_message_params(\n        cls, tools_and_outputs: list[tuple[GeminiTool, object]]\n    ) -&gt; list[FunctionResponse]:\n        \"\"\"Returns the tool message parameters for tool call results.\"\"\"\n        return [\n            FunctionResponse(name=tool._name(), response={\"result\": output})\n            for tool, output in tools_and_outputs\n        ]\n\n    @property\n    def tool_calls(self) -&gt; list[FunctionCall]:\n        \"\"\"Returns the tool calls for the 0th candidate content.\"\"\"\n        return [\n            part.function_call for part in self.response.candidates[0].content.parts\n        ]\n\n    @property\n    def content(self) -&gt; str:\n        \"\"\"Returns the contained string content for the 0th choice.\"\"\"\n        return self.response.candidates[0].content.parts[0].text\n\n    @property\n    def id(self) -&gt; Optional[str]:\n        \"\"\"Returns the id of the response.\n\n        google.generativeai does not return an id\n        \"\"\"\n        return None\n\n    @property\n    def finish_reasons(self) -&gt; list[str]:\n        \"\"\"Returns the finish reasons of the response.\"\"\"\n        finish_reasons = [\n            \"FINISH_REASON_UNSPECIFIED\",\n            \"STOP\",\n            \"MAX_TOKENS\",\n            \"SAFETY\",\n            \"RECITATION\",\n            \"OTHER\",\n        ]\n\n        return [\n            finish_reasons[candidate.finish_reason]\n            for candidate in self.response.candidates\n        ]\n\n    @property\n    def model(self) -&gt; None:\n        \"\"\"Returns the model name.\n\n        google.generativeai does not return model, so we return None\n        \"\"\"\n        return None\n\n    @property\n    def usage(self) -&gt; None:\n        \"\"\"Returns the usage of the chat completion.\n\n        google.generativeai does not have Usage, so we return None\n        \"\"\"\n        return None\n\n    @property\n    def input_tokens(self) -&gt; None:\n        \"\"\"Returns the number of input tokens.\"\"\"\n        return None\n\n    @property\n    def output_tokens(self) -&gt; None:\n        \"\"\"Returns the number of output tokens.\"\"\"\n        return None\n</code></pre>"},{"location":"api/core/gemini/#mirascope.core.gemini.GeminiCallResponse.content","title":"<code>content: str</code>  <code>property</code>","text":"<p>Returns the contained string content for the 0th choice.</p>"},{"location":"api/core/gemini/#mirascope.core.gemini.GeminiCallResponse.finish_reasons","title":"<code>finish_reasons: list[str]</code>  <code>property</code>","text":"<p>Returns the finish reasons of the response.</p>"},{"location":"api/core/gemini/#mirascope.core.gemini.GeminiCallResponse.id","title":"<code>id: Optional[str]</code>  <code>property</code>","text":"<p>Returns the id of the response.</p> <p>google.generativeai does not return an id</p>"},{"location":"api/core/gemini/#mirascope.core.gemini.GeminiCallResponse.input_tokens","title":"<code>input_tokens: None</code>  <code>property</code>","text":"<p>Returns the number of input tokens.</p>"},{"location":"api/core/gemini/#mirascope.core.gemini.GeminiCallResponse.message_param","title":"<code>message_param: ContentDict</code>  <code>property</code>","text":"<p>Returns the models's response as a message parameter.</p>"},{"location":"api/core/gemini/#mirascope.core.gemini.GeminiCallResponse.model","title":"<code>model: None</code>  <code>property</code>","text":"<p>Returns the model name.</p> <p>google.generativeai does not return model, so we return None</p>"},{"location":"api/core/gemini/#mirascope.core.gemini.GeminiCallResponse.output_tokens","title":"<code>output_tokens: None</code>  <code>property</code>","text":"<p>Returns the number of output tokens.</p>"},{"location":"api/core/gemini/#mirascope.core.gemini.GeminiCallResponse.tool","title":"<code>tool: GeminiTool | None</code>  <code>property</code>","text":"<p>Returns the 0th tool for the 0th candidate's 0th content part.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if the tool call doesn't match the tool's schema.</p>"},{"location":"api/core/gemini/#mirascope.core.gemini.GeminiCallResponse.tool_calls","title":"<code>tool_calls: list[FunctionCall]</code>  <code>property</code>","text":"<p>Returns the tool calls for the 0th candidate content.</p>"},{"location":"api/core/gemini/#mirascope.core.gemini.GeminiCallResponse.tools","title":"<code>tools: list[GeminiTool] | None</code>  <code>property</code>","text":"<p>Returns the list of tools for the 0th candidate's 0th content part.</p>"},{"location":"api/core/gemini/#mirascope.core.gemini.GeminiCallResponse.usage","title":"<code>usage: None</code>  <code>property</code>","text":"<p>Returns the usage of the chat completion.</p> <p>google.generativeai does not have Usage, so we return None</p>"},{"location":"api/core/gemini/#mirascope.core.gemini.GeminiCallResponse.tool_message_params","title":"<code>tool_message_params(tools_and_outputs)</code>  <code>classmethod</code>","text":"<p>Returns the tool message parameters for tool call results.</p> Source code in <code>mirascope/core/gemini/call_response.py</code> <pre><code>@classmethod\ndef tool_message_params(\n    cls, tools_and_outputs: list[tuple[GeminiTool, object]]\n) -&gt; list[FunctionResponse]:\n    \"\"\"Returns the tool message parameters for tool call results.\"\"\"\n    return [\n        FunctionResponse(name=tool._name(), response={\"result\": output})\n        for tool, output in tools_and_outputs\n    ]\n</code></pre>"},{"location":"api/core/gemini/#mirascope.core.gemini.GeminiCallResponseChunk","title":"<code>GeminiCallResponseChunk</code>","text":"<p>               Bases: <code>BaseCallResponseChunk[GenerateContentResponse]</code></p> <p>Convenience wrapper around chat completion streaming chunks.</p> <p>When using Mirascope's convenience wrappers to interact with Gemini models via <code>GeminiCall</code>, responses using <code>GeminiCall.stream()</code> will return a <code>GeminiCallResponseChunk</code>, whereby the implemented properties allow for simpler syntax and a convenient developer experience.</p> <p>Example:</p> <pre><code>from mirascope.gemini import GeminiCall\n\n\nclass Math(GeminiCall):\n    prompt_template = \"What is 1 + 2?\"\n\n\ncontent = \"\"\nfor chunk in Math().stream():\n    content += chunk.content\n    print(content)\n#&gt; 1\n#  1 +\n#  1 + 2\n#  1 + 2 equals\n#  1 + 2 equals\n#  1 + 2 equals 3\n#  1 + 2 equals 3.\n</code></pre> Source code in <code>mirascope/core/gemini/call_response_chunk.py</code> <pre><code>class GeminiCallResponseChunk(BaseCallResponseChunk[GenerateContentResponse]):\n    \"\"\"Convenience wrapper around chat completion streaming chunks.\n\n    When using Mirascope's convenience wrappers to interact with Gemini models via\n    `GeminiCall`, responses using `GeminiCall.stream()` will return a\n    `GeminiCallResponseChunk`, whereby the implemented properties allow for simpler\n    syntax and a convenient developer experience.\n\n    Example:\n\n    ```python\n    from mirascope.gemini import GeminiCall\n\n\n    class Math(GeminiCall):\n        prompt_template = \"What is 1 + 2?\"\n\n\n    content = \"\"\n    for chunk in Math().stream():\n        content += chunk.content\n        print(content)\n    #&gt; 1\n    #  1 +\n    #  1 + 2\n    #  1 + 2 equals\n    #  1 + 2 equals\n    #  1 + 2 equals 3\n    #  1 + 2 equals 3.\n    ```\n    \"\"\"\n\n    @property\n    def content(self) -&gt; str:\n        \"\"\"Returns the chunk content for the 0th choice.\"\"\"\n        return self.chunk.candidates[0].content.parts[0].text\n\n    @property\n    def id(self) -&gt; str | None:\n        \"\"\"Returns the id of the response.\n\n        google.generativeai does not return an id\n        \"\"\"\n        return None\n\n    @property\n    def finish_reasons(self) -&gt; list[str]:\n        \"\"\"Returns the finish reasons of the response.\"\"\"\n        finish_reasons = [\n            \"FINISH_REASON_UNSPECIFIED\",\n            \"STOP\",\n            \"MAX_TOKENS\",\n            \"SAFETY\",\n            \"RECITATION\",\n            \"OTHER\",\n        ]\n\n        return [\n            finish_reasons[candidate.finish_reason]\n            for candidate in self.chunk.candidates\n        ]\n\n    @property\n    def model(self) -&gt; None:\n        \"\"\"Returns the model name.\n\n        google.generativeai does not return model, so we return None\n        \"\"\"\n        return None\n\n    @property\n    def usage(self) -&gt; None:\n        \"\"\"Returns the usage of the chat completion.\n\n        google.generativeai does not have Usage, so we return None\n        \"\"\"\n        return None\n\n    @property\n    def input_tokens(self) -&gt; None:\n        \"\"\"Returns the number of input tokens.\"\"\"\n        return None\n\n    @property\n    def output_tokens(self) -&gt; None:\n        \"\"\"Returns the number of output tokens.\"\"\"\n        return None\n</code></pre>"},{"location":"api/core/gemini/#mirascope.core.gemini.GeminiCallResponseChunk.content","title":"<code>content: str</code>  <code>property</code>","text":"<p>Returns the chunk content for the 0th choice.</p>"},{"location":"api/core/gemini/#mirascope.core.gemini.GeminiCallResponseChunk.finish_reasons","title":"<code>finish_reasons: list[str]</code>  <code>property</code>","text":"<p>Returns the finish reasons of the response.</p>"},{"location":"api/core/gemini/#mirascope.core.gemini.GeminiCallResponseChunk.id","title":"<code>id: str | None</code>  <code>property</code>","text":"<p>Returns the id of the response.</p> <p>google.generativeai does not return an id</p>"},{"location":"api/core/gemini/#mirascope.core.gemini.GeminiCallResponseChunk.input_tokens","title":"<code>input_tokens: None</code>  <code>property</code>","text":"<p>Returns the number of input tokens.</p>"},{"location":"api/core/gemini/#mirascope.core.gemini.GeminiCallResponseChunk.model","title":"<code>model: None</code>  <code>property</code>","text":"<p>Returns the model name.</p> <p>google.generativeai does not return model, so we return None</p>"},{"location":"api/core/gemini/#mirascope.core.gemini.GeminiCallResponseChunk.output_tokens","title":"<code>output_tokens: None</code>  <code>property</code>","text":"<p>Returns the number of output tokens.</p>"},{"location":"api/core/gemini/#mirascope.core.gemini.GeminiCallResponseChunk.usage","title":"<code>usage: None</code>  <code>property</code>","text":"<p>Returns the usage of the chat completion.</p> <p>google.generativeai does not have Usage, so we return None</p>"},{"location":"api/core/gemini/#mirascope.core.gemini.GeminiTool","title":"<code>GeminiTool</code>","text":"<p>               Bases: <code>BaseTool</code></p> <p>A class for defining tools for Gemini LLM calls.</p> Source code in <code>mirascope/core/gemini/tool.py</code> <pre><code>class GeminiTool(BaseTool):\n    \"\"\"A class for defining tools for Gemini LLM calls.\"\"\"\n\n    tool_call: SkipJsonSchema[FunctionCall]\n\n    @classmethod\n    def tool_schema(cls) -&gt; Tool:\n        \"\"\"Constructs a JSON Schema tool schema from the `BaseModel` schema defined.\"\"\"\n        model_schema = cls.model_json_schema()\n        model_schema.pop(\"title\", None)\n        model_schema.pop(\"description\", None)\n\n        fn: dict[str, Any] = {\"name\": cls._name(), \"description\": cls._description()}\n        if model_schema[\"properties\"]:\n            fn[\"parameters\"] = model_schema  # type: ignore\n        if model_schema[\"required\"]:\n            fn[\"parameters\"][\"required\"] = model_schema[\"required\"]\n        if \"parameters\" in fn:\n            if \"$defs\" in fn[\"parameters\"]:\n                raise ValueError(\n                    \"Unfortunately Google's Gemini API cannot handle nested structures \"\n                    \"with $defs.\"\n                )\n\n            def handle_enum_schema(prop_schema):\n                if \"enum\" in prop_schema:\n                    prop_schema[\"format\"] = \"enum\"\n                return prop_schema\n\n            fn[\"parameters\"][\"properties\"] = {\n                prop: {\n                    key: value\n                    for key, value in handle_enum_schema(prop_schema).items()\n                    if key != \"title\"\n                }\n                for prop, prop_schema in fn[\"parameters\"][\"properties\"].items()\n            }\n        return Tool(function_declarations=[FunctionDeclaration(**fn)])\n\n    @classmethod\n    def from_tool_call(cls, tool_call: FunctionCall) -&gt; GeminiTool:\n        \"\"\"Constructs an `GeminiTool` instance from a `tool_call`.\"\"\"\n        if not tool_call.args:\n            raise ValueError(\"Tool call doesn't have any arguments.\")\n        model_json: dict[str, Any] = {\n            key: value for key, value in tool_call.args.items()\n        }\n        model_json[\"tool_call\"] = tool_call\n        return cls.model_validate(model_json)\n</code></pre>"},{"location":"api/core/gemini/#mirascope.core.gemini.GeminiTool.from_tool_call","title":"<code>from_tool_call(tool_call)</code>  <code>classmethod</code>","text":"<p>Constructs an <code>GeminiTool</code> instance from a <code>tool_call</code>.</p> Source code in <code>mirascope/core/gemini/tool.py</code> <pre><code>@classmethod\ndef from_tool_call(cls, tool_call: FunctionCall) -&gt; GeminiTool:\n    \"\"\"Constructs an `GeminiTool` instance from a `tool_call`.\"\"\"\n    if not tool_call.args:\n        raise ValueError(\"Tool call doesn't have any arguments.\")\n    model_json: dict[str, Any] = {\n        key: value for key, value in tool_call.args.items()\n    }\n    model_json[\"tool_call\"] = tool_call\n    return cls.model_validate(model_json)\n</code></pre>"},{"location":"api/core/gemini/#mirascope.core.gemini.GeminiTool.tool_schema","title":"<code>tool_schema()</code>  <code>classmethod</code>","text":"<p>Constructs a JSON Schema tool schema from the <code>BaseModel</code> schema defined.</p> Source code in <code>mirascope/core/gemini/tool.py</code> <pre><code>@classmethod\ndef tool_schema(cls) -&gt; Tool:\n    \"\"\"Constructs a JSON Schema tool schema from the `BaseModel` schema defined.\"\"\"\n    model_schema = cls.model_json_schema()\n    model_schema.pop(\"title\", None)\n    model_schema.pop(\"description\", None)\n\n    fn: dict[str, Any] = {\"name\": cls._name(), \"description\": cls._description()}\n    if model_schema[\"properties\"]:\n        fn[\"parameters\"] = model_schema  # type: ignore\n    if model_schema[\"required\"]:\n        fn[\"parameters\"][\"required\"] = model_schema[\"required\"]\n    if \"parameters\" in fn:\n        if \"$defs\" in fn[\"parameters\"]:\n            raise ValueError(\n                \"Unfortunately Google's Gemini API cannot handle nested structures \"\n                \"with $defs.\"\n            )\n\n        def handle_enum_schema(prop_schema):\n            if \"enum\" in prop_schema:\n                prop_schema[\"format\"] = \"enum\"\n            return prop_schema\n\n        fn[\"parameters\"][\"properties\"] = {\n            prop: {\n                key: value\n                for key, value in handle_enum_schema(prop_schema).items()\n                if key != \"title\"\n            }\n            for prop, prop_schema in fn[\"parameters\"][\"properties\"].items()\n        }\n    return Tool(function_declarations=[FunctionDeclaration(**fn)])\n</code></pre>"},{"location":"api/core/gemini/call/","title":"mirascope.core.gemini.call","text":"<p>The <code>gemini_call</code> decorator for functions as LLM calls.</p>"},{"location":"api/core/gemini/call/#mirascope.core.gemini.call.gemini_call","title":"<code>gemini_call = call_factory(TCallResponse=GeminiCallResponse, TCallResponseChunk=GeminiCallResponseChunk, TDynamicConfig=GeminiDynamicConfig, TStream=GeminiStream, TMessageParamType=ContentDict, TToolType=GeminiTool, TCallParams=GeminiCallParams, default_call_params=GeminiCallParams(), setup_call=setup_call, get_json_output=get_json_output, handle_stream=handle_stream, handle_stream_async=handle_stream_async, calculate_cost=calculate_cost, provider='gemini')</code>  <code>module-attribute</code>","text":"<p>A decorator for calling the Gemini API with a typed function.</p> <p>This decorator is used to wrap a typed function that calls the Gemini API. It parses the docstring of the wrapped function as the messages array and templates the input arguments for the function into each message's template.</p> <p>Example:</p> <pre><code>@gemini_call(model=\"gemini-1.5-pro\")\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\nresponse = recommend_book(\"fantasy\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>The Gemini model to use in the API call.</p> required <code>stream</code> <p>Whether to stream the response from the API call.</p> required <code>tools</code> <p>The tools to use in the Gemini API call.</p> required <code>**call_params</code> <p>The <code>GeminiCallParams</code> call parameters to use in the API call.</p> required <p>Returns:</p> Type Description <p>The decorator for turning a typed function into an Gemini API call.</p>"},{"location":"api/core/gemini/call_params/","title":"mirascope.core.gemini.call_params","text":"<p>The parameters to use when calling the Gemini API.</p>"},{"location":"api/core/gemini/call_params/#mirascope.core.gemini.call_params.GeminiCallParams","title":"<code>GeminiCallParams</code>","text":"<p>               Bases: <code>BaseCallParams</code></p> <p>The parameters to use when calling the Gemini API.</p> Source code in <code>mirascope/core/gemini/call_params.py</code> <pre><code>class GeminiCallParams(BaseCallParams):\n    \"\"\"The parameters to use when calling the Gemini API.\"\"\"\n\n    generation_config: NotRequired[GenerationConfig]\n    safety_settings: NotRequired[SafetySettingOptions]\n    request_options: NotRequired[RequestOptions]\n    tool_config: NotRequired[ToolConfigType]\n</code></pre>"},{"location":"api/core/gemini/call_response/","title":"mirascope.core.gemini.call_response","text":"<p>This module contains the <code>GeminiCallResponse</code> class.</p>"},{"location":"api/core/gemini/call_response/#mirascope.core.gemini.call_response.GeminiCallResponse","title":"<code>GeminiCallResponse</code>","text":"<p>               Bases: <code>BaseCallResponse[GenerateContentResponse | AsyncGenerateContentResponse, GeminiTool, GeminiDynamicConfig, ContentsType, GeminiCallParams, ContentDict]</code></p> <p>Convenience wrapper around Gemini's <code>GenerateContentResponse</code>.</p> <p>When using Mirascope's convenience wrappers to interact with Gemini models via <code>GeminiCall</code>, responses using <code>GeminiCall.call()</code> will return a <code>GeminiCallResponse</code>, whereby the implemented properties allow for simpler syntax and a convenient developer experience.</p> <p>Example:</p> <pre><code>from mirascope.core.gemini import gemini_call\n\n@gemini_call(model=\"gemini-1.5-pro\")\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\nresponse = recommend_book(\"fantasy\")  # response is an `GeminiCallResponse` instance\nprint(response.content)\n#&gt; Sure! I would recommend...\n</code></pre> Source code in <code>mirascope/core/gemini/call_response.py</code> <pre><code>class GeminiCallResponse(\n    BaseCallResponse[\n        GenerateContentResponse | AsyncGenerateContentResponse,\n        GeminiTool,\n        GeminiDynamicConfig,\n        ContentsType,\n        GeminiCallParams,\n        ContentDict,\n    ]\n):\n    '''Convenience wrapper around Gemini's `GenerateContentResponse`.\n\n    When using Mirascope's convenience wrappers to interact with Gemini models via\n    `GeminiCall`, responses using `GeminiCall.call()` will return a\n    `GeminiCallResponse`, whereby the implemented properties allow for simpler syntax\n    and a convenient developer experience.\n\n    Example:\n\n    ```python\n    from mirascope.core.gemini import gemini_call\n\n    @gemini_call(model=\"gemini-1.5-pro\")\n    def recommend_book(genre: str):\n        \"\"\"Recommend a {genre} book.\"\"\"\n\n    response = recommend_book(\"fantasy\")  # response is an `GeminiCallResponse` instance\n    print(response.content)\n    #&gt; Sure! I would recommend...\n    ```\n    '''\n\n    _provider = \"gemini\"\n\n    @computed_field\n    @property\n    def message_param(self) -&gt; ContentDict:\n        \"\"\"Returns the models's response as a message parameter.\"\"\"\n        return {\"role\": \"model\", \"parts\": self.response.parts}  # type: ignore\n\n    @computed_field\n    @property\n    def tools(self) -&gt; list[GeminiTool] | None:\n        \"\"\"Returns the list of tools for the 0th candidate's 0th content part.\"\"\"\n        if self.tool_types is None:\n            return None\n\n        if self.finish_reasons[0] != \"STOP\":\n            raise RuntimeError(\n                \"Generation stopped before the stop sequence. \"\n                \"This is likely due to a limit on output tokens that is too low. \"\n                \"Note that this could also indicate no tool is beind called, so we \"\n                \"recommend that you check the output of the call to confirm.\"\n                f\"Finish Reason: {self.finish_reasons[0]}\"\n            )\n\n        extracted_tools = []\n        for tool_call in self.tool_calls:\n            for tool_type in self.tool_types:\n                if tool_call.name == tool_type._name():\n                    extracted_tools.append(tool_type.from_tool_call(tool_call))\n                    break\n\n        return extracted_tools\n\n    @computed_field\n    @property\n    def tool(self) -&gt; GeminiTool | None:\n        \"\"\"Returns the 0th tool for the 0th candidate's 0th content part.\n\n        Raises:\n            ValidationError: if the tool call doesn't match the tool's schema.\n        \"\"\"\n        tools = self.tools\n        if tools:\n            return tools[0]\n        return None\n\n    @classmethod\n    def tool_message_params(\n        cls, tools_and_outputs: list[tuple[GeminiTool, object]]\n    ) -&gt; list[FunctionResponse]:\n        \"\"\"Returns the tool message parameters for tool call results.\"\"\"\n        return [\n            FunctionResponse(name=tool._name(), response={\"result\": output})\n            for tool, output in tools_and_outputs\n        ]\n\n    @property\n    def tool_calls(self) -&gt; list[FunctionCall]:\n        \"\"\"Returns the tool calls for the 0th candidate content.\"\"\"\n        return [\n            part.function_call for part in self.response.candidates[0].content.parts\n        ]\n\n    @property\n    def content(self) -&gt; str:\n        \"\"\"Returns the contained string content for the 0th choice.\"\"\"\n        return self.response.candidates[0].content.parts[0].text\n\n    @property\n    def id(self) -&gt; Optional[str]:\n        \"\"\"Returns the id of the response.\n\n        google.generativeai does not return an id\n        \"\"\"\n        return None\n\n    @property\n    def finish_reasons(self) -&gt; list[str]:\n        \"\"\"Returns the finish reasons of the response.\"\"\"\n        finish_reasons = [\n            \"FINISH_REASON_UNSPECIFIED\",\n            \"STOP\",\n            \"MAX_TOKENS\",\n            \"SAFETY\",\n            \"RECITATION\",\n            \"OTHER\",\n        ]\n\n        return [\n            finish_reasons[candidate.finish_reason]\n            for candidate in self.response.candidates\n        ]\n\n    @property\n    def model(self) -&gt; None:\n        \"\"\"Returns the model name.\n\n        google.generativeai does not return model, so we return None\n        \"\"\"\n        return None\n\n    @property\n    def usage(self) -&gt; None:\n        \"\"\"Returns the usage of the chat completion.\n\n        google.generativeai does not have Usage, so we return None\n        \"\"\"\n        return None\n\n    @property\n    def input_tokens(self) -&gt; None:\n        \"\"\"Returns the number of input tokens.\"\"\"\n        return None\n\n    @property\n    def output_tokens(self) -&gt; None:\n        \"\"\"Returns the number of output tokens.\"\"\"\n        return None\n</code></pre>"},{"location":"api/core/gemini/call_response/#mirascope.core.gemini.call_response.GeminiCallResponse.content","title":"<code>content: str</code>  <code>property</code>","text":"<p>Returns the contained string content for the 0th choice.</p>"},{"location":"api/core/gemini/call_response/#mirascope.core.gemini.call_response.GeminiCallResponse.finish_reasons","title":"<code>finish_reasons: list[str]</code>  <code>property</code>","text":"<p>Returns the finish reasons of the response.</p>"},{"location":"api/core/gemini/call_response/#mirascope.core.gemini.call_response.GeminiCallResponse.id","title":"<code>id: Optional[str]</code>  <code>property</code>","text":"<p>Returns the id of the response.</p> <p>google.generativeai does not return an id</p>"},{"location":"api/core/gemini/call_response/#mirascope.core.gemini.call_response.GeminiCallResponse.input_tokens","title":"<code>input_tokens: None</code>  <code>property</code>","text":"<p>Returns the number of input tokens.</p>"},{"location":"api/core/gemini/call_response/#mirascope.core.gemini.call_response.GeminiCallResponse.message_param","title":"<code>message_param: ContentDict</code>  <code>property</code>","text":"<p>Returns the models's response as a message parameter.</p>"},{"location":"api/core/gemini/call_response/#mirascope.core.gemini.call_response.GeminiCallResponse.model","title":"<code>model: None</code>  <code>property</code>","text":"<p>Returns the model name.</p> <p>google.generativeai does not return model, so we return None</p>"},{"location":"api/core/gemini/call_response/#mirascope.core.gemini.call_response.GeminiCallResponse.output_tokens","title":"<code>output_tokens: None</code>  <code>property</code>","text":"<p>Returns the number of output tokens.</p>"},{"location":"api/core/gemini/call_response/#mirascope.core.gemini.call_response.GeminiCallResponse.tool","title":"<code>tool: GeminiTool | None</code>  <code>property</code>","text":"<p>Returns the 0th tool for the 0th candidate's 0th content part.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if the tool call doesn't match the tool's schema.</p>"},{"location":"api/core/gemini/call_response/#mirascope.core.gemini.call_response.GeminiCallResponse.tool_calls","title":"<code>tool_calls: list[FunctionCall]</code>  <code>property</code>","text":"<p>Returns the tool calls for the 0th candidate content.</p>"},{"location":"api/core/gemini/call_response/#mirascope.core.gemini.call_response.GeminiCallResponse.tools","title":"<code>tools: list[GeminiTool] | None</code>  <code>property</code>","text":"<p>Returns the list of tools for the 0th candidate's 0th content part.</p>"},{"location":"api/core/gemini/call_response/#mirascope.core.gemini.call_response.GeminiCallResponse.usage","title":"<code>usage: None</code>  <code>property</code>","text":"<p>Returns the usage of the chat completion.</p> <p>google.generativeai does not have Usage, so we return None</p>"},{"location":"api/core/gemini/call_response/#mirascope.core.gemini.call_response.GeminiCallResponse.tool_message_params","title":"<code>tool_message_params(tools_and_outputs)</code>  <code>classmethod</code>","text":"<p>Returns the tool message parameters for tool call results.</p> Source code in <code>mirascope/core/gemini/call_response.py</code> <pre><code>@classmethod\ndef tool_message_params(\n    cls, tools_and_outputs: list[tuple[GeminiTool, object]]\n) -&gt; list[FunctionResponse]:\n    \"\"\"Returns the tool message parameters for tool call results.\"\"\"\n    return [\n        FunctionResponse(name=tool._name(), response={\"result\": output})\n        for tool, output in tools_and_outputs\n    ]\n</code></pre>"},{"location":"api/core/gemini/call_response_chunk/","title":"mirascope.core.gemini.call_response_chunk","text":""},{"location":"api/core/gemini/call_response_chunk/#mirascope.core.gemini.call_response_chunk.GeminiCallResponseChunk","title":"<code>GeminiCallResponseChunk</code>","text":"<p>               Bases: <code>BaseCallResponseChunk[GenerateContentResponse]</code></p> <p>Convenience wrapper around chat completion streaming chunks.</p> <p>When using Mirascope's convenience wrappers to interact with Gemini models via <code>GeminiCall</code>, responses using <code>GeminiCall.stream()</code> will return a <code>GeminiCallResponseChunk</code>, whereby the implemented properties allow for simpler syntax and a convenient developer experience.</p> <p>Example:</p> <pre><code>from mirascope.gemini import GeminiCall\n\n\nclass Math(GeminiCall):\n    prompt_template = \"What is 1 + 2?\"\n\n\ncontent = \"\"\nfor chunk in Math().stream():\n    content += chunk.content\n    print(content)\n#&gt; 1\n#  1 +\n#  1 + 2\n#  1 + 2 equals\n#  1 + 2 equals\n#  1 + 2 equals 3\n#  1 + 2 equals 3.\n</code></pre> Source code in <code>mirascope/core/gemini/call_response_chunk.py</code> <pre><code>class GeminiCallResponseChunk(BaseCallResponseChunk[GenerateContentResponse]):\n    \"\"\"Convenience wrapper around chat completion streaming chunks.\n\n    When using Mirascope's convenience wrappers to interact with Gemini models via\n    `GeminiCall`, responses using `GeminiCall.stream()` will return a\n    `GeminiCallResponseChunk`, whereby the implemented properties allow for simpler\n    syntax and a convenient developer experience.\n\n    Example:\n\n    ```python\n    from mirascope.gemini import GeminiCall\n\n\n    class Math(GeminiCall):\n        prompt_template = \"What is 1 + 2?\"\n\n\n    content = \"\"\n    for chunk in Math().stream():\n        content += chunk.content\n        print(content)\n    #&gt; 1\n    #  1 +\n    #  1 + 2\n    #  1 + 2 equals\n    #  1 + 2 equals\n    #  1 + 2 equals 3\n    #  1 + 2 equals 3.\n    ```\n    \"\"\"\n\n    @property\n    def content(self) -&gt; str:\n        \"\"\"Returns the chunk content for the 0th choice.\"\"\"\n        return self.chunk.candidates[0].content.parts[0].text\n\n    @property\n    def id(self) -&gt; str | None:\n        \"\"\"Returns the id of the response.\n\n        google.generativeai does not return an id\n        \"\"\"\n        return None\n\n    @property\n    def finish_reasons(self) -&gt; list[str]:\n        \"\"\"Returns the finish reasons of the response.\"\"\"\n        finish_reasons = [\n            \"FINISH_REASON_UNSPECIFIED\",\n            \"STOP\",\n            \"MAX_TOKENS\",\n            \"SAFETY\",\n            \"RECITATION\",\n            \"OTHER\",\n        ]\n\n        return [\n            finish_reasons[candidate.finish_reason]\n            for candidate in self.chunk.candidates\n        ]\n\n    @property\n    def model(self) -&gt; None:\n        \"\"\"Returns the model name.\n\n        google.generativeai does not return model, so we return None\n        \"\"\"\n        return None\n\n    @property\n    def usage(self) -&gt; None:\n        \"\"\"Returns the usage of the chat completion.\n\n        google.generativeai does not have Usage, so we return None\n        \"\"\"\n        return None\n\n    @property\n    def input_tokens(self) -&gt; None:\n        \"\"\"Returns the number of input tokens.\"\"\"\n        return None\n\n    @property\n    def output_tokens(self) -&gt; None:\n        \"\"\"Returns the number of output tokens.\"\"\"\n        return None\n</code></pre>"},{"location":"api/core/gemini/call_response_chunk/#mirascope.core.gemini.call_response_chunk.GeminiCallResponseChunk.content","title":"<code>content: str</code>  <code>property</code>","text":"<p>Returns the chunk content for the 0th choice.</p>"},{"location":"api/core/gemini/call_response_chunk/#mirascope.core.gemini.call_response_chunk.GeminiCallResponseChunk.finish_reasons","title":"<code>finish_reasons: list[str]</code>  <code>property</code>","text":"<p>Returns the finish reasons of the response.</p>"},{"location":"api/core/gemini/call_response_chunk/#mirascope.core.gemini.call_response_chunk.GeminiCallResponseChunk.id","title":"<code>id: str | None</code>  <code>property</code>","text":"<p>Returns the id of the response.</p> <p>google.generativeai does not return an id</p>"},{"location":"api/core/gemini/call_response_chunk/#mirascope.core.gemini.call_response_chunk.GeminiCallResponseChunk.input_tokens","title":"<code>input_tokens: None</code>  <code>property</code>","text":"<p>Returns the number of input tokens.</p>"},{"location":"api/core/gemini/call_response_chunk/#mirascope.core.gemini.call_response_chunk.GeminiCallResponseChunk.model","title":"<code>model: None</code>  <code>property</code>","text":"<p>Returns the model name.</p> <p>google.generativeai does not return model, so we return None</p>"},{"location":"api/core/gemini/call_response_chunk/#mirascope.core.gemini.call_response_chunk.GeminiCallResponseChunk.output_tokens","title":"<code>output_tokens: None</code>  <code>property</code>","text":"<p>Returns the number of output tokens.</p>"},{"location":"api/core/gemini/call_response_chunk/#mirascope.core.gemini.call_response_chunk.GeminiCallResponseChunk.usage","title":"<code>usage: None</code>  <code>property</code>","text":"<p>Returns the usage of the chat completion.</p> <p>google.generativeai does not have Usage, so we return None</p>"},{"location":"api/core/gemini/dynamic_config/","title":"mirascope.core.gemini.dynamic_config","text":"<p>This module defines the function return type for functions as LLM calls.</p>"},{"location":"api/core/gemini/dynamic_config/#mirascope.core.gemini.dynamic_config.GeminiDynamicConfig","title":"<code>GeminiDynamicConfig = BaseDynamicConfig[ContentsType, GeminiCallParams]</code>  <code>module-attribute</code>","text":"<p>The function return type for functions wrapped with the <code>gemini_call</code> decorator.</p> <p>Attributes:</p> Name Type Description <code>computed_fields</code> <p>The computed fields to use in the prompt template.</p> <code>messages</code> <p>The messages to send to the Gemini API. If provided, <code>computed_fields</code> will be ignored.</p> <code>call_params</code> <p>The call parameters to use when calling the Gemini API. These will override any call parameters provided to the decorator.</p> <p>Example:</p> <pre><code>from mirascope.core.gemini import GeminiDynamicConfig, gemini_call\n\n@gemini_call(model=\"gemini-pro\")\ndef recommend_book(genre: str) -&gt; GeminiDynamicConfig:\n    \"\"\"Recommend a {capitalized_genre} book.\"\"\"\n    reeturn {\"computed_fields\": {\"capitalized_genre\": genre.capitalize()}}\n</code></pre>"},{"location":"api/core/gemini/tool/","title":"mirascope.core.gemini.tool","text":"<p>The <code>GeminiTool</code> class for easy tool usage with Google's Gemini LLM calls.</p>"},{"location":"api/core/gemini/tool/#mirascope.core.gemini.tool.GeminiTool","title":"<code>GeminiTool</code>","text":"<p>               Bases: <code>BaseTool</code></p> <p>A class for defining tools for Gemini LLM calls.</p> Source code in <code>mirascope/core/gemini/tool.py</code> <pre><code>class GeminiTool(BaseTool):\n    \"\"\"A class for defining tools for Gemini LLM calls.\"\"\"\n\n    tool_call: SkipJsonSchema[FunctionCall]\n\n    @classmethod\n    def tool_schema(cls) -&gt; Tool:\n        \"\"\"Constructs a JSON Schema tool schema from the `BaseModel` schema defined.\"\"\"\n        model_schema = cls.model_json_schema()\n        model_schema.pop(\"title\", None)\n        model_schema.pop(\"description\", None)\n\n        fn: dict[str, Any] = {\"name\": cls._name(), \"description\": cls._description()}\n        if model_schema[\"properties\"]:\n            fn[\"parameters\"] = model_schema  # type: ignore\n        if model_schema[\"required\"]:\n            fn[\"parameters\"][\"required\"] = model_schema[\"required\"]\n        if \"parameters\" in fn:\n            if \"$defs\" in fn[\"parameters\"]:\n                raise ValueError(\n                    \"Unfortunately Google's Gemini API cannot handle nested structures \"\n                    \"with $defs.\"\n                )\n\n            def handle_enum_schema(prop_schema):\n                if \"enum\" in prop_schema:\n                    prop_schema[\"format\"] = \"enum\"\n                return prop_schema\n\n            fn[\"parameters\"][\"properties\"] = {\n                prop: {\n                    key: value\n                    for key, value in handle_enum_schema(prop_schema).items()\n                    if key != \"title\"\n                }\n                for prop, prop_schema in fn[\"parameters\"][\"properties\"].items()\n            }\n        return Tool(function_declarations=[FunctionDeclaration(**fn)])\n\n    @classmethod\n    def from_tool_call(cls, tool_call: FunctionCall) -&gt; GeminiTool:\n        \"\"\"Constructs an `GeminiTool` instance from a `tool_call`.\"\"\"\n        if not tool_call.args:\n            raise ValueError(\"Tool call doesn't have any arguments.\")\n        model_json: dict[str, Any] = {\n            key: value for key, value in tool_call.args.items()\n        }\n        model_json[\"tool_call\"] = tool_call\n        return cls.model_validate(model_json)\n</code></pre>"},{"location":"api/core/gemini/tool/#mirascope.core.gemini.tool.GeminiTool.from_tool_call","title":"<code>from_tool_call(tool_call)</code>  <code>classmethod</code>","text":"<p>Constructs an <code>GeminiTool</code> instance from a <code>tool_call</code>.</p> Source code in <code>mirascope/core/gemini/tool.py</code> <pre><code>@classmethod\ndef from_tool_call(cls, tool_call: FunctionCall) -&gt; GeminiTool:\n    \"\"\"Constructs an `GeminiTool` instance from a `tool_call`.\"\"\"\n    if not tool_call.args:\n        raise ValueError(\"Tool call doesn't have any arguments.\")\n    model_json: dict[str, Any] = {\n        key: value for key, value in tool_call.args.items()\n    }\n    model_json[\"tool_call\"] = tool_call\n    return cls.model_validate(model_json)\n</code></pre>"},{"location":"api/core/gemini/tool/#mirascope.core.gemini.tool.GeminiTool.tool_schema","title":"<code>tool_schema()</code>  <code>classmethod</code>","text":"<p>Constructs a JSON Schema tool schema from the <code>BaseModel</code> schema defined.</p> Source code in <code>mirascope/core/gemini/tool.py</code> <pre><code>@classmethod\ndef tool_schema(cls) -&gt; Tool:\n    \"\"\"Constructs a JSON Schema tool schema from the `BaseModel` schema defined.\"\"\"\n    model_schema = cls.model_json_schema()\n    model_schema.pop(\"title\", None)\n    model_schema.pop(\"description\", None)\n\n    fn: dict[str, Any] = {\"name\": cls._name(), \"description\": cls._description()}\n    if model_schema[\"properties\"]:\n        fn[\"parameters\"] = model_schema  # type: ignore\n    if model_schema[\"required\"]:\n        fn[\"parameters\"][\"required\"] = model_schema[\"required\"]\n    if \"parameters\" in fn:\n        if \"$defs\" in fn[\"parameters\"]:\n            raise ValueError(\n                \"Unfortunately Google's Gemini API cannot handle nested structures \"\n                \"with $defs.\"\n            )\n\n        def handle_enum_schema(prop_schema):\n            if \"enum\" in prop_schema:\n                prop_schema[\"format\"] = \"enum\"\n            return prop_schema\n\n        fn[\"parameters\"][\"properties\"] = {\n            prop: {\n                key: value\n                for key, value in handle_enum_schema(prop_schema).items()\n                if key != \"title\"\n            }\n            for prop, prop_schema in fn[\"parameters\"][\"properties\"].items()\n        }\n    return Tool(function_declarations=[FunctionDeclaration(**fn)])\n</code></pre>"},{"location":"api/core/groq/","title":"mirascope.core.groq","text":"<p>The Mirascope Groq Module.</p>"},{"location":"api/core/groq/#mirascope.core.groq.GroqDynamicConfig","title":"<code>GroqDynamicConfig = BaseDynamicConfig[ChatCompletionMessageParam, GroqCallParams]</code>  <code>module-attribute</code>","text":"<p>The function return type for functions wrapped with the <code>groq_call</code> decorator.</p> <p>Attributes:</p> Name Type Description <code>computed_fields</code> <p>The computed fields to use in the prompt template.</p> <code>messages</code> <p>The messages to send to the Groq API. If provided, <code>computed_fields</code> will be ignored.</p> <code>call_params</code> <p>The call parameters to use when calling the Groq API. These will override any call parameters provided to the decorator.</p> <p>Example:</p> <pre><code>from mirascope.core.groq import GroqDynamicConfig, groq_call\n\n@groq_call(model=\"gpt-4o\")\ndef recommend_book(genre: str) -&gt; GroqDynamicConfig:\n    \"\"\"Recommend a {capitalized_genre} book.\"\"\"\n    reeturn {\"computed_fields\": {\"capitalized_genre\": genre.capitalize()}}\n</code></pre>"},{"location":"api/core/groq/#mirascope.core.groq.groq_call","title":"<code>groq_call = call_factory(TCallResponse=GroqCallResponse, TCallResponseChunk=GroqCallResponseChunk, TDynamicConfig=GroqDynamicConfig, TMessageParamType=ChatCompletionAssistantMessageParam, TToolType=GroqTool, TStream=GroqStream, TCallParams=GroqCallParams, default_call_params=GroqCallParams(), setup_call=setup_call, get_json_output=get_json_output, handle_stream=handle_stream, handle_stream_async=handle_stream_async, calculate_cost=calculate_cost, provider='groq')</code>  <code>module-attribute</code>","text":"<p>A decorator for calling the Groq API with a typed function.</p> <p>This decorator is used to wrap a typed function that calls the Groq API. It parses the docstring of the wrapped function as the messages array and templates the input arguments for the function into each message's template.</p> <p>Example:</p> <pre><code>@groq_call(model=\"gpt-4o\")\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\nresponse = recommend_book(\"fantasy\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>The Groq model to use in the API call.</p> required <code>stream</code> <p>Whether to stream the response from the API call.</p> required <code>tools</code> <p>The tools to use in the Groq API call.</p> required <code>**call_params</code> <p>The <code>GroqCallParams</code> call parameters to use in the API call.</p> required <p>Returns:</p> Type Description <p>The decorator for turning a typed function into an Groq API call.</p>"},{"location":"api/core/groq/#mirascope.core.groq.GroqCallParams","title":"<code>GroqCallParams</code>","text":"<p>               Bases: <code>BaseCallParams</code></p> <p>The parameters to use when calling the Groq API.</p> Source code in <code>mirascope/core/groq/call_params.py</code> <pre><code>class GroqCallParams(BaseCallParams):\n    \"\"\"The parameters to use when calling the Groq API.\"\"\"\n\n    frequency_penalty: NotRequired[float | None]\n    logit_bias: NotRequired[dict[str, int] | None]\n    logprobs: NotRequired[bool | None]\n    max_tokens: NotRequired[int | None]\n    n: NotRequired[int | None]\n    parallel_tool_calls: NotRequired[bool]\n    presence_penalty: NotRequired[float | None]\n    response_format: NotRequired[ResponseFormat]\n    seed: NotRequired[int | None]\n    stop: NotRequired[str | list[str] | None]\n    temperature: NotRequired[float | None]\n    tool_choice: NotRequired[ChatCompletionToolChoiceOptionParam]\n    top_logprobs: NotRequired[int | None]\n    top_p: NotRequired[float | None]\n    user: NotRequired[str]\n</code></pre>"},{"location":"api/core/groq/#mirascope.core.groq.GroqCallResponse","title":"<code>GroqCallResponse</code>","text":"<p>               Bases: <code>BaseCallResponse[ChatCompletion, GroqTool, GroqDynamicConfig, ChatCompletionMessageParam, GroqCallParams, ChatCompletionUserMessageParam]</code></p> <p>A convenience wrapper around the Groq <code>ChatCompletion</code> response.</p> <p>When calling the Groq API using a function decorated with <code>groq_call</code>, the response will be an <code>GroqCallResponse</code> instance with properties that allow for more convenience access to commonly used attributes.</p> <p>Example:</p> <pre><code>from mirascope.core.groq import groq_call\n\n@groq_call(model=\"gpt-4o\")\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\nresponse = recommend_book(\"fantasy\")  # response is an `GroqCallResponse` instance\nprint(response.content)\n#&gt; Sure! I would recommend...\n</code></pre> Source code in <code>mirascope/core/groq/call_response.py</code> <pre><code>class GroqCallResponse(\n    BaseCallResponse[\n        ChatCompletion,\n        GroqTool,\n        GroqDynamicConfig,\n        ChatCompletionMessageParam,\n        GroqCallParams,\n        ChatCompletionUserMessageParam,\n    ]\n):\n    '''A convenience wrapper around the Groq `ChatCompletion` response.\n\n    When calling the Groq API using a function decorated with `groq_call`, the\n    response will be an `GroqCallResponse` instance with properties that allow for\n    more convenience access to commonly used attributes.\n\n    Example:\n\n    ```python\n    from mirascope.core.groq import groq_call\n\n    @groq_call(model=\"gpt-4o\")\n    def recommend_book(genre: str):\n        \"\"\"Recommend a {genre} book.\"\"\"\n\n    response = recommend_book(\"fantasy\")  # response is an `GroqCallResponse` instance\n    print(response.content)\n    #&gt; Sure! I would recommend...\n    ```\n    '''\n\n    _provider = \"groq\"\n\n    @computed_field\n    @property\n    def message_param(self) -&gt; ChatCompletionAssistantMessageParam:\n        \"\"\"Returns the assistants's response as a message parameter.\"\"\"\n        return self.message.model_dump(exclude={\"function_call\"})  # type: ignore\n\n    @property\n    def choices(self) -&gt; list[Choice]:\n        \"\"\"Returns the array of chat completion choices.\"\"\"\n        return self.response.choices\n\n    @property\n    def choice(self) -&gt; Choice:\n        \"\"\"Returns the 0th choice.\"\"\"\n        return self.choices[0]\n\n    @property\n    def message(self) -&gt; ChatCompletionMessage:\n        \"\"\"Returns the message of the chat completion for the 0th choice.\"\"\"\n        return self.choice.message\n\n    @property\n    def content(self) -&gt; str:\n        \"\"\"Returns the content of the chat completion for the 0th choice.\"\"\"\n        return self.message.content if self.message.content is not None else \"\"\n\n    @property\n    def model(self) -&gt; str:\n        \"\"\"Returns the name of the response model.\"\"\"\n        return self.response.model\n\n    @property\n    def id(self) -&gt; str:\n        \"\"\"Returns the id of the response.\"\"\"\n        return self.response.id\n\n    @property\n    def finish_reasons(self) -&gt; list[str]:\n        \"\"\"Returns the finish reasons of the response.\"\"\"\n        return [str(choice.finish_reason) for choice in self.response.choices]\n\n    @property\n    def tool_calls(self) -&gt; list[ChatCompletionMessageToolCall] | None:\n        \"\"\"Returns the tool calls for the 0th choice message.\"\"\"\n        return self.message.tool_calls\n\n    @computed_field\n    @property\n    def tools(self) -&gt; list[GroqTool] | None:\n        \"\"\"Returns any available tool calls as their `GroqTool` definition.\n\n        Raises:\n            ValidationError: if a tool call doesn't match the tool's schema.\n        \"\"\"\n        if not self.tool_types or not self.tool_calls:\n            return None\n\n        extracted_tools = []\n        for tool_call in self.tool_calls:\n            for tool_type in self.tool_types:\n                if tool_call.function.name == tool_type._name():\n                    extracted_tools.append(tool_type.from_tool_call(tool_call))\n                    break\n\n        return extracted_tools\n\n    @computed_field\n    @property\n    def tool(self) -&gt; GroqTool | None:\n        \"\"\"Returns the 0th tool for the 0th choice message.\n\n        Raises:\n            ValidationError: if the tool call doesn't match the tool's schema.\n        \"\"\"\n        if tools := self.tools:\n            return tools[0]\n        return None\n\n    @classmethod\n    def tool_message_params(\n        cls, tools_and_outputs: list[tuple[GroqTool, str]]\n    ) -&gt; list[ChatCompletionToolMessageParam]:\n        \"\"\"Returns the tool message parameters for tool call results.\"\"\"\n        return [\n            ChatCompletionToolMessageParam(\n                role=\"tool\",\n                content=output,\n                tool_call_id=tool.tool_call.id,\n                name=tool._name(),  # type: ignore\n            )\n            for tool, output in tools_and_outputs\n        ]\n\n    @property\n    def usage(self) -&gt; CompletionUsage | None:\n        \"\"\"Returns the usage of the chat completion.\"\"\"\n        return self.response.usage\n\n    @property\n    def input_tokens(self) -&gt; int | None:\n        \"\"\"Returns the number of input tokens.\"\"\"\n        return self.usage.prompt_tokens if self.usage else None\n\n    @property\n    def output_tokens(self) -&gt; int | None:\n        \"\"\"Returns the number of output tokens.\"\"\"\n        return self.usage.completion_tokens if self.usage else None\n</code></pre>"},{"location":"api/core/groq/#mirascope.core.groq.GroqCallResponse.choice","title":"<code>choice: Choice</code>  <code>property</code>","text":"<p>Returns the 0th choice.</p>"},{"location":"api/core/groq/#mirascope.core.groq.GroqCallResponse.choices","title":"<code>choices: list[Choice]</code>  <code>property</code>","text":"<p>Returns the array of chat completion choices.</p>"},{"location":"api/core/groq/#mirascope.core.groq.GroqCallResponse.content","title":"<code>content: str</code>  <code>property</code>","text":"<p>Returns the content of the chat completion for the 0th choice.</p>"},{"location":"api/core/groq/#mirascope.core.groq.GroqCallResponse.finish_reasons","title":"<code>finish_reasons: list[str]</code>  <code>property</code>","text":"<p>Returns the finish reasons of the response.</p>"},{"location":"api/core/groq/#mirascope.core.groq.GroqCallResponse.id","title":"<code>id: str</code>  <code>property</code>","text":"<p>Returns the id of the response.</p>"},{"location":"api/core/groq/#mirascope.core.groq.GroqCallResponse.input_tokens","title":"<code>input_tokens: int | None</code>  <code>property</code>","text":"<p>Returns the number of input tokens.</p>"},{"location":"api/core/groq/#mirascope.core.groq.GroqCallResponse.message","title":"<code>message: ChatCompletionMessage</code>  <code>property</code>","text":"<p>Returns the message of the chat completion for the 0th choice.</p>"},{"location":"api/core/groq/#mirascope.core.groq.GroqCallResponse.message_param","title":"<code>message_param: ChatCompletionAssistantMessageParam</code>  <code>property</code>","text":"<p>Returns the assistants's response as a message parameter.</p>"},{"location":"api/core/groq/#mirascope.core.groq.GroqCallResponse.model","title":"<code>model: str</code>  <code>property</code>","text":"<p>Returns the name of the response model.</p>"},{"location":"api/core/groq/#mirascope.core.groq.GroqCallResponse.output_tokens","title":"<code>output_tokens: int | None</code>  <code>property</code>","text":"<p>Returns the number of output tokens.</p>"},{"location":"api/core/groq/#mirascope.core.groq.GroqCallResponse.tool","title":"<code>tool: GroqTool | None</code>  <code>property</code>","text":"<p>Returns the 0th tool for the 0th choice message.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if the tool call doesn't match the tool's schema.</p>"},{"location":"api/core/groq/#mirascope.core.groq.GroqCallResponse.tool_calls","title":"<code>tool_calls: list[ChatCompletionMessageToolCall] | None</code>  <code>property</code>","text":"<p>Returns the tool calls for the 0th choice message.</p>"},{"location":"api/core/groq/#mirascope.core.groq.GroqCallResponse.tools","title":"<code>tools: list[GroqTool] | None</code>  <code>property</code>","text":"<p>Returns any available tool calls as their <code>GroqTool</code> definition.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if a tool call doesn't match the tool's schema.</p>"},{"location":"api/core/groq/#mirascope.core.groq.GroqCallResponse.usage","title":"<code>usage: CompletionUsage | None</code>  <code>property</code>","text":"<p>Returns the usage of the chat completion.</p>"},{"location":"api/core/groq/#mirascope.core.groq.GroqCallResponse.tool_message_params","title":"<code>tool_message_params(tools_and_outputs)</code>  <code>classmethod</code>","text":"<p>Returns the tool message parameters for tool call results.</p> Source code in <code>mirascope/core/groq/call_response.py</code> <pre><code>@classmethod\ndef tool_message_params(\n    cls, tools_and_outputs: list[tuple[GroqTool, str]]\n) -&gt; list[ChatCompletionToolMessageParam]:\n    \"\"\"Returns the tool message parameters for tool call results.\"\"\"\n    return [\n        ChatCompletionToolMessageParam(\n            role=\"tool\",\n            content=output,\n            tool_call_id=tool.tool_call.id,\n            name=tool._name(),  # type: ignore\n        )\n        for tool, output in tools_and_outputs\n    ]\n</code></pre>"},{"location":"api/core/groq/#mirascope.core.groq.GroqCallResponseChunk","title":"<code>GroqCallResponseChunk</code>","text":"<p>               Bases: <code>BaseCallResponseChunk[ChatCompletionChunk]</code></p> <p>A convenience wrapper around the Groq <code>ChatCompletionChunk</code> streamed chunks.</p> <p>When calling the Groq API using a function decorated with <code>groq_call</code> and <code>stream</code> set to <code>True</code>, the stream will contain <code>GroqResponseChunk</code> instances with properties that allow for more convenient access to commonly used attributes.</p> <p>Example:</p> <pre><code>from mirascope.core.groq import groq_call\n\n@groq_call(model=\"gpt-4o\", stream=True)\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\nstream = recommend_book(\"fantasy\")  # response is an `GroqStream`\nfor chunk in stream:\n    print(chunk.content, end=\"\", flush=True)\n#&gt; Sure! I would recommend...\n</code></pre> Source code in <code>mirascope/core/groq/call_response_chunk.py</code> <pre><code>class GroqCallResponseChunk(BaseCallResponseChunk[ChatCompletionChunk]):\n    '''A convenience wrapper around the Groq `ChatCompletionChunk` streamed chunks.\n\n    When calling the Groq API using a function decorated with `groq_call` and\n    `stream` set to `True`, the stream will contain `GroqResponseChunk` instances with\n    properties that allow for more convenient access to commonly used attributes.\n\n    Example:\n\n    ```python\n    from mirascope.core.groq import groq_call\n\n    @groq_call(model=\"gpt-4o\", stream=True)\n    def recommend_book(genre: str):\n        \"\"\"Recommend a {genre} book.\"\"\"\n\n    stream = recommend_book(\"fantasy\")  # response is an `GroqStream`\n    for chunk in stream:\n        print(chunk.content, end=\"\", flush=True)\n    #&gt; Sure! I would recommend...\n    ```\n    '''\n\n    @property\n    def choices(self) -&gt; list[Choice]:\n        \"\"\"Returns the array of chat completion choices.\"\"\"\n        return self.chunk.choices\n\n    @property\n    def choice(self) -&gt; Choice:\n        \"\"\"Returns the 0th choice.\"\"\"\n        return self.chunk.choices[0]\n\n    @property\n    def delta(self) -&gt; ChoiceDelta | None:\n        \"\"\"Returns the delta for the 0th choice.\"\"\"\n        if self.chunk.choices:\n            return self.chunk.choices[0].delta\n        return None\n\n    @property\n    def content(self) -&gt; str:\n        \"\"\"Returns the content for the 0th choice delta.\"\"\"\n        return (\n            self.delta.content if self.delta is not None and self.delta.content else \"\"\n        )\n\n    @property\n    def model(self) -&gt; str:\n        \"\"\"Returns the name of the response model.\"\"\"\n        return self.chunk.model\n\n    @property\n    def id(self) -&gt; str:\n        \"\"\"Returns the id of the response.\"\"\"\n        return self.chunk.id\n\n    @property\n    def finish_reasons(self) -&gt; list[str]:\n        \"\"\"Returns the finish reasons of the response.\"\"\"\n        return [str(choice.finish_reason) for choice in self.chunk.choices]\n\n    @property\n    def tool_calls(self) -&gt; list[ChoiceDeltaToolCall] | None:\n        \"\"\"Returns the partial tool calls for the 0th choice message.\n\n        The first `list[ChoiceDeltaToolCall]` will contain the name of the tool and\n        index, and subsequent `list[ChoiceDeltaToolCall]`s will contain the arguments\n        which will be strings that need to be concatenated with future\n        `list[ChoiceDeltaToolCall]`s to form a complete JSON tool calls. The last\n        `list[ChoiceDeltaToolCall]` will be None indicating end of stream.\n        \"\"\"\n        if self.delta:\n            return self.delta.tool_calls\n        return None\n\n    @property\n    def usage(self) -&gt; CompletionUsage | None:\n        \"\"\"Returns the usage of the chat completion.\"\"\"\n        if self.chunk.usage:\n            return self.chunk.usage\n        return None\n\n    @property\n    def input_tokens(self) -&gt; int | None:\n        \"\"\"Returns the number of input tokens.\"\"\"\n        if self.usage:\n            return self.usage.prompt_tokens\n        return None\n\n    @property\n    def output_tokens(self) -&gt; int | None:\n        \"\"\"Returns the number of output tokens.\"\"\"\n        if self.usage:\n            return self.usage.completion_tokens\n        return None\n</code></pre>"},{"location":"api/core/groq/#mirascope.core.groq.GroqCallResponseChunk.choice","title":"<code>choice: Choice</code>  <code>property</code>","text":"<p>Returns the 0th choice.</p>"},{"location":"api/core/groq/#mirascope.core.groq.GroqCallResponseChunk.choices","title":"<code>choices: list[Choice]</code>  <code>property</code>","text":"<p>Returns the array of chat completion choices.</p>"},{"location":"api/core/groq/#mirascope.core.groq.GroqCallResponseChunk.content","title":"<code>content: str</code>  <code>property</code>","text":"<p>Returns the content for the 0th choice delta.</p>"},{"location":"api/core/groq/#mirascope.core.groq.GroqCallResponseChunk.delta","title":"<code>delta: ChoiceDelta | None</code>  <code>property</code>","text":"<p>Returns the delta for the 0th choice.</p>"},{"location":"api/core/groq/#mirascope.core.groq.GroqCallResponseChunk.finish_reasons","title":"<code>finish_reasons: list[str]</code>  <code>property</code>","text":"<p>Returns the finish reasons of the response.</p>"},{"location":"api/core/groq/#mirascope.core.groq.GroqCallResponseChunk.id","title":"<code>id: str</code>  <code>property</code>","text":"<p>Returns the id of the response.</p>"},{"location":"api/core/groq/#mirascope.core.groq.GroqCallResponseChunk.input_tokens","title":"<code>input_tokens: int | None</code>  <code>property</code>","text":"<p>Returns the number of input tokens.</p>"},{"location":"api/core/groq/#mirascope.core.groq.GroqCallResponseChunk.model","title":"<code>model: str</code>  <code>property</code>","text":"<p>Returns the name of the response model.</p>"},{"location":"api/core/groq/#mirascope.core.groq.GroqCallResponseChunk.output_tokens","title":"<code>output_tokens: int | None</code>  <code>property</code>","text":"<p>Returns the number of output tokens.</p>"},{"location":"api/core/groq/#mirascope.core.groq.GroqCallResponseChunk.tool_calls","title":"<code>tool_calls: list[ChoiceDeltaToolCall] | None</code>  <code>property</code>","text":"<p>Returns the partial tool calls for the 0th choice message.</p> <p>The first <code>list[ChoiceDeltaToolCall]</code> will contain the name of the tool and index, and subsequent <code>list[ChoiceDeltaToolCall]</code>s will contain the arguments which will be strings that need to be concatenated with future <code>list[ChoiceDeltaToolCall]</code>s to form a complete JSON tool calls. The last <code>list[ChoiceDeltaToolCall]</code> will be None indicating end of stream.</p>"},{"location":"api/core/groq/#mirascope.core.groq.GroqCallResponseChunk.usage","title":"<code>usage: CompletionUsage | None</code>  <code>property</code>","text":"<p>Returns the usage of the chat completion.</p>"},{"location":"api/core/groq/#mirascope.core.groq.GroqTool","title":"<code>GroqTool</code>","text":"<p>               Bases: <code>BaseTool</code></p> <p>A class for defining tools for Groq LLM calls.</p> Source code in <code>mirascope/core/groq/tool.py</code> <pre><code>class GroqTool(BaseTool):\n    \"\"\"A class for defining tools for Groq LLM calls.\"\"\"\n\n    tool_call: ChatCompletionMessageToolCall\n\n    @classmethod\n    def tool_schema(cls) -&gt; ChatCompletionToolParam:\n        \"\"\"Constructs a JSON Schema tool schema from the `BaseModel` schema defined.\"\"\"\n        model_schema = cls.model_json_schema()\n        model_schema.pop(\"title\", None)\n        model_schema.pop(\"description\", None)\n\n        fn = FunctionDefinition(name=cls._name(), description=cls._description())\n        if model_schema[\"properties\"]:\n            fn[\"parameters\"] = model_schema\n\n        return ChatCompletionToolParam(function=fn, type=\"function\")\n\n    @classmethod\n    def from_tool_call(cls, tool_call: ChatCompletionMessageToolCall) -&gt; GroqTool:\n        \"\"\"Constructs an `GroqTool` instance from a `tool_call`.\"\"\"\n        model_json = jiter.from_json(tool_call.function.arguments.encode())\n        model_json[\"tool_call\"] = tool_call.model_dump()\n        return cls.model_validate(model_json)\n</code></pre>"},{"location":"api/core/groq/#mirascope.core.groq.GroqTool.from_tool_call","title":"<code>from_tool_call(tool_call)</code>  <code>classmethod</code>","text":"<p>Constructs an <code>GroqTool</code> instance from a <code>tool_call</code>.</p> Source code in <code>mirascope/core/groq/tool.py</code> <pre><code>@classmethod\ndef from_tool_call(cls, tool_call: ChatCompletionMessageToolCall) -&gt; GroqTool:\n    \"\"\"Constructs an `GroqTool` instance from a `tool_call`.\"\"\"\n    model_json = jiter.from_json(tool_call.function.arguments.encode())\n    model_json[\"tool_call\"] = tool_call.model_dump()\n    return cls.model_validate(model_json)\n</code></pre>"},{"location":"api/core/groq/#mirascope.core.groq.GroqTool.tool_schema","title":"<code>tool_schema()</code>  <code>classmethod</code>","text":"<p>Constructs a JSON Schema tool schema from the <code>BaseModel</code> schema defined.</p> Source code in <code>mirascope/core/groq/tool.py</code> <pre><code>@classmethod\ndef tool_schema(cls) -&gt; ChatCompletionToolParam:\n    \"\"\"Constructs a JSON Schema tool schema from the `BaseModel` schema defined.\"\"\"\n    model_schema = cls.model_json_schema()\n    model_schema.pop(\"title\", None)\n    model_schema.pop(\"description\", None)\n\n    fn = FunctionDefinition(name=cls._name(), description=cls._description())\n    if model_schema[\"properties\"]:\n        fn[\"parameters\"] = model_schema\n\n    return ChatCompletionToolParam(function=fn, type=\"function\")\n</code></pre>"},{"location":"api/core/groq/call/","title":"mirascope.core.groq.call","text":"<p>The <code>groq_call</code> decorator for functions as LLM calls.</p>"},{"location":"api/core/groq/call/#mirascope.core.groq.call.groq_call","title":"<code>groq_call = call_factory(TCallResponse=GroqCallResponse, TCallResponseChunk=GroqCallResponseChunk, TDynamicConfig=GroqDynamicConfig, TMessageParamType=ChatCompletionAssistantMessageParam, TToolType=GroqTool, TStream=GroqStream, TCallParams=GroqCallParams, default_call_params=GroqCallParams(), setup_call=setup_call, get_json_output=get_json_output, handle_stream=handle_stream, handle_stream_async=handle_stream_async, calculate_cost=calculate_cost, provider='groq')</code>  <code>module-attribute</code>","text":"<p>A decorator for calling the Groq API with a typed function.</p> <p>This decorator is used to wrap a typed function that calls the Groq API. It parses the docstring of the wrapped function as the messages array and templates the input arguments for the function into each message's template.</p> <p>Example:</p> <pre><code>@groq_call(model=\"gpt-4o\")\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\nresponse = recommend_book(\"fantasy\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>The Groq model to use in the API call.</p> required <code>stream</code> <p>Whether to stream the response from the API call.</p> required <code>tools</code> <p>The tools to use in the Groq API call.</p> required <code>**call_params</code> <p>The <code>GroqCallParams</code> call parameters to use in the API call.</p> required <p>Returns:</p> Type Description <p>The decorator for turning a typed function into an Groq API call.</p>"},{"location":"api/core/groq/call_params/","title":"mirascope.core.groq.call_params","text":"<p>The parameters to use when calling the Groq API.</p>"},{"location":"api/core/groq/call_params/#mirascope.core.groq.call_params.GroqCallParams","title":"<code>GroqCallParams</code>","text":"<p>               Bases: <code>BaseCallParams</code></p> <p>The parameters to use when calling the Groq API.</p> Source code in <code>mirascope/core/groq/call_params.py</code> <pre><code>class GroqCallParams(BaseCallParams):\n    \"\"\"The parameters to use when calling the Groq API.\"\"\"\n\n    frequency_penalty: NotRequired[float | None]\n    logit_bias: NotRequired[dict[str, int] | None]\n    logprobs: NotRequired[bool | None]\n    max_tokens: NotRequired[int | None]\n    n: NotRequired[int | None]\n    parallel_tool_calls: NotRequired[bool]\n    presence_penalty: NotRequired[float | None]\n    response_format: NotRequired[ResponseFormat]\n    seed: NotRequired[int | None]\n    stop: NotRequired[str | list[str] | None]\n    temperature: NotRequired[float | None]\n    tool_choice: NotRequired[ChatCompletionToolChoiceOptionParam]\n    top_logprobs: NotRequired[int | None]\n    top_p: NotRequired[float | None]\n    user: NotRequired[str]\n</code></pre>"},{"location":"api/core/groq/call_response/","title":"mirascope.core.groq.call_response","text":"<p>This module contains the <code>GroqCallResponse</code> class.</p>"},{"location":"api/core/groq/call_response/#mirascope.core.groq.call_response.GroqCallResponse","title":"<code>GroqCallResponse</code>","text":"<p>               Bases: <code>BaseCallResponse[ChatCompletion, GroqTool, GroqDynamicConfig, ChatCompletionMessageParam, GroqCallParams, ChatCompletionUserMessageParam]</code></p> <p>A convenience wrapper around the Groq <code>ChatCompletion</code> response.</p> <p>When calling the Groq API using a function decorated with <code>groq_call</code>, the response will be an <code>GroqCallResponse</code> instance with properties that allow for more convenience access to commonly used attributes.</p> <p>Example:</p> <pre><code>from mirascope.core.groq import groq_call\n\n@groq_call(model=\"gpt-4o\")\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\nresponse = recommend_book(\"fantasy\")  # response is an `GroqCallResponse` instance\nprint(response.content)\n#&gt; Sure! I would recommend...\n</code></pre> Source code in <code>mirascope/core/groq/call_response.py</code> <pre><code>class GroqCallResponse(\n    BaseCallResponse[\n        ChatCompletion,\n        GroqTool,\n        GroqDynamicConfig,\n        ChatCompletionMessageParam,\n        GroqCallParams,\n        ChatCompletionUserMessageParam,\n    ]\n):\n    '''A convenience wrapper around the Groq `ChatCompletion` response.\n\n    When calling the Groq API using a function decorated with `groq_call`, the\n    response will be an `GroqCallResponse` instance with properties that allow for\n    more convenience access to commonly used attributes.\n\n    Example:\n\n    ```python\n    from mirascope.core.groq import groq_call\n\n    @groq_call(model=\"gpt-4o\")\n    def recommend_book(genre: str):\n        \"\"\"Recommend a {genre} book.\"\"\"\n\n    response = recommend_book(\"fantasy\")  # response is an `GroqCallResponse` instance\n    print(response.content)\n    #&gt; Sure! I would recommend...\n    ```\n    '''\n\n    _provider = \"groq\"\n\n    @computed_field\n    @property\n    def message_param(self) -&gt; ChatCompletionAssistantMessageParam:\n        \"\"\"Returns the assistants's response as a message parameter.\"\"\"\n        return self.message.model_dump(exclude={\"function_call\"})  # type: ignore\n\n    @property\n    def choices(self) -&gt; list[Choice]:\n        \"\"\"Returns the array of chat completion choices.\"\"\"\n        return self.response.choices\n\n    @property\n    def choice(self) -&gt; Choice:\n        \"\"\"Returns the 0th choice.\"\"\"\n        return self.choices[0]\n\n    @property\n    def message(self) -&gt; ChatCompletionMessage:\n        \"\"\"Returns the message of the chat completion for the 0th choice.\"\"\"\n        return self.choice.message\n\n    @property\n    def content(self) -&gt; str:\n        \"\"\"Returns the content of the chat completion for the 0th choice.\"\"\"\n        return self.message.content if self.message.content is not None else \"\"\n\n    @property\n    def model(self) -&gt; str:\n        \"\"\"Returns the name of the response model.\"\"\"\n        return self.response.model\n\n    @property\n    def id(self) -&gt; str:\n        \"\"\"Returns the id of the response.\"\"\"\n        return self.response.id\n\n    @property\n    def finish_reasons(self) -&gt; list[str]:\n        \"\"\"Returns the finish reasons of the response.\"\"\"\n        return [str(choice.finish_reason) for choice in self.response.choices]\n\n    @property\n    def tool_calls(self) -&gt; list[ChatCompletionMessageToolCall] | None:\n        \"\"\"Returns the tool calls for the 0th choice message.\"\"\"\n        return self.message.tool_calls\n\n    @computed_field\n    @property\n    def tools(self) -&gt; list[GroqTool] | None:\n        \"\"\"Returns any available tool calls as their `GroqTool` definition.\n\n        Raises:\n            ValidationError: if a tool call doesn't match the tool's schema.\n        \"\"\"\n        if not self.tool_types or not self.tool_calls:\n            return None\n\n        extracted_tools = []\n        for tool_call in self.tool_calls:\n            for tool_type in self.tool_types:\n                if tool_call.function.name == tool_type._name():\n                    extracted_tools.append(tool_type.from_tool_call(tool_call))\n                    break\n\n        return extracted_tools\n\n    @computed_field\n    @property\n    def tool(self) -&gt; GroqTool | None:\n        \"\"\"Returns the 0th tool for the 0th choice message.\n\n        Raises:\n            ValidationError: if the tool call doesn't match the tool's schema.\n        \"\"\"\n        if tools := self.tools:\n            return tools[0]\n        return None\n\n    @classmethod\n    def tool_message_params(\n        cls, tools_and_outputs: list[tuple[GroqTool, str]]\n    ) -&gt; list[ChatCompletionToolMessageParam]:\n        \"\"\"Returns the tool message parameters for tool call results.\"\"\"\n        return [\n            ChatCompletionToolMessageParam(\n                role=\"tool\",\n                content=output,\n                tool_call_id=tool.tool_call.id,\n                name=tool._name(),  # type: ignore\n            )\n            for tool, output in tools_and_outputs\n        ]\n\n    @property\n    def usage(self) -&gt; CompletionUsage | None:\n        \"\"\"Returns the usage of the chat completion.\"\"\"\n        return self.response.usage\n\n    @property\n    def input_tokens(self) -&gt; int | None:\n        \"\"\"Returns the number of input tokens.\"\"\"\n        return self.usage.prompt_tokens if self.usage else None\n\n    @property\n    def output_tokens(self) -&gt; int | None:\n        \"\"\"Returns the number of output tokens.\"\"\"\n        return self.usage.completion_tokens if self.usage else None\n</code></pre>"},{"location":"api/core/groq/call_response/#mirascope.core.groq.call_response.GroqCallResponse.choice","title":"<code>choice: Choice</code>  <code>property</code>","text":"<p>Returns the 0th choice.</p>"},{"location":"api/core/groq/call_response/#mirascope.core.groq.call_response.GroqCallResponse.choices","title":"<code>choices: list[Choice]</code>  <code>property</code>","text":"<p>Returns the array of chat completion choices.</p>"},{"location":"api/core/groq/call_response/#mirascope.core.groq.call_response.GroqCallResponse.content","title":"<code>content: str</code>  <code>property</code>","text":"<p>Returns the content of the chat completion for the 0th choice.</p>"},{"location":"api/core/groq/call_response/#mirascope.core.groq.call_response.GroqCallResponse.finish_reasons","title":"<code>finish_reasons: list[str]</code>  <code>property</code>","text":"<p>Returns the finish reasons of the response.</p>"},{"location":"api/core/groq/call_response/#mirascope.core.groq.call_response.GroqCallResponse.id","title":"<code>id: str</code>  <code>property</code>","text":"<p>Returns the id of the response.</p>"},{"location":"api/core/groq/call_response/#mirascope.core.groq.call_response.GroqCallResponse.input_tokens","title":"<code>input_tokens: int | None</code>  <code>property</code>","text":"<p>Returns the number of input tokens.</p>"},{"location":"api/core/groq/call_response/#mirascope.core.groq.call_response.GroqCallResponse.message","title":"<code>message: ChatCompletionMessage</code>  <code>property</code>","text":"<p>Returns the message of the chat completion for the 0th choice.</p>"},{"location":"api/core/groq/call_response/#mirascope.core.groq.call_response.GroqCallResponse.message_param","title":"<code>message_param: ChatCompletionAssistantMessageParam</code>  <code>property</code>","text":"<p>Returns the assistants's response as a message parameter.</p>"},{"location":"api/core/groq/call_response/#mirascope.core.groq.call_response.GroqCallResponse.model","title":"<code>model: str</code>  <code>property</code>","text":"<p>Returns the name of the response model.</p>"},{"location":"api/core/groq/call_response/#mirascope.core.groq.call_response.GroqCallResponse.output_tokens","title":"<code>output_tokens: int | None</code>  <code>property</code>","text":"<p>Returns the number of output tokens.</p>"},{"location":"api/core/groq/call_response/#mirascope.core.groq.call_response.GroqCallResponse.tool","title":"<code>tool: GroqTool | None</code>  <code>property</code>","text":"<p>Returns the 0th tool for the 0th choice message.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if the tool call doesn't match the tool's schema.</p>"},{"location":"api/core/groq/call_response/#mirascope.core.groq.call_response.GroqCallResponse.tool_calls","title":"<code>tool_calls: list[ChatCompletionMessageToolCall] | None</code>  <code>property</code>","text":"<p>Returns the tool calls for the 0th choice message.</p>"},{"location":"api/core/groq/call_response/#mirascope.core.groq.call_response.GroqCallResponse.tools","title":"<code>tools: list[GroqTool] | None</code>  <code>property</code>","text":"<p>Returns any available tool calls as their <code>GroqTool</code> definition.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if a tool call doesn't match the tool's schema.</p>"},{"location":"api/core/groq/call_response/#mirascope.core.groq.call_response.GroqCallResponse.usage","title":"<code>usage: CompletionUsage | None</code>  <code>property</code>","text":"<p>Returns the usage of the chat completion.</p>"},{"location":"api/core/groq/call_response/#mirascope.core.groq.call_response.GroqCallResponse.tool_message_params","title":"<code>tool_message_params(tools_and_outputs)</code>  <code>classmethod</code>","text":"<p>Returns the tool message parameters for tool call results.</p> Source code in <code>mirascope/core/groq/call_response.py</code> <pre><code>@classmethod\ndef tool_message_params(\n    cls, tools_and_outputs: list[tuple[GroqTool, str]]\n) -&gt; list[ChatCompletionToolMessageParam]:\n    \"\"\"Returns the tool message parameters for tool call results.\"\"\"\n    return [\n        ChatCompletionToolMessageParam(\n            role=\"tool\",\n            content=output,\n            tool_call_id=tool.tool_call.id,\n            name=tool._name(),  # type: ignore\n        )\n        for tool, output in tools_and_outputs\n    ]\n</code></pre>"},{"location":"api/core/groq/call_response_chunk/","title":"mirascope.core.groq.call_response_chunk","text":"<p>This module contains the <code>GroqCallResponseChunk</code> class.</p>"},{"location":"api/core/groq/call_response_chunk/#mirascope.core.groq.call_response_chunk.GroqCallResponseChunk","title":"<code>GroqCallResponseChunk</code>","text":"<p>               Bases: <code>BaseCallResponseChunk[ChatCompletionChunk]</code></p> <p>A convenience wrapper around the Groq <code>ChatCompletionChunk</code> streamed chunks.</p> <p>When calling the Groq API using a function decorated with <code>groq_call</code> and <code>stream</code> set to <code>True</code>, the stream will contain <code>GroqResponseChunk</code> instances with properties that allow for more convenient access to commonly used attributes.</p> <p>Example:</p> <pre><code>from mirascope.core.groq import groq_call\n\n@groq_call(model=\"gpt-4o\", stream=True)\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\nstream = recommend_book(\"fantasy\")  # response is an `GroqStream`\nfor chunk in stream:\n    print(chunk.content, end=\"\", flush=True)\n#&gt; Sure! I would recommend...\n</code></pre> Source code in <code>mirascope/core/groq/call_response_chunk.py</code> <pre><code>class GroqCallResponseChunk(BaseCallResponseChunk[ChatCompletionChunk]):\n    '''A convenience wrapper around the Groq `ChatCompletionChunk` streamed chunks.\n\n    When calling the Groq API using a function decorated with `groq_call` and\n    `stream` set to `True`, the stream will contain `GroqResponseChunk` instances with\n    properties that allow for more convenient access to commonly used attributes.\n\n    Example:\n\n    ```python\n    from mirascope.core.groq import groq_call\n\n    @groq_call(model=\"gpt-4o\", stream=True)\n    def recommend_book(genre: str):\n        \"\"\"Recommend a {genre} book.\"\"\"\n\n    stream = recommend_book(\"fantasy\")  # response is an `GroqStream`\n    for chunk in stream:\n        print(chunk.content, end=\"\", flush=True)\n    #&gt; Sure! I would recommend...\n    ```\n    '''\n\n    @property\n    def choices(self) -&gt; list[Choice]:\n        \"\"\"Returns the array of chat completion choices.\"\"\"\n        return self.chunk.choices\n\n    @property\n    def choice(self) -&gt; Choice:\n        \"\"\"Returns the 0th choice.\"\"\"\n        return self.chunk.choices[0]\n\n    @property\n    def delta(self) -&gt; ChoiceDelta | None:\n        \"\"\"Returns the delta for the 0th choice.\"\"\"\n        if self.chunk.choices:\n            return self.chunk.choices[0].delta\n        return None\n\n    @property\n    def content(self) -&gt; str:\n        \"\"\"Returns the content for the 0th choice delta.\"\"\"\n        return (\n            self.delta.content if self.delta is not None and self.delta.content else \"\"\n        )\n\n    @property\n    def model(self) -&gt; str:\n        \"\"\"Returns the name of the response model.\"\"\"\n        return self.chunk.model\n\n    @property\n    def id(self) -&gt; str:\n        \"\"\"Returns the id of the response.\"\"\"\n        return self.chunk.id\n\n    @property\n    def finish_reasons(self) -&gt; list[str]:\n        \"\"\"Returns the finish reasons of the response.\"\"\"\n        return [str(choice.finish_reason) for choice in self.chunk.choices]\n\n    @property\n    def tool_calls(self) -&gt; list[ChoiceDeltaToolCall] | None:\n        \"\"\"Returns the partial tool calls for the 0th choice message.\n\n        The first `list[ChoiceDeltaToolCall]` will contain the name of the tool and\n        index, and subsequent `list[ChoiceDeltaToolCall]`s will contain the arguments\n        which will be strings that need to be concatenated with future\n        `list[ChoiceDeltaToolCall]`s to form a complete JSON tool calls. The last\n        `list[ChoiceDeltaToolCall]` will be None indicating end of stream.\n        \"\"\"\n        if self.delta:\n            return self.delta.tool_calls\n        return None\n\n    @property\n    def usage(self) -&gt; CompletionUsage | None:\n        \"\"\"Returns the usage of the chat completion.\"\"\"\n        if self.chunk.usage:\n            return self.chunk.usage\n        return None\n\n    @property\n    def input_tokens(self) -&gt; int | None:\n        \"\"\"Returns the number of input tokens.\"\"\"\n        if self.usage:\n            return self.usage.prompt_tokens\n        return None\n\n    @property\n    def output_tokens(self) -&gt; int | None:\n        \"\"\"Returns the number of output tokens.\"\"\"\n        if self.usage:\n            return self.usage.completion_tokens\n        return None\n</code></pre>"},{"location":"api/core/groq/call_response_chunk/#mirascope.core.groq.call_response_chunk.GroqCallResponseChunk.choice","title":"<code>choice: Choice</code>  <code>property</code>","text":"<p>Returns the 0th choice.</p>"},{"location":"api/core/groq/call_response_chunk/#mirascope.core.groq.call_response_chunk.GroqCallResponseChunk.choices","title":"<code>choices: list[Choice]</code>  <code>property</code>","text":"<p>Returns the array of chat completion choices.</p>"},{"location":"api/core/groq/call_response_chunk/#mirascope.core.groq.call_response_chunk.GroqCallResponseChunk.content","title":"<code>content: str</code>  <code>property</code>","text":"<p>Returns the content for the 0th choice delta.</p>"},{"location":"api/core/groq/call_response_chunk/#mirascope.core.groq.call_response_chunk.GroqCallResponseChunk.delta","title":"<code>delta: ChoiceDelta | None</code>  <code>property</code>","text":"<p>Returns the delta for the 0th choice.</p>"},{"location":"api/core/groq/call_response_chunk/#mirascope.core.groq.call_response_chunk.GroqCallResponseChunk.finish_reasons","title":"<code>finish_reasons: list[str]</code>  <code>property</code>","text":"<p>Returns the finish reasons of the response.</p>"},{"location":"api/core/groq/call_response_chunk/#mirascope.core.groq.call_response_chunk.GroqCallResponseChunk.id","title":"<code>id: str</code>  <code>property</code>","text":"<p>Returns the id of the response.</p>"},{"location":"api/core/groq/call_response_chunk/#mirascope.core.groq.call_response_chunk.GroqCallResponseChunk.input_tokens","title":"<code>input_tokens: int | None</code>  <code>property</code>","text":"<p>Returns the number of input tokens.</p>"},{"location":"api/core/groq/call_response_chunk/#mirascope.core.groq.call_response_chunk.GroqCallResponseChunk.model","title":"<code>model: str</code>  <code>property</code>","text":"<p>Returns the name of the response model.</p>"},{"location":"api/core/groq/call_response_chunk/#mirascope.core.groq.call_response_chunk.GroqCallResponseChunk.output_tokens","title":"<code>output_tokens: int | None</code>  <code>property</code>","text":"<p>Returns the number of output tokens.</p>"},{"location":"api/core/groq/call_response_chunk/#mirascope.core.groq.call_response_chunk.GroqCallResponseChunk.tool_calls","title":"<code>tool_calls: list[ChoiceDeltaToolCall] | None</code>  <code>property</code>","text":"<p>Returns the partial tool calls for the 0th choice message.</p> <p>The first <code>list[ChoiceDeltaToolCall]</code> will contain the name of the tool and index, and subsequent <code>list[ChoiceDeltaToolCall]</code>s will contain the arguments which will be strings that need to be concatenated with future <code>list[ChoiceDeltaToolCall]</code>s to form a complete JSON tool calls. The last <code>list[ChoiceDeltaToolCall]</code> will be None indicating end of stream.</p>"},{"location":"api/core/groq/call_response_chunk/#mirascope.core.groq.call_response_chunk.GroqCallResponseChunk.usage","title":"<code>usage: CompletionUsage | None</code>  <code>property</code>","text":"<p>Returns the usage of the chat completion.</p>"},{"location":"api/core/groq/dynamic_config/","title":"mirascope.core.groq.dynamic_config","text":"<p>This module defines the function return type for functions as LLM calls.</p>"},{"location":"api/core/groq/dynamic_config/#mirascope.core.groq.dynamic_config.GroqDynamicConfig","title":"<code>GroqDynamicConfig = BaseDynamicConfig[ChatCompletionMessageParam, GroqCallParams]</code>  <code>module-attribute</code>","text":"<p>The function return type for functions wrapped with the <code>groq_call</code> decorator.</p> <p>Attributes:</p> Name Type Description <code>computed_fields</code> <p>The computed fields to use in the prompt template.</p> <code>messages</code> <p>The messages to send to the Groq API. If provided, <code>computed_fields</code> will be ignored.</p> <code>call_params</code> <p>The call parameters to use when calling the Groq API. These will override any call parameters provided to the decorator.</p> <p>Example:</p> <pre><code>from mirascope.core.groq import GroqDynamicConfig, groq_call\n\n@groq_call(model=\"gpt-4o\")\ndef recommend_book(genre: str) -&gt; GroqDynamicConfig:\n    \"\"\"Recommend a {capitalized_genre} book.\"\"\"\n    reeturn {\"computed_fields\": {\"capitalized_genre\": genre.capitalize()}}\n</code></pre>"},{"location":"api/core/groq/tool/","title":"mirascope.core.groq.tool","text":"<p>The <code>GroqTool</code> class for easy tool usage with Groq LLM calls.</p>"},{"location":"api/core/groq/tool/#mirascope.core.groq.tool.GroqTool","title":"<code>GroqTool</code>","text":"<p>               Bases: <code>BaseTool</code></p> <p>A class for defining tools for Groq LLM calls.</p> Source code in <code>mirascope/core/groq/tool.py</code> <pre><code>class GroqTool(BaseTool):\n    \"\"\"A class for defining tools for Groq LLM calls.\"\"\"\n\n    tool_call: ChatCompletionMessageToolCall\n\n    @classmethod\n    def tool_schema(cls) -&gt; ChatCompletionToolParam:\n        \"\"\"Constructs a JSON Schema tool schema from the `BaseModel` schema defined.\"\"\"\n        model_schema = cls.model_json_schema()\n        model_schema.pop(\"title\", None)\n        model_schema.pop(\"description\", None)\n\n        fn = FunctionDefinition(name=cls._name(), description=cls._description())\n        if model_schema[\"properties\"]:\n            fn[\"parameters\"] = model_schema\n\n        return ChatCompletionToolParam(function=fn, type=\"function\")\n\n    @classmethod\n    def from_tool_call(cls, tool_call: ChatCompletionMessageToolCall) -&gt; GroqTool:\n        \"\"\"Constructs an `GroqTool` instance from a `tool_call`.\"\"\"\n        model_json = jiter.from_json(tool_call.function.arguments.encode())\n        model_json[\"tool_call\"] = tool_call.model_dump()\n        return cls.model_validate(model_json)\n</code></pre>"},{"location":"api/core/groq/tool/#mirascope.core.groq.tool.GroqTool.from_tool_call","title":"<code>from_tool_call(tool_call)</code>  <code>classmethod</code>","text":"<p>Constructs an <code>GroqTool</code> instance from a <code>tool_call</code>.</p> Source code in <code>mirascope/core/groq/tool.py</code> <pre><code>@classmethod\ndef from_tool_call(cls, tool_call: ChatCompletionMessageToolCall) -&gt; GroqTool:\n    \"\"\"Constructs an `GroqTool` instance from a `tool_call`.\"\"\"\n    model_json = jiter.from_json(tool_call.function.arguments.encode())\n    model_json[\"tool_call\"] = tool_call.model_dump()\n    return cls.model_validate(model_json)\n</code></pre>"},{"location":"api/core/groq/tool/#mirascope.core.groq.tool.GroqTool.tool_schema","title":"<code>tool_schema()</code>  <code>classmethod</code>","text":"<p>Constructs a JSON Schema tool schema from the <code>BaseModel</code> schema defined.</p> Source code in <code>mirascope/core/groq/tool.py</code> <pre><code>@classmethod\ndef tool_schema(cls) -&gt; ChatCompletionToolParam:\n    \"\"\"Constructs a JSON Schema tool schema from the `BaseModel` schema defined.\"\"\"\n    model_schema = cls.model_json_schema()\n    model_schema.pop(\"title\", None)\n    model_schema.pop(\"description\", None)\n\n    fn = FunctionDefinition(name=cls._name(), description=cls._description())\n    if model_schema[\"properties\"]:\n        fn[\"parameters\"] = model_schema\n\n    return ChatCompletionToolParam(function=fn, type=\"function\")\n</code></pre>"},{"location":"api/core/litellm/","title":"mirascope.core.litellm","text":"<p>The Mirascope LiteLLM Module.</p>"},{"location":"api/core/litellm/#mirascope.core.litellm.LiteLLMDynamicConfig","title":"<code>LiteLLMDynamicConfig = BaseDynamicConfig[ChatCompletionMessageParam, LiteLLMCallParams]</code>  <code>module-attribute</code>","text":"<p>The function return type for functions wrapped with the <code>litellm_call</code> decorator.</p> <p>Attributes:</p> Name Type Description <code>computed_fields</code> <p>The computed fields to use in the prompt template.</p> <code>messages</code> <p>The messages to send to the LiteLLM API. If provided, <code>computed_fields</code> will be ignored.</p> <code>call_params</code> <p>The call parameters to use when calling the LiteLLM API. These will override any call parameters provided to the decorator.</p> <p>Example:</p> <pre><code>from mirascope.core.litellm import LiteLLMDynamicConfig, litellm_call\n\n@litellm_call(model=\"gpt-4o\")\ndef recommend_book(genre: str) -&gt; LiteLLMDynamicConfig:\n    \"\"\"Recommend a {capitalized_genre} book.\"\"\"\n    reeturn {\"computed_fields\": {\"capitalized_genre\": genre.capitalize()}}\n</code></pre>"},{"location":"api/core/litellm/#mirascope.core.litellm.litellm_call","title":"<code>litellm_call = call_factory(TCallResponse=LiteLLMCallResponse, TCallResponseChunk=LiteLLMCallResponseChunk, TDynamicConfig=LiteLLMDynamicConfig, TMessageParamType=ChatCompletionAssistantMessageParam, TToolType=LiteLLMTool, TStream=LiteLLMStream, TCallParams=LiteLLMCallParams, default_call_params=LiteLLMCallParams(), setup_call=setup_call, get_json_output=get_json_output, handle_stream=handle_stream, handle_stream_async=handle_stream_async, calculate_cost=calculate_cost, provider='litellm')</code>  <code>module-attribute</code>","text":"<p>A decorator for calling the LiteLLM API with a typed function.</p> <p>This decorator is used to wrap a typed function that calls the LiteLLM API. It parses the docstring of the wrapped function as the messages array and templates the input arguments for the function into each message's template.</p> <p>Example:</p> <pre><code>@litellm_call(model=\"gpt-4o\")\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\nresponse = recommend_book(\"fantasy\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>The LiteLLM model to use in the API call.</p> required <code>stream</code> <p>Whether to stream the response from the API call.</p> required <code>tools</code> <p>The tools to use in the LiteLLM API call.</p> required <code>**call_params</code> <p>The <code>LiteLLMCallParams</code> call parameters to use in the API call.</p> required <p>Returns:</p> Type Description <p>The decorator for turning a typed function into an LiteLLM API call.</p>"},{"location":"api/core/litellm/#mirascope.core.litellm.LiteLLMCallParams","title":"<code>LiteLLMCallParams</code>","text":"<p>               Bases: <code>BaseCallParams</code></p> <p>The parameters to use when calling the LiteLLM API.</p> Source code in <code>mirascope/core/litellm/call_params.py</code> <pre><code>class LiteLLMCallParams(BaseCallParams):\n    \"\"\"The parameters to use when calling the LiteLLM API.\"\"\"\n\n    frequency_penalty: NotRequired[float | None]\n    logit_bias: NotRequired[dict[str, int] | None]\n    logprobs: NotRequired[bool | None]\n    max_tokens: NotRequired[int | None]\n    n: NotRequired[int | None]\n    parallel_tool_calls: NotRequired[bool]\n    presence_penalty: NotRequired[float | None]\n    response_format: NotRequired[ResponseFormat]\n    seed: NotRequired[int | None]\n    stop: NotRequired[str | list[str] | None]\n    stream_options: NotRequired[ChatCompletionStreamOptionsParam | None]\n    temperature: NotRequired[float | None]\n    tool_choice: NotRequired[ChatCompletionToolChoiceOptionParam]\n    top_logprobs: NotRequired[int | None]\n    top_p: NotRequired[float | None]\n    user: NotRequired[str]\n</code></pre>"},{"location":"api/core/litellm/#mirascope.core.litellm.LiteLLMCallResponse","title":"<code>LiteLLMCallResponse</code>","text":"<p>               Bases: <code>BaseCallResponse[ModelResponse, LiteLLMTool, LiteLLMDynamicConfig, ChatCompletionMessageParam, LiteLLMCallParams, ChatCompletionUserMessageParam]</code></p> <p>A convenience wrapper around the LiteLLM <code>ChatCompletion</code> response.</p> <p>When calling the LiteLLM API using a function decorated with <code>openai_call</code>, the response will be an <code>LiteLLMCallResponse</code> instance with properties that allow for more convenience access to commonly used attributes.</p> <p>Example:</p> <pre><code>from mirascope.core.openai import openai_call\n\n@openai_call(model=\"gpt-4o\")\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\nresponse = recommend_book(\"fantasy\")  # response is an `LiteLLMCallResponse` instance\nprint(response.content)\n#&gt; Sure! I would recommend...\n</code></pre> Source code in <code>mirascope/core/litellm/call_response.py</code> <pre><code>class LiteLLMCallResponse(\n    BaseCallResponse[\n        ModelResponse,\n        LiteLLMTool,\n        LiteLLMDynamicConfig,\n        ChatCompletionMessageParam,\n        LiteLLMCallParams,\n        ChatCompletionUserMessageParam,\n    ]\n):\n    '''A convenience wrapper around the LiteLLM `ChatCompletion` response.\n\n    When calling the LiteLLM API using a function decorated with `openai_call`, the\n    response will be an `LiteLLMCallResponse` instance with properties that allow for\n    more convenience access to commonly used attributes.\n\n    Example:\n\n    ```python\n    from mirascope.core.openai import openai_call\n\n    @openai_call(model=\"gpt-4o\")\n    def recommend_book(genre: str):\n        \"\"\"Recommend a {genre} book.\"\"\"\n\n    response = recommend_book(\"fantasy\")  # response is an `LiteLLMCallResponse` instance\n    print(response.content)\n    #&gt; Sure! I would recommend...\n    ```\n    '''\n\n    _provider = \"openai\"\n\n    @computed_field\n    @property\n    def message_param(self) -&gt; ChatCompletionAssistantMessageParam:\n        \"\"\"Returns the assistants's response as a message parameter.\"\"\"\n        return self.message.model_dump(exclude={\"function_call\"})  # type: ignore\n\n    @property\n    def choices(self) -&gt; list[Choice]:\n        \"\"\"Returns the array of chat completion choices.\"\"\"\n        return self.response.choices  # type: ignore\n\n    @property\n    def choice(self) -&gt; Choice:\n        \"\"\"Returns the 0th choice.\"\"\"\n        return self.choices[0]\n\n    @property\n    def message(self) -&gt; ChatCompletionMessage:\n        \"\"\"Returns the message of the chat completion for the 0th choice.\"\"\"\n        return self.choice.message\n\n    @property\n    def content(self) -&gt; str:\n        \"\"\"Returns the content of the chat completion for the 0th choice.\"\"\"\n        return self.message.content if self.message.content is not None else \"\"\n\n    @property\n    def model(self) -&gt; str | None:\n        \"\"\"Returns the name of the response model.\"\"\"\n        return self.response.model\n\n    @property\n    def id(self) -&gt; str:\n        \"\"\"Returns the id of the response.\"\"\"\n        return self.response.id\n\n    @property\n    def finish_reasons(self) -&gt; list[str]:\n        \"\"\"Returns the finish reasons of the response.\"\"\"\n        return [str(choice.finish_reason) for choice in self.response.choices]\n\n    @property\n    def tool_calls(self) -&gt; list[ChatCompletionMessageToolCall] | None:\n        \"\"\"Returns the tool calls for the 0th choice message.\"\"\"\n        return self.message.tool_calls\n\n    @computed_field\n    @property\n    def tools(self) -&gt; list[LiteLLMTool] | None:\n        \"\"\"Returns any available tool calls as their `LiteLLMTool` definition.\n\n        Raises:\n            ValidationError: if a tool call doesn't match the tool's schema.\n        \"\"\"\n        if not self.tool_types or not self.tool_calls:\n            return None\n\n        extracted_tools = []\n        for tool_call in self.tool_calls:\n            for tool_type in self.tool_types:\n                if tool_call.function.name == tool_type._name():\n                    extracted_tools.append(tool_type.from_tool_call(tool_call))\n                    break\n\n        return extracted_tools\n\n    @computed_field\n    @property\n    def tool(self) -&gt; LiteLLMTool | None:\n        \"\"\"Returns the 0th tool for the 0th choice message.\n\n        Raises:\n            ValidationError: if the tool call doesn't match the tool's schema.\n        \"\"\"\n        if tools := self.tools:\n            return tools[0]\n        return None\n\n    @classmethod\n    def tool_message_params(\n        cls, tools_and_outputs: list[tuple[LiteLLMTool, str]]\n    ) -&gt; list[ChatCompletionToolMessageParam]:\n        \"\"\"Returns the tool message parameters for tool call results.\"\"\"\n        return [\n            ChatCompletionToolMessageParam(\n                role=\"tool\",\n                content=output,\n                tool_call_id=tool.tool_call.id,\n                name=tool._name(),  # type: ignore\n            )\n            for tool, output in tools_and_outputs\n        ]\n\n    @property\n    def usage(self) -&gt; Any | None:\n        \"\"\"Returns the usage of the chat completion.\"\"\"\n        return self.response.usage  # type: ignore\n\n    @property\n    def input_tokens(self) -&gt; int | None:\n        \"\"\"Returns the number of input tokens.\"\"\"\n        return self.usage.prompt_tokens if self.usage else None\n\n    @property\n    def output_tokens(self) -&gt; int | None:\n        \"\"\"Returns the number of output tokens.\"\"\"\n        return self.usage.completion_tokens if self.usage else None\n</code></pre>"},{"location":"api/core/litellm/#mirascope.core.litellm.LiteLLMCallResponse.choice","title":"<code>choice: Choice</code>  <code>property</code>","text":"<p>Returns the 0th choice.</p>"},{"location":"api/core/litellm/#mirascope.core.litellm.LiteLLMCallResponse.choices","title":"<code>choices: list[Choice]</code>  <code>property</code>","text":"<p>Returns the array of chat completion choices.</p>"},{"location":"api/core/litellm/#mirascope.core.litellm.LiteLLMCallResponse.content","title":"<code>content: str</code>  <code>property</code>","text":"<p>Returns the content of the chat completion for the 0th choice.</p>"},{"location":"api/core/litellm/#mirascope.core.litellm.LiteLLMCallResponse.finish_reasons","title":"<code>finish_reasons: list[str]</code>  <code>property</code>","text":"<p>Returns the finish reasons of the response.</p>"},{"location":"api/core/litellm/#mirascope.core.litellm.LiteLLMCallResponse.id","title":"<code>id: str</code>  <code>property</code>","text":"<p>Returns the id of the response.</p>"},{"location":"api/core/litellm/#mirascope.core.litellm.LiteLLMCallResponse.input_tokens","title":"<code>input_tokens: int | None</code>  <code>property</code>","text":"<p>Returns the number of input tokens.</p>"},{"location":"api/core/litellm/#mirascope.core.litellm.LiteLLMCallResponse.message","title":"<code>message: ChatCompletionMessage</code>  <code>property</code>","text":"<p>Returns the message of the chat completion for the 0th choice.</p>"},{"location":"api/core/litellm/#mirascope.core.litellm.LiteLLMCallResponse.message_param","title":"<code>message_param: ChatCompletionAssistantMessageParam</code>  <code>property</code>","text":"<p>Returns the assistants's response as a message parameter.</p>"},{"location":"api/core/litellm/#mirascope.core.litellm.LiteLLMCallResponse.model","title":"<code>model: str | None</code>  <code>property</code>","text":"<p>Returns the name of the response model.</p>"},{"location":"api/core/litellm/#mirascope.core.litellm.LiteLLMCallResponse.output_tokens","title":"<code>output_tokens: int | None</code>  <code>property</code>","text":"<p>Returns the number of output tokens.</p>"},{"location":"api/core/litellm/#mirascope.core.litellm.LiteLLMCallResponse.tool","title":"<code>tool: LiteLLMTool | None</code>  <code>property</code>","text":"<p>Returns the 0th tool for the 0th choice message.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if the tool call doesn't match the tool's schema.</p>"},{"location":"api/core/litellm/#mirascope.core.litellm.LiteLLMCallResponse.tool_calls","title":"<code>tool_calls: list[ChatCompletionMessageToolCall] | None</code>  <code>property</code>","text":"<p>Returns the tool calls for the 0th choice message.</p>"},{"location":"api/core/litellm/#mirascope.core.litellm.LiteLLMCallResponse.tools","title":"<code>tools: list[LiteLLMTool] | None</code>  <code>property</code>","text":"<p>Returns any available tool calls as their <code>LiteLLMTool</code> definition.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if a tool call doesn't match the tool's schema.</p>"},{"location":"api/core/litellm/#mirascope.core.litellm.LiteLLMCallResponse.usage","title":"<code>usage: Any | None</code>  <code>property</code>","text":"<p>Returns the usage of the chat completion.</p>"},{"location":"api/core/litellm/#mirascope.core.litellm.LiteLLMCallResponse.tool_message_params","title":"<code>tool_message_params(tools_and_outputs)</code>  <code>classmethod</code>","text":"<p>Returns the tool message parameters for tool call results.</p> Source code in <code>mirascope/core/litellm/call_response.py</code> <pre><code>@classmethod\ndef tool_message_params(\n    cls, tools_and_outputs: list[tuple[LiteLLMTool, str]]\n) -&gt; list[ChatCompletionToolMessageParam]:\n    \"\"\"Returns the tool message parameters for tool call results.\"\"\"\n    return [\n        ChatCompletionToolMessageParam(\n            role=\"tool\",\n            content=output,\n            tool_call_id=tool.tool_call.id,\n            name=tool._name(),  # type: ignore\n        )\n        for tool, output in tools_and_outputs\n    ]\n</code></pre>"},{"location":"api/core/litellm/#mirascope.core.litellm.LiteLLMCallResponseChunk","title":"<code>LiteLLMCallResponseChunk</code>","text":"<p>               Bases: <code>BaseCallResponseChunk[ModelResponse]</code></p> <p>A convenience wrapper around the LiteLLM <code>ChatCompletionChunk</code> streamed chunks.</p> <p>When calling the LiteLLM API using a function decorated with <code>openai_call</code> and <code>stream</code> set to <code>True</code>, the stream will contain <code>LiteLLMResponseChunk</code> instances with properties that allow for more convenient access to commonly used attributes.</p> <p>Example:</p> <pre><code>from mirascope.core.openai import openai_call\n\n@openai_call(model=\"gpt-4o\", stream=True)\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\nstream = recommend_book(\"fantasy\")  # response is an `LiteLLMStream`\nfor chunk in stream:\n    print(chunk.content, end=\"\", flush=True)\n#&gt; Sure! I would recommend...\n</code></pre> Source code in <code>mirascope/core/litellm/call_response_chunk.py</code> <pre><code>class LiteLLMCallResponseChunk(BaseCallResponseChunk[ModelResponse]):\n    '''A convenience wrapper around the LiteLLM `ChatCompletionChunk` streamed chunks.\n\n    When calling the LiteLLM API using a function decorated with `openai_call` and\n    `stream` set to `True`, the stream will contain `LiteLLMResponseChunk` instances with\n    properties that allow for more convenient access to commonly used attributes.\n\n    Example:\n\n    ```python\n    from mirascope.core.openai import openai_call\n\n    @openai_call(model=\"gpt-4o\", stream=True)\n    def recommend_book(genre: str):\n        \"\"\"Recommend a {genre} book.\"\"\"\n\n    stream = recommend_book(\"fantasy\")  # response is an `LiteLLMStream`\n    for chunk in stream:\n        print(chunk.content, end=\"\", flush=True)\n    #&gt; Sure! I would recommend...\n    ```\n    '''\n\n    @property\n    def choices(self) -&gt; list[Choice]:\n        \"\"\"Returns the array of chat completion choices.\"\"\"\n        return self.chunk.choices  # type: ignore\n\n    @property\n    def choice(self) -&gt; Choice:\n        \"\"\"Returns the 0th choice.\"\"\"\n        return self.choices[0]\n\n    @property\n    def delta(self) -&gt; ChoiceDelta | None:\n        \"\"\"Returns the delta for the 0th choice.\"\"\"\n        if self.choices:\n            return self.choices[0].delta\n        return None\n\n    @property\n    def content(self) -&gt; str:\n        \"\"\"Returns the content for the 0th choice delta.\"\"\"\n        return (\n            self.delta.content if self.delta is not None and self.delta.content else \"\"\n        )\n\n    @property\n    def model(self) -&gt; str | None:\n        \"\"\"Returns the name of the response model.\"\"\"\n        return self.chunk.model\n\n    @property\n    def id(self) -&gt; str:\n        \"\"\"Returns the id of the response.\"\"\"\n        return self.chunk.id\n\n    @property\n    def finish_reasons(self) -&gt; list[str]:\n        \"\"\"Returns the finish reasons of the response.\"\"\"\n        return [str(choice.finish_reason) for choice in self.chunk.choices]\n\n    @property\n    def tool_calls(self) -&gt; list[ChoiceDeltaToolCall] | None:\n        \"\"\"Returns the partial tool calls for the 0th choice message.\n\n        The first `list[ChoiceDeltaToolCall]` will contain the name of the tool and\n        index, and subsequent `list[ChoiceDeltaToolCall]`s will contain the arguments\n        which will be strings that need to be concatenated with future\n        `list[ChoiceDeltaToolCall]`s to form a complete JSON tool calls. The last\n        `list[ChoiceDeltaToolCall]` will be None indicating end of stream.\n        \"\"\"\n        if self.delta:\n            return self.delta.tool_calls\n        return None\n\n    @property\n    def usage(self) -&gt; CompletionUsage | None:\n        \"\"\"Returns the usage of the chat completion.\"\"\"\n        if hasattr(self.chunk, \"usage\") and (usage := getattr(self.chunk, \"usage\")):\n            return usage  # type: ignore\n        return None\n\n    @property\n    def input_tokens(self) -&gt; int | None:\n        \"\"\"Returns the number of input tokens.\"\"\"\n        if self.usage:\n            return self.usage.prompt_tokens\n        return None\n\n    @property\n    def output_tokens(self) -&gt; int | None:\n        \"\"\"Returns the number of output tokens.\"\"\"\n        if self.usage:\n            return self.usage.completion_tokens\n        return None\n</code></pre>"},{"location":"api/core/litellm/#mirascope.core.litellm.LiteLLMCallResponseChunk.choice","title":"<code>choice: Choice</code>  <code>property</code>","text":"<p>Returns the 0th choice.</p>"},{"location":"api/core/litellm/#mirascope.core.litellm.LiteLLMCallResponseChunk.choices","title":"<code>choices: list[Choice]</code>  <code>property</code>","text":"<p>Returns the array of chat completion choices.</p>"},{"location":"api/core/litellm/#mirascope.core.litellm.LiteLLMCallResponseChunk.content","title":"<code>content: str</code>  <code>property</code>","text":"<p>Returns the content for the 0th choice delta.</p>"},{"location":"api/core/litellm/#mirascope.core.litellm.LiteLLMCallResponseChunk.delta","title":"<code>delta: ChoiceDelta | None</code>  <code>property</code>","text":"<p>Returns the delta for the 0th choice.</p>"},{"location":"api/core/litellm/#mirascope.core.litellm.LiteLLMCallResponseChunk.finish_reasons","title":"<code>finish_reasons: list[str]</code>  <code>property</code>","text":"<p>Returns the finish reasons of the response.</p>"},{"location":"api/core/litellm/#mirascope.core.litellm.LiteLLMCallResponseChunk.id","title":"<code>id: str</code>  <code>property</code>","text":"<p>Returns the id of the response.</p>"},{"location":"api/core/litellm/#mirascope.core.litellm.LiteLLMCallResponseChunk.input_tokens","title":"<code>input_tokens: int | None</code>  <code>property</code>","text":"<p>Returns the number of input tokens.</p>"},{"location":"api/core/litellm/#mirascope.core.litellm.LiteLLMCallResponseChunk.model","title":"<code>model: str | None</code>  <code>property</code>","text":"<p>Returns the name of the response model.</p>"},{"location":"api/core/litellm/#mirascope.core.litellm.LiteLLMCallResponseChunk.output_tokens","title":"<code>output_tokens: int | None</code>  <code>property</code>","text":"<p>Returns the number of output tokens.</p>"},{"location":"api/core/litellm/#mirascope.core.litellm.LiteLLMCallResponseChunk.tool_calls","title":"<code>tool_calls: list[ChoiceDeltaToolCall] | None</code>  <code>property</code>","text":"<p>Returns the partial tool calls for the 0th choice message.</p> <p>The first <code>list[ChoiceDeltaToolCall]</code> will contain the name of the tool and index, and subsequent <code>list[ChoiceDeltaToolCall]</code>s will contain the arguments which will be strings that need to be concatenated with future <code>list[ChoiceDeltaToolCall]</code>s to form a complete JSON tool calls. The last <code>list[ChoiceDeltaToolCall]</code> will be None indicating end of stream.</p>"},{"location":"api/core/litellm/#mirascope.core.litellm.LiteLLMCallResponseChunk.usage","title":"<code>usage: CompletionUsage | None</code>  <code>property</code>","text":"<p>Returns the usage of the chat completion.</p>"},{"location":"api/core/litellm/#mirascope.core.litellm.LiteLLMTool","title":"<code>LiteLLMTool</code>","text":"<p>               Bases: <code>BaseTool</code></p> <p>A class for defining tools for LiteLLM LLM calls.</p> Source code in <code>mirascope/core/litellm/tool.py</code> <pre><code>class LiteLLMTool(BaseTool):\n    \"\"\"A class for defining tools for LiteLLM LLM calls.\"\"\"\n\n    tool_call: ChatCompletionMessageToolCall\n\n    @classmethod\n    def tool_schema(cls) -&gt; ChatCompletionToolParam:\n        \"\"\"Constructs a JSON Schema tool schema from the `BaseModel` schema defined.\"\"\"\n        model_schema = cls.model_json_schema()\n        model_schema.pop(\"title\", None)\n        model_schema.pop(\"description\", None)\n\n        fn = FunctionDefinition(name=cls._name(), description=cls._description())\n        if model_schema[\"properties\"]:\n            fn[\"parameters\"] = model_schema\n\n        return ChatCompletionToolParam(function=fn, type=\"function\")\n\n    @classmethod\n    def from_tool_call(cls, tool_call: ChatCompletionMessageToolCall) -&gt; LiteLLMTool:\n        \"\"\"Constructs an `LiteLLMTool` instance from a `tool_call`.\"\"\"\n        model_json = jiter.from_json(tool_call.function.arguments.encode())\n        model_json[\"tool_call\"] = tool_call.model_dump()\n        return cls.model_validate(model_json)\n</code></pre>"},{"location":"api/core/litellm/#mirascope.core.litellm.LiteLLMTool.from_tool_call","title":"<code>from_tool_call(tool_call)</code>  <code>classmethod</code>","text":"<p>Constructs an <code>LiteLLMTool</code> instance from a <code>tool_call</code>.</p> Source code in <code>mirascope/core/litellm/tool.py</code> <pre><code>@classmethod\ndef from_tool_call(cls, tool_call: ChatCompletionMessageToolCall) -&gt; LiteLLMTool:\n    \"\"\"Constructs an `LiteLLMTool` instance from a `tool_call`.\"\"\"\n    model_json = jiter.from_json(tool_call.function.arguments.encode())\n    model_json[\"tool_call\"] = tool_call.model_dump()\n    return cls.model_validate(model_json)\n</code></pre>"},{"location":"api/core/litellm/#mirascope.core.litellm.LiteLLMTool.tool_schema","title":"<code>tool_schema()</code>  <code>classmethod</code>","text":"<p>Constructs a JSON Schema tool schema from the <code>BaseModel</code> schema defined.</p> Source code in <code>mirascope/core/litellm/tool.py</code> <pre><code>@classmethod\ndef tool_schema(cls) -&gt; ChatCompletionToolParam:\n    \"\"\"Constructs a JSON Schema tool schema from the `BaseModel` schema defined.\"\"\"\n    model_schema = cls.model_json_schema()\n    model_schema.pop(\"title\", None)\n    model_schema.pop(\"description\", None)\n\n    fn = FunctionDefinition(name=cls._name(), description=cls._description())\n    if model_schema[\"properties\"]:\n        fn[\"parameters\"] = model_schema\n\n    return ChatCompletionToolParam(function=fn, type=\"function\")\n</code></pre>"},{"location":"api/core/litellm/call/","title":"mirascope.core.litellm.call","text":"<p>The <code>litellm_call</code> decorator for functions as LLM calls.</p>"},{"location":"api/core/litellm/call/#mirascope.core.litellm.call.litellm_call","title":"<code>litellm_call = call_factory(TCallResponse=LiteLLMCallResponse, TCallResponseChunk=LiteLLMCallResponseChunk, TDynamicConfig=LiteLLMDynamicConfig, TMessageParamType=ChatCompletionAssistantMessageParam, TToolType=LiteLLMTool, TStream=LiteLLMStream, TCallParams=LiteLLMCallParams, default_call_params=LiteLLMCallParams(), setup_call=setup_call, get_json_output=get_json_output, handle_stream=handle_stream, handle_stream_async=handle_stream_async, calculate_cost=calculate_cost, provider='litellm')</code>  <code>module-attribute</code>","text":"<p>A decorator for calling the LiteLLM API with a typed function.</p> <p>This decorator is used to wrap a typed function that calls the LiteLLM API. It parses the docstring of the wrapped function as the messages array and templates the input arguments for the function into each message's template.</p> <p>Example:</p> <pre><code>@litellm_call(model=\"gpt-4o\")\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\nresponse = recommend_book(\"fantasy\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>The LiteLLM model to use in the API call.</p> required <code>stream</code> <p>Whether to stream the response from the API call.</p> required <code>tools</code> <p>The tools to use in the LiteLLM API call.</p> required <code>**call_params</code> <p>The <code>LiteLLMCallParams</code> call parameters to use in the API call.</p> required <p>Returns:</p> Type Description <p>The decorator for turning a typed function into an LiteLLM API call.</p>"},{"location":"api/core/litellm/call_params/","title":"mirascope.core.litellm.call_params","text":"<p>The parameters to use when calling the LiteLLM API.</p>"},{"location":"api/core/litellm/call_params/#mirascope.core.litellm.call_params.LiteLLMCallParams","title":"<code>LiteLLMCallParams</code>","text":"<p>               Bases: <code>BaseCallParams</code></p> <p>The parameters to use when calling the LiteLLM API.</p> Source code in <code>mirascope/core/litellm/call_params.py</code> <pre><code>class LiteLLMCallParams(BaseCallParams):\n    \"\"\"The parameters to use when calling the LiteLLM API.\"\"\"\n\n    frequency_penalty: NotRequired[float | None]\n    logit_bias: NotRequired[dict[str, int] | None]\n    logprobs: NotRequired[bool | None]\n    max_tokens: NotRequired[int | None]\n    n: NotRequired[int | None]\n    parallel_tool_calls: NotRequired[bool]\n    presence_penalty: NotRequired[float | None]\n    response_format: NotRequired[ResponseFormat]\n    seed: NotRequired[int | None]\n    stop: NotRequired[str | list[str] | None]\n    stream_options: NotRequired[ChatCompletionStreamOptionsParam | None]\n    temperature: NotRequired[float | None]\n    tool_choice: NotRequired[ChatCompletionToolChoiceOptionParam]\n    top_logprobs: NotRequired[int | None]\n    top_p: NotRequired[float | None]\n    user: NotRequired[str]\n</code></pre>"},{"location":"api/core/litellm/call_response/","title":"mirascope.core.litellm.call_response","text":"<p>This module contains the <code>LiteLLMCallResponse</code> class.</p>"},{"location":"api/core/litellm/call_response/#mirascope.core.litellm.call_response.LiteLLMCallResponse","title":"<code>LiteLLMCallResponse</code>","text":"<p>               Bases: <code>BaseCallResponse[ModelResponse, LiteLLMTool, LiteLLMDynamicConfig, ChatCompletionMessageParam, LiteLLMCallParams, ChatCompletionUserMessageParam]</code></p> <p>A convenience wrapper around the LiteLLM <code>ChatCompletion</code> response.</p> <p>When calling the LiteLLM API using a function decorated with <code>openai_call</code>, the response will be an <code>LiteLLMCallResponse</code> instance with properties that allow for more convenience access to commonly used attributes.</p> <p>Example:</p> <pre><code>from mirascope.core.openai import openai_call\n\n@openai_call(model=\"gpt-4o\")\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\nresponse = recommend_book(\"fantasy\")  # response is an `LiteLLMCallResponse` instance\nprint(response.content)\n#&gt; Sure! I would recommend...\n</code></pre> Source code in <code>mirascope/core/litellm/call_response.py</code> <pre><code>class LiteLLMCallResponse(\n    BaseCallResponse[\n        ModelResponse,\n        LiteLLMTool,\n        LiteLLMDynamicConfig,\n        ChatCompletionMessageParam,\n        LiteLLMCallParams,\n        ChatCompletionUserMessageParam,\n    ]\n):\n    '''A convenience wrapper around the LiteLLM `ChatCompletion` response.\n\n    When calling the LiteLLM API using a function decorated with `openai_call`, the\n    response will be an `LiteLLMCallResponse` instance with properties that allow for\n    more convenience access to commonly used attributes.\n\n    Example:\n\n    ```python\n    from mirascope.core.openai import openai_call\n\n    @openai_call(model=\"gpt-4o\")\n    def recommend_book(genre: str):\n        \"\"\"Recommend a {genre} book.\"\"\"\n\n    response = recommend_book(\"fantasy\")  # response is an `LiteLLMCallResponse` instance\n    print(response.content)\n    #&gt; Sure! I would recommend...\n    ```\n    '''\n\n    _provider = \"openai\"\n\n    @computed_field\n    @property\n    def message_param(self) -&gt; ChatCompletionAssistantMessageParam:\n        \"\"\"Returns the assistants's response as a message parameter.\"\"\"\n        return self.message.model_dump(exclude={\"function_call\"})  # type: ignore\n\n    @property\n    def choices(self) -&gt; list[Choice]:\n        \"\"\"Returns the array of chat completion choices.\"\"\"\n        return self.response.choices  # type: ignore\n\n    @property\n    def choice(self) -&gt; Choice:\n        \"\"\"Returns the 0th choice.\"\"\"\n        return self.choices[0]\n\n    @property\n    def message(self) -&gt; ChatCompletionMessage:\n        \"\"\"Returns the message of the chat completion for the 0th choice.\"\"\"\n        return self.choice.message\n\n    @property\n    def content(self) -&gt; str:\n        \"\"\"Returns the content of the chat completion for the 0th choice.\"\"\"\n        return self.message.content if self.message.content is not None else \"\"\n\n    @property\n    def model(self) -&gt; str | None:\n        \"\"\"Returns the name of the response model.\"\"\"\n        return self.response.model\n\n    @property\n    def id(self) -&gt; str:\n        \"\"\"Returns the id of the response.\"\"\"\n        return self.response.id\n\n    @property\n    def finish_reasons(self) -&gt; list[str]:\n        \"\"\"Returns the finish reasons of the response.\"\"\"\n        return [str(choice.finish_reason) for choice in self.response.choices]\n\n    @property\n    def tool_calls(self) -&gt; list[ChatCompletionMessageToolCall] | None:\n        \"\"\"Returns the tool calls for the 0th choice message.\"\"\"\n        return self.message.tool_calls\n\n    @computed_field\n    @property\n    def tools(self) -&gt; list[LiteLLMTool] | None:\n        \"\"\"Returns any available tool calls as their `LiteLLMTool` definition.\n\n        Raises:\n            ValidationError: if a tool call doesn't match the tool's schema.\n        \"\"\"\n        if not self.tool_types or not self.tool_calls:\n            return None\n\n        extracted_tools = []\n        for tool_call in self.tool_calls:\n            for tool_type in self.tool_types:\n                if tool_call.function.name == tool_type._name():\n                    extracted_tools.append(tool_type.from_tool_call(tool_call))\n                    break\n\n        return extracted_tools\n\n    @computed_field\n    @property\n    def tool(self) -&gt; LiteLLMTool | None:\n        \"\"\"Returns the 0th tool for the 0th choice message.\n\n        Raises:\n            ValidationError: if the tool call doesn't match the tool's schema.\n        \"\"\"\n        if tools := self.tools:\n            return tools[0]\n        return None\n\n    @classmethod\n    def tool_message_params(\n        cls, tools_and_outputs: list[tuple[LiteLLMTool, str]]\n    ) -&gt; list[ChatCompletionToolMessageParam]:\n        \"\"\"Returns the tool message parameters for tool call results.\"\"\"\n        return [\n            ChatCompletionToolMessageParam(\n                role=\"tool\",\n                content=output,\n                tool_call_id=tool.tool_call.id,\n                name=tool._name(),  # type: ignore\n            )\n            for tool, output in tools_and_outputs\n        ]\n\n    @property\n    def usage(self) -&gt; Any | None:\n        \"\"\"Returns the usage of the chat completion.\"\"\"\n        return self.response.usage  # type: ignore\n\n    @property\n    def input_tokens(self) -&gt; int | None:\n        \"\"\"Returns the number of input tokens.\"\"\"\n        return self.usage.prompt_tokens if self.usage else None\n\n    @property\n    def output_tokens(self) -&gt; int | None:\n        \"\"\"Returns the number of output tokens.\"\"\"\n        return self.usage.completion_tokens if self.usage else None\n</code></pre>"},{"location":"api/core/litellm/call_response/#mirascope.core.litellm.call_response.LiteLLMCallResponse.choice","title":"<code>choice: Choice</code>  <code>property</code>","text":"<p>Returns the 0th choice.</p>"},{"location":"api/core/litellm/call_response/#mirascope.core.litellm.call_response.LiteLLMCallResponse.choices","title":"<code>choices: list[Choice]</code>  <code>property</code>","text":"<p>Returns the array of chat completion choices.</p>"},{"location":"api/core/litellm/call_response/#mirascope.core.litellm.call_response.LiteLLMCallResponse.content","title":"<code>content: str</code>  <code>property</code>","text":"<p>Returns the content of the chat completion for the 0th choice.</p>"},{"location":"api/core/litellm/call_response/#mirascope.core.litellm.call_response.LiteLLMCallResponse.finish_reasons","title":"<code>finish_reasons: list[str]</code>  <code>property</code>","text":"<p>Returns the finish reasons of the response.</p>"},{"location":"api/core/litellm/call_response/#mirascope.core.litellm.call_response.LiteLLMCallResponse.id","title":"<code>id: str</code>  <code>property</code>","text":"<p>Returns the id of the response.</p>"},{"location":"api/core/litellm/call_response/#mirascope.core.litellm.call_response.LiteLLMCallResponse.input_tokens","title":"<code>input_tokens: int | None</code>  <code>property</code>","text":"<p>Returns the number of input tokens.</p>"},{"location":"api/core/litellm/call_response/#mirascope.core.litellm.call_response.LiteLLMCallResponse.message","title":"<code>message: ChatCompletionMessage</code>  <code>property</code>","text":"<p>Returns the message of the chat completion for the 0th choice.</p>"},{"location":"api/core/litellm/call_response/#mirascope.core.litellm.call_response.LiteLLMCallResponse.message_param","title":"<code>message_param: ChatCompletionAssistantMessageParam</code>  <code>property</code>","text":"<p>Returns the assistants's response as a message parameter.</p>"},{"location":"api/core/litellm/call_response/#mirascope.core.litellm.call_response.LiteLLMCallResponse.model","title":"<code>model: str | None</code>  <code>property</code>","text":"<p>Returns the name of the response model.</p>"},{"location":"api/core/litellm/call_response/#mirascope.core.litellm.call_response.LiteLLMCallResponse.output_tokens","title":"<code>output_tokens: int | None</code>  <code>property</code>","text":"<p>Returns the number of output tokens.</p>"},{"location":"api/core/litellm/call_response/#mirascope.core.litellm.call_response.LiteLLMCallResponse.tool","title":"<code>tool: LiteLLMTool | None</code>  <code>property</code>","text":"<p>Returns the 0th tool for the 0th choice message.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if the tool call doesn't match the tool's schema.</p>"},{"location":"api/core/litellm/call_response/#mirascope.core.litellm.call_response.LiteLLMCallResponse.tool_calls","title":"<code>tool_calls: list[ChatCompletionMessageToolCall] | None</code>  <code>property</code>","text":"<p>Returns the tool calls for the 0th choice message.</p>"},{"location":"api/core/litellm/call_response/#mirascope.core.litellm.call_response.LiteLLMCallResponse.tools","title":"<code>tools: list[LiteLLMTool] | None</code>  <code>property</code>","text":"<p>Returns any available tool calls as their <code>LiteLLMTool</code> definition.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if a tool call doesn't match the tool's schema.</p>"},{"location":"api/core/litellm/call_response/#mirascope.core.litellm.call_response.LiteLLMCallResponse.usage","title":"<code>usage: Any | None</code>  <code>property</code>","text":"<p>Returns the usage of the chat completion.</p>"},{"location":"api/core/litellm/call_response/#mirascope.core.litellm.call_response.LiteLLMCallResponse.tool_message_params","title":"<code>tool_message_params(tools_and_outputs)</code>  <code>classmethod</code>","text":"<p>Returns the tool message parameters for tool call results.</p> Source code in <code>mirascope/core/litellm/call_response.py</code> <pre><code>@classmethod\ndef tool_message_params(\n    cls, tools_and_outputs: list[tuple[LiteLLMTool, str]]\n) -&gt; list[ChatCompletionToolMessageParam]:\n    \"\"\"Returns the tool message parameters for tool call results.\"\"\"\n    return [\n        ChatCompletionToolMessageParam(\n            role=\"tool\",\n            content=output,\n            tool_call_id=tool.tool_call.id,\n            name=tool._name(),  # type: ignore\n        )\n        for tool, output in tools_and_outputs\n    ]\n</code></pre>"},{"location":"api/core/litellm/call_response_chunk/","title":"mirascope.core.litellm.call_response_chunk","text":"<p>This module contains the <code>LiteLLMCallResponseChunk</code> class.</p>"},{"location":"api/core/litellm/call_response_chunk/#mirascope.core.litellm.call_response_chunk.LiteLLMCallResponseChunk","title":"<code>LiteLLMCallResponseChunk</code>","text":"<p>               Bases: <code>BaseCallResponseChunk[ModelResponse]</code></p> <p>A convenience wrapper around the LiteLLM <code>ChatCompletionChunk</code> streamed chunks.</p> <p>When calling the LiteLLM API using a function decorated with <code>openai_call</code> and <code>stream</code> set to <code>True</code>, the stream will contain <code>LiteLLMResponseChunk</code> instances with properties that allow for more convenient access to commonly used attributes.</p> <p>Example:</p> <pre><code>from mirascope.core.openai import openai_call\n\n@openai_call(model=\"gpt-4o\", stream=True)\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\nstream = recommend_book(\"fantasy\")  # response is an `LiteLLMStream`\nfor chunk in stream:\n    print(chunk.content, end=\"\", flush=True)\n#&gt; Sure! I would recommend...\n</code></pre> Source code in <code>mirascope/core/litellm/call_response_chunk.py</code> <pre><code>class LiteLLMCallResponseChunk(BaseCallResponseChunk[ModelResponse]):\n    '''A convenience wrapper around the LiteLLM `ChatCompletionChunk` streamed chunks.\n\n    When calling the LiteLLM API using a function decorated with `openai_call` and\n    `stream` set to `True`, the stream will contain `LiteLLMResponseChunk` instances with\n    properties that allow for more convenient access to commonly used attributes.\n\n    Example:\n\n    ```python\n    from mirascope.core.openai import openai_call\n\n    @openai_call(model=\"gpt-4o\", stream=True)\n    def recommend_book(genre: str):\n        \"\"\"Recommend a {genre} book.\"\"\"\n\n    stream = recommend_book(\"fantasy\")  # response is an `LiteLLMStream`\n    for chunk in stream:\n        print(chunk.content, end=\"\", flush=True)\n    #&gt; Sure! I would recommend...\n    ```\n    '''\n\n    @property\n    def choices(self) -&gt; list[Choice]:\n        \"\"\"Returns the array of chat completion choices.\"\"\"\n        return self.chunk.choices  # type: ignore\n\n    @property\n    def choice(self) -&gt; Choice:\n        \"\"\"Returns the 0th choice.\"\"\"\n        return self.choices[0]\n\n    @property\n    def delta(self) -&gt; ChoiceDelta | None:\n        \"\"\"Returns the delta for the 0th choice.\"\"\"\n        if self.choices:\n            return self.choices[0].delta\n        return None\n\n    @property\n    def content(self) -&gt; str:\n        \"\"\"Returns the content for the 0th choice delta.\"\"\"\n        return (\n            self.delta.content if self.delta is not None and self.delta.content else \"\"\n        )\n\n    @property\n    def model(self) -&gt; str | None:\n        \"\"\"Returns the name of the response model.\"\"\"\n        return self.chunk.model\n\n    @property\n    def id(self) -&gt; str:\n        \"\"\"Returns the id of the response.\"\"\"\n        return self.chunk.id\n\n    @property\n    def finish_reasons(self) -&gt; list[str]:\n        \"\"\"Returns the finish reasons of the response.\"\"\"\n        return [str(choice.finish_reason) for choice in self.chunk.choices]\n\n    @property\n    def tool_calls(self) -&gt; list[ChoiceDeltaToolCall] | None:\n        \"\"\"Returns the partial tool calls for the 0th choice message.\n\n        The first `list[ChoiceDeltaToolCall]` will contain the name of the tool and\n        index, and subsequent `list[ChoiceDeltaToolCall]`s will contain the arguments\n        which will be strings that need to be concatenated with future\n        `list[ChoiceDeltaToolCall]`s to form a complete JSON tool calls. The last\n        `list[ChoiceDeltaToolCall]` will be None indicating end of stream.\n        \"\"\"\n        if self.delta:\n            return self.delta.tool_calls\n        return None\n\n    @property\n    def usage(self) -&gt; CompletionUsage | None:\n        \"\"\"Returns the usage of the chat completion.\"\"\"\n        if hasattr(self.chunk, \"usage\") and (usage := getattr(self.chunk, \"usage\")):\n            return usage  # type: ignore\n        return None\n\n    @property\n    def input_tokens(self) -&gt; int | None:\n        \"\"\"Returns the number of input tokens.\"\"\"\n        if self.usage:\n            return self.usage.prompt_tokens\n        return None\n\n    @property\n    def output_tokens(self) -&gt; int | None:\n        \"\"\"Returns the number of output tokens.\"\"\"\n        if self.usage:\n            return self.usage.completion_tokens\n        return None\n</code></pre>"},{"location":"api/core/litellm/call_response_chunk/#mirascope.core.litellm.call_response_chunk.LiteLLMCallResponseChunk.choice","title":"<code>choice: Choice</code>  <code>property</code>","text":"<p>Returns the 0th choice.</p>"},{"location":"api/core/litellm/call_response_chunk/#mirascope.core.litellm.call_response_chunk.LiteLLMCallResponseChunk.choices","title":"<code>choices: list[Choice]</code>  <code>property</code>","text":"<p>Returns the array of chat completion choices.</p>"},{"location":"api/core/litellm/call_response_chunk/#mirascope.core.litellm.call_response_chunk.LiteLLMCallResponseChunk.content","title":"<code>content: str</code>  <code>property</code>","text":"<p>Returns the content for the 0th choice delta.</p>"},{"location":"api/core/litellm/call_response_chunk/#mirascope.core.litellm.call_response_chunk.LiteLLMCallResponseChunk.delta","title":"<code>delta: ChoiceDelta | None</code>  <code>property</code>","text":"<p>Returns the delta for the 0th choice.</p>"},{"location":"api/core/litellm/call_response_chunk/#mirascope.core.litellm.call_response_chunk.LiteLLMCallResponseChunk.finish_reasons","title":"<code>finish_reasons: list[str]</code>  <code>property</code>","text":"<p>Returns the finish reasons of the response.</p>"},{"location":"api/core/litellm/call_response_chunk/#mirascope.core.litellm.call_response_chunk.LiteLLMCallResponseChunk.id","title":"<code>id: str</code>  <code>property</code>","text":"<p>Returns the id of the response.</p>"},{"location":"api/core/litellm/call_response_chunk/#mirascope.core.litellm.call_response_chunk.LiteLLMCallResponseChunk.input_tokens","title":"<code>input_tokens: int | None</code>  <code>property</code>","text":"<p>Returns the number of input tokens.</p>"},{"location":"api/core/litellm/call_response_chunk/#mirascope.core.litellm.call_response_chunk.LiteLLMCallResponseChunk.model","title":"<code>model: str | None</code>  <code>property</code>","text":"<p>Returns the name of the response model.</p>"},{"location":"api/core/litellm/call_response_chunk/#mirascope.core.litellm.call_response_chunk.LiteLLMCallResponseChunk.output_tokens","title":"<code>output_tokens: int | None</code>  <code>property</code>","text":"<p>Returns the number of output tokens.</p>"},{"location":"api/core/litellm/call_response_chunk/#mirascope.core.litellm.call_response_chunk.LiteLLMCallResponseChunk.tool_calls","title":"<code>tool_calls: list[ChoiceDeltaToolCall] | None</code>  <code>property</code>","text":"<p>Returns the partial tool calls for the 0th choice message.</p> <p>The first <code>list[ChoiceDeltaToolCall]</code> will contain the name of the tool and index, and subsequent <code>list[ChoiceDeltaToolCall]</code>s will contain the arguments which will be strings that need to be concatenated with future <code>list[ChoiceDeltaToolCall]</code>s to form a complete JSON tool calls. The last <code>list[ChoiceDeltaToolCall]</code> will be None indicating end of stream.</p>"},{"location":"api/core/litellm/call_response_chunk/#mirascope.core.litellm.call_response_chunk.LiteLLMCallResponseChunk.usage","title":"<code>usage: CompletionUsage | None</code>  <code>property</code>","text":"<p>Returns the usage of the chat completion.</p>"},{"location":"api/core/litellm/dynamic_config/","title":"mirascope.core.litellm.dynamic_config","text":"<p>This module defines the function return type for functions as LLM calls.</p>"},{"location":"api/core/litellm/dynamic_config/#mirascope.core.litellm.dynamic_config.LiteLLMDynamicConfig","title":"<code>LiteLLMDynamicConfig = BaseDynamicConfig[ChatCompletionMessageParam, LiteLLMCallParams]</code>  <code>module-attribute</code>","text":"<p>The function return type for functions wrapped with the <code>litellm_call</code> decorator.</p> <p>Attributes:</p> Name Type Description <code>computed_fields</code> <p>The computed fields to use in the prompt template.</p> <code>messages</code> <p>The messages to send to the LiteLLM API. If provided, <code>computed_fields</code> will be ignored.</p> <code>call_params</code> <p>The call parameters to use when calling the LiteLLM API. These will override any call parameters provided to the decorator.</p> <p>Example:</p> <pre><code>from mirascope.core.litellm import LiteLLMDynamicConfig, litellm_call\n\n@litellm_call(model=\"gpt-4o\")\ndef recommend_book(genre: str) -&gt; LiteLLMDynamicConfig:\n    \"\"\"Recommend a {capitalized_genre} book.\"\"\"\n    reeturn {\"computed_fields\": {\"capitalized_genre\": genre.capitalize()}}\n</code></pre>"},{"location":"api/core/litellm/tool/","title":"mirascope.core.litellm.tool","text":"<p>The <code>LiteLLMTool</code> class for easy tool usage with LiteLLM LLM calls.</p>"},{"location":"api/core/litellm/tool/#mirascope.core.litellm.tool.LiteLLMTool","title":"<code>LiteLLMTool</code>","text":"<p>               Bases: <code>BaseTool</code></p> <p>A class for defining tools for LiteLLM LLM calls.</p> Source code in <code>mirascope/core/litellm/tool.py</code> <pre><code>class LiteLLMTool(BaseTool):\n    \"\"\"A class for defining tools for LiteLLM LLM calls.\"\"\"\n\n    tool_call: ChatCompletionMessageToolCall\n\n    @classmethod\n    def tool_schema(cls) -&gt; ChatCompletionToolParam:\n        \"\"\"Constructs a JSON Schema tool schema from the `BaseModel` schema defined.\"\"\"\n        model_schema = cls.model_json_schema()\n        model_schema.pop(\"title\", None)\n        model_schema.pop(\"description\", None)\n\n        fn = FunctionDefinition(name=cls._name(), description=cls._description())\n        if model_schema[\"properties\"]:\n            fn[\"parameters\"] = model_schema\n\n        return ChatCompletionToolParam(function=fn, type=\"function\")\n\n    @classmethod\n    def from_tool_call(cls, tool_call: ChatCompletionMessageToolCall) -&gt; LiteLLMTool:\n        \"\"\"Constructs an `LiteLLMTool` instance from a `tool_call`.\"\"\"\n        model_json = jiter.from_json(tool_call.function.arguments.encode())\n        model_json[\"tool_call\"] = tool_call.model_dump()\n        return cls.model_validate(model_json)\n</code></pre>"},{"location":"api/core/litellm/tool/#mirascope.core.litellm.tool.LiteLLMTool.from_tool_call","title":"<code>from_tool_call(tool_call)</code>  <code>classmethod</code>","text":"<p>Constructs an <code>LiteLLMTool</code> instance from a <code>tool_call</code>.</p> Source code in <code>mirascope/core/litellm/tool.py</code> <pre><code>@classmethod\ndef from_tool_call(cls, tool_call: ChatCompletionMessageToolCall) -&gt; LiteLLMTool:\n    \"\"\"Constructs an `LiteLLMTool` instance from a `tool_call`.\"\"\"\n    model_json = jiter.from_json(tool_call.function.arguments.encode())\n    model_json[\"tool_call\"] = tool_call.model_dump()\n    return cls.model_validate(model_json)\n</code></pre>"},{"location":"api/core/litellm/tool/#mirascope.core.litellm.tool.LiteLLMTool.tool_schema","title":"<code>tool_schema()</code>  <code>classmethod</code>","text":"<p>Constructs a JSON Schema tool schema from the <code>BaseModel</code> schema defined.</p> Source code in <code>mirascope/core/litellm/tool.py</code> <pre><code>@classmethod\ndef tool_schema(cls) -&gt; ChatCompletionToolParam:\n    \"\"\"Constructs a JSON Schema tool schema from the `BaseModel` schema defined.\"\"\"\n    model_schema = cls.model_json_schema()\n    model_schema.pop(\"title\", None)\n    model_schema.pop(\"description\", None)\n\n    fn = FunctionDefinition(name=cls._name(), description=cls._description())\n    if model_schema[\"properties\"]:\n        fn[\"parameters\"] = model_schema\n\n    return ChatCompletionToolParam(function=fn, type=\"function\")\n</code></pre>"},{"location":"api/core/mistral/","title":"mirascope.core.mistral","text":"<p>The Mirascope Mistral Module.</p>"},{"location":"api/core/mistral/#mirascope.core.mistral.MistralDynamicConfig","title":"<code>MistralDynamicConfig = BaseDynamicConfig[BaseMessageParam, MistralCallParams]</code>  <code>module-attribute</code>","text":"<p>The function return type for functions wrapped with the <code>mistral_call</code> decorator.</p> <p>Attributes:</p> Name Type Description <code>computed_fields</code> <p>The computed fields to use in the prompt template.</p> <code>messages</code> <p>The messages to send to the Mistral API. If provided, <code>computed_fields</code> will be ignored.</p> <code>call_params</code> <p>The call parameters to use when calling the Mistral API. These will override any call parameters provided to the decorator.</p> <p>Example:</p> <pre><code>from mirascope.core.mistral import MistralDynamicConfig, mistral_call\n\n@mistral_call(model=\"gpt-4o\")\ndef recommend_book(genre: str) -&gt; MistralDynamicConfig:\n    \"\"\"Recommend a {capitalized_genre} book.\"\"\"\n    reeturn {\"computed_fields\": {\"capitalized_genre\": genre.capitalize()}}\n</code></pre>"},{"location":"api/core/mistral/#mirascope.core.mistral.mistral_call","title":"<code>mistral_call = call_factory(TCallResponse=MistralCallResponse, TCallResponseChunk=MistralCallResponseChunk, TDynamicConfig=MistralDynamicConfig, TMessageParamType=BaseMessageParam, TToolType=MistralTool, TStream=MistralStream, TCallParams=MistralCallParams, default_call_params=MistralCallParams(), setup_call=setup_call, get_json_output=get_json_output, handle_stream=handle_stream, handle_stream_async=handle_stream_async, calculate_cost=calculate_cost, provider='mistral')</code>  <code>module-attribute</code>","text":"<p>A decorator for calling the Mistral API with a typed function.</p> <p>This decorator is used to wrap a typed function that calls the Mistral API. It parses the docstring of the wrapped function as the messages array and templates the input arguments for the function into each message's template.</p> <p>Example:</p> <pre><code>@mistral_call(model=\"gpt-4o\")\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\nresponse = recommend_book(\"fantasy\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>The Mistral model to use in the API call.</p> required <code>stream</code> <p>Whether to stream the response from the API call.</p> required <code>tools</code> <p>The tools to use in the Mistral API call.</p> required <code>**call_params</code> <p>The <code>MistralCallParams</code> call parameters to use in the API call.</p> required <p>Returns:</p> Type Description <p>The decorator for turning a typed function into an Mistral API call.</p>"},{"location":"api/core/mistral/#mirascope.core.mistral.MistralCallParams","title":"<code>MistralCallParams</code>","text":"<p>               Bases: <code>BaseCallParams</code></p> <p>The parameters to use when calling the Mistral API.</p> Source code in <code>mirascope/core/mistral/call_params.py</code> <pre><code>class MistralCallParams(BaseCallParams):\n    \"\"\"The parameters to use when calling the Mistral API.\"\"\"\n\n    endpoint: NotRequired[str | None]\n    max_tokens: NotRequired[int | None]\n    random_seed: NotRequired[int | None]\n    response_format: NotRequired[ResponseFormat | None]\n    safe_mode: NotRequired[bool | None]\n    safe_prompt: NotRequired[bool | None]\n    temperature: NotRequired[float | None]\n    tool_choice: NotRequired[ToolChoice | None]\n    top_p: NotRequired[float | None]\n</code></pre>"},{"location":"api/core/mistral/#mirascope.core.mistral.MistralCallResponse","title":"<code>MistralCallResponse</code>","text":"<p>               Bases: <code>BaseCallResponse[ChatCompletionResponse, MistralTool, MistralDynamicConfig, BaseMessageParam, MistralCallParams, BaseMessageParam]</code></p> <p>A convenience wrapper around the Mistral <code>ChatCompletion</code> response.</p> <p>When calling the Mistral API using a function decorated with <code>mistral_call</code>, the response will be an <code>MistralCallResponse</code> instance with properties that allow for more convenience access to commonly used attributes.</p> <p>Example:</p> <pre><code>from mirascope.core.mistral import mistral_call\n\n@mistral_call(model=\"gpt-4o\")\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\nresponse = recommend_book(\"fantasy\")  # response is an `MistralCallResponse` instance\nprint(response.content)\n#&gt; Sure! I would recommend...\n</code></pre> Source code in <code>mirascope/core/mistral/call_response.py</code> <pre><code>class MistralCallResponse(\n    BaseCallResponse[\n        ChatCompletionResponse,\n        MistralTool,\n        MistralDynamicConfig,\n        BaseMessageParam,\n        MistralCallParams,\n        BaseMessageParam,\n    ]\n):\n    '''A convenience wrapper around the Mistral `ChatCompletion` response.\n\n    When calling the Mistral API using a function decorated with `mistral_call`, the\n    response will be an `MistralCallResponse` instance with properties that allow for\n    more convenience access to commonly used attributes.\n\n    Example:\n\n    ```python\n    from mirascope.core.mistral import mistral_call\n\n    @mistral_call(model=\"gpt-4o\")\n    def recommend_book(genre: str):\n        \"\"\"Recommend a {genre} book.\"\"\"\n\n    response = recommend_book(\"fantasy\")  # response is an `MistralCallResponse` instance\n    print(response.content)\n    #&gt; Sure! I would recommend...\n    ```\n    '''\n\n    _provider = \"mistral\"\n\n    @computed_field\n    @property\n    def message_param(self) -&gt; BaseMessageParam:\n        \"\"\"Returns the assistants's response as a message parameter.\"\"\"\n        return self.message.model_dump()  # type: ignore\n\n    @property\n    def choices(self) -&gt; list[ChatCompletionResponseChoice]:\n        \"\"\"Returns the array of chat completion choices.\"\"\"\n        return self.response.choices\n\n    @property\n    def choice(self) -&gt; ChatCompletionResponseChoice:\n        \"\"\"Returns the 0th choice.\"\"\"\n        return self.choices[0]\n\n    @property\n    def message(self) -&gt; ChatMessage:\n        \"\"\"Returns the message of the chat completion for the 0th choice.\"\"\"\n        return self.choice.message\n\n    @property\n    def content(self) -&gt; str:\n        \"\"\"The content of the chat completion for the 0th choice.\"\"\"\n        content = self.message.content\n        # We haven't seen the `list[str]` response type in practice, so for now we\n        # return the first item in the list\n        return content if isinstance(content, str) else content[0]\n\n    @property\n    def model(self) -&gt; str:\n        \"\"\"Returns the name of the response model.\"\"\"\n        return self.response.model\n\n    @property\n    def id(self) -&gt; str:\n        \"\"\"Returns the id of the response.\"\"\"\n        return self.response.id\n\n    @property\n    def finish_reasons(self) -&gt; list[str]:\n        \"\"\"Returns the finish reasons of the response.\"\"\"\n        return [\n            choice.finish_reason if choice.finish_reason else \"\"\n            for choice in self.choices\n        ]\n\n    @property\n    def tool_calls(self) -&gt; list[ToolCall] | None:\n        \"\"\"Returns the tool calls for the 0th choice message.\"\"\"\n        return self.message.tool_calls\n\n    @computed_field\n    @property\n    def tools(self) -&gt; list[MistralTool] | None:\n        \"\"\"Returns the tools for the 0th choice message.\n\n        Raises:\n            ValidationError: if the tool call doesn't match the tool's schema.\n        \"\"\"\n        if not self.tool_types or not self.tool_calls:\n            return None\n\n        extracted_tools = []\n        for tool_call in self.tool_calls:\n            for tool_type in self.tool_types:\n                if tool_call.function.name == tool_type._name():\n                    extracted_tools.append(tool_type.from_tool_call(tool_call))\n                    break\n\n        return extracted_tools\n\n    @computed_field\n    @property\n    def tool(self) -&gt; MistralTool | None:\n        \"\"\"Returns the 0th tool for the 0th choice message.\n\n        Raises:\n            ValidationError: if the tool call doesn't match the tool's schema.\n        \"\"\"\n        if tools := self.tools:\n            return tools[0]\n        return None\n\n    @classmethod\n    def tool_message_params(\n        cls, tools_and_outputs: list[tuple[MistralTool, str]]\n    ) -&gt; list[BaseMessageParam]:\n        \"\"\"Returns the tool message parameters for tool call results.\"\"\"\n        return [\n            {\n                \"role\": \"tool\",\n                \"content\": output,\n                \"tool_call_id\": tool.tool_call.id,\n                \"name\": tool._name(),\n            }  # type: ignore\n            for tool, output in tools_and_outputs\n        ]\n\n    @property\n    def usage(self) -&gt; UsageInfo:\n        \"\"\"Returns the usage of the chat completion.\"\"\"\n        return self.response.usage\n\n    @property\n    def input_tokens(self) -&gt; int:\n        \"\"\"Returns the number of input tokens.\"\"\"\n        return self.usage.prompt_tokens\n\n    @property\n    def output_tokens(self) -&gt; int | None:\n        \"\"\"Returns the number of output tokens.\"\"\"\n        return self.usage.completion_tokens\n</code></pre>"},{"location":"api/core/mistral/#mirascope.core.mistral.MistralCallResponse.choice","title":"<code>choice: ChatCompletionResponseChoice</code>  <code>property</code>","text":"<p>Returns the 0th choice.</p>"},{"location":"api/core/mistral/#mirascope.core.mistral.MistralCallResponse.choices","title":"<code>choices: list[ChatCompletionResponseChoice]</code>  <code>property</code>","text":"<p>Returns the array of chat completion choices.</p>"},{"location":"api/core/mistral/#mirascope.core.mistral.MistralCallResponse.content","title":"<code>content: str</code>  <code>property</code>","text":"<p>The content of the chat completion for the 0th choice.</p>"},{"location":"api/core/mistral/#mirascope.core.mistral.MistralCallResponse.finish_reasons","title":"<code>finish_reasons: list[str]</code>  <code>property</code>","text":"<p>Returns the finish reasons of the response.</p>"},{"location":"api/core/mistral/#mirascope.core.mistral.MistralCallResponse.id","title":"<code>id: str</code>  <code>property</code>","text":"<p>Returns the id of the response.</p>"},{"location":"api/core/mistral/#mirascope.core.mistral.MistralCallResponse.input_tokens","title":"<code>input_tokens: int</code>  <code>property</code>","text":"<p>Returns the number of input tokens.</p>"},{"location":"api/core/mistral/#mirascope.core.mistral.MistralCallResponse.message","title":"<code>message: ChatMessage</code>  <code>property</code>","text":"<p>Returns the message of the chat completion for the 0th choice.</p>"},{"location":"api/core/mistral/#mirascope.core.mistral.MistralCallResponse.message_param","title":"<code>message_param: BaseMessageParam</code>  <code>property</code>","text":"<p>Returns the assistants's response as a message parameter.</p>"},{"location":"api/core/mistral/#mirascope.core.mistral.MistralCallResponse.model","title":"<code>model: str</code>  <code>property</code>","text":"<p>Returns the name of the response model.</p>"},{"location":"api/core/mistral/#mirascope.core.mistral.MistralCallResponse.output_tokens","title":"<code>output_tokens: int | None</code>  <code>property</code>","text":"<p>Returns the number of output tokens.</p>"},{"location":"api/core/mistral/#mirascope.core.mistral.MistralCallResponse.tool","title":"<code>tool: MistralTool | None</code>  <code>property</code>","text":"<p>Returns the 0th tool for the 0th choice message.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if the tool call doesn't match the tool's schema.</p>"},{"location":"api/core/mistral/#mirascope.core.mistral.MistralCallResponse.tool_calls","title":"<code>tool_calls: list[ToolCall] | None</code>  <code>property</code>","text":"<p>Returns the tool calls for the 0th choice message.</p>"},{"location":"api/core/mistral/#mirascope.core.mistral.MistralCallResponse.tools","title":"<code>tools: list[MistralTool] | None</code>  <code>property</code>","text":"<p>Returns the tools for the 0th choice message.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if the tool call doesn't match the tool's schema.</p>"},{"location":"api/core/mistral/#mirascope.core.mistral.MistralCallResponse.usage","title":"<code>usage: UsageInfo</code>  <code>property</code>","text":"<p>Returns the usage of the chat completion.</p>"},{"location":"api/core/mistral/#mirascope.core.mistral.MistralCallResponse.tool_message_params","title":"<code>tool_message_params(tools_and_outputs)</code>  <code>classmethod</code>","text":"<p>Returns the tool message parameters for tool call results.</p> Source code in <code>mirascope/core/mistral/call_response.py</code> <pre><code>@classmethod\ndef tool_message_params(\n    cls, tools_and_outputs: list[tuple[MistralTool, str]]\n) -&gt; list[BaseMessageParam]:\n    \"\"\"Returns the tool message parameters for tool call results.\"\"\"\n    return [\n        {\n            \"role\": \"tool\",\n            \"content\": output,\n            \"tool_call_id\": tool.tool_call.id,\n            \"name\": tool._name(),\n        }  # type: ignore\n        for tool, output in tools_and_outputs\n    ]\n</code></pre>"},{"location":"api/core/mistral/#mirascope.core.mistral.MistralCallResponseChunk","title":"<code>MistralCallResponseChunk</code>","text":"<p>               Bases: <code>BaseCallResponseChunk[ChatCompletionStreamResponse]</code></p> <p>A convenience wrapper around the Mistral <code>ChatCompletionChunk</code> streamed chunks.</p> <p>When calling the Mistral API using a function decorated with <code>mistral_call</code> and <code>stream</code> set to <code>True</code>, the stream will contain <code>MistralResponseChunk</code> instances with properties that allow for more convenient access to commonly used attributes.</p> <p>Example:</p> <pre><code>from mirascope.core.mistral import mistral_call\n\n@mistral_call(model=\"gpt-4o\", stream=True)\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\nstream = recommend_book(\"fantasy\")  # response is an `MistralStream`\nfor chunk in stream:\n    print(chunk.content, end=\"\", flush=True)\n#&gt; Sure! I would recommend...\n</code></pre> Source code in <code>mirascope/core/mistral/call_response_chunk.py</code> <pre><code>class MistralCallResponseChunk(BaseCallResponseChunk[ChatCompletionStreamResponse]):\n    '''A convenience wrapper around the Mistral `ChatCompletionChunk` streamed chunks.\n\n    When calling the Mistral API using a function decorated with `mistral_call` and\n    `stream` set to `True`, the stream will contain `MistralResponseChunk` instances with\n    properties that allow for more convenient access to commonly used attributes.\n\n    Example:\n\n    ```python\n    from mirascope.core.mistral import mistral_call\n\n    @mistral_call(model=\"gpt-4o\", stream=True)\n    def recommend_book(genre: str):\n        \"\"\"Recommend a {genre} book.\"\"\"\n\n    stream = recommend_book(\"fantasy\")  # response is an `MistralStream`\n    for chunk in stream:\n        print(chunk.content, end=\"\", flush=True)\n    #&gt; Sure! I would recommend...\n    ```\n    '''\n\n    @property\n    def choices(self) -&gt; list[ChatCompletionResponseStreamChoice]:\n        \"\"\"Returns the array of chat completion choices.\"\"\"\n        return self.chunk.choices\n\n    @property\n    def choice(self) -&gt; ChatCompletionResponseStreamChoice:\n        \"\"\"Returns the 0th choice.\"\"\"\n        return self.choices[0]\n\n    @property\n    def delta(self) -&gt; DeltaMessage:\n        \"\"\"Returns the delta of the 0th choice.\"\"\"\n        return self.choice.delta\n\n    @property\n    def content(self) -&gt; str:\n        \"\"\"Returns the content of the delta.\"\"\"\n        return self.delta.content if self.delta.content is not None else \"\"\n\n    @property\n    def model(self) -&gt; str:\n        \"\"\"Returns the name of the response model.\"\"\"\n        return self.chunk.model\n\n    @property\n    def id(self) -&gt; str:\n        \"\"\"Returns the id of the response.\"\"\"\n        return self.chunk.id\n\n    @property\n    def finish_reasons(self) -&gt; list[str]:\n        \"\"\"Returns the finish reasons of the response.\"\"\"\n        return [\n            choice.finish_reason if choice.finish_reason else \"\"\n            for choice in self.choices\n        ]\n\n    @property\n    def tool_calls(self) -&gt; list[ToolCall] | None:\n        \"\"\"Returns the partial tool calls for the 0th choice message.\"\"\"\n        return self.delta.tool_calls\n\n    @property\n    def usage(self) -&gt; UsageInfo | None:\n        \"\"\"Returns the usage of the chat completion.\"\"\"\n        return self.chunk.usage\n\n    @property\n    def input_tokens(self) -&gt; int | None:\n        \"\"\"Returns the number of input tokens.\"\"\"\n        if self.usage:\n            return self.usage.prompt_tokens\n        return None\n\n    @property\n    def output_tokens(self) -&gt; int | None:\n        \"\"\"Returns the number of output tokens.\"\"\"\n        if self.usage:\n            return self.usage.completion_tokens\n        return None\n</code></pre>"},{"location":"api/core/mistral/#mirascope.core.mistral.MistralCallResponseChunk.choice","title":"<code>choice: ChatCompletionResponseStreamChoice</code>  <code>property</code>","text":"<p>Returns the 0th choice.</p>"},{"location":"api/core/mistral/#mirascope.core.mistral.MistralCallResponseChunk.choices","title":"<code>choices: list[ChatCompletionResponseStreamChoice]</code>  <code>property</code>","text":"<p>Returns the array of chat completion choices.</p>"},{"location":"api/core/mistral/#mirascope.core.mistral.MistralCallResponseChunk.content","title":"<code>content: str</code>  <code>property</code>","text":"<p>Returns the content of the delta.</p>"},{"location":"api/core/mistral/#mirascope.core.mistral.MistralCallResponseChunk.delta","title":"<code>delta: DeltaMessage</code>  <code>property</code>","text":"<p>Returns the delta of the 0th choice.</p>"},{"location":"api/core/mistral/#mirascope.core.mistral.MistralCallResponseChunk.finish_reasons","title":"<code>finish_reasons: list[str]</code>  <code>property</code>","text":"<p>Returns the finish reasons of the response.</p>"},{"location":"api/core/mistral/#mirascope.core.mistral.MistralCallResponseChunk.id","title":"<code>id: str</code>  <code>property</code>","text":"<p>Returns the id of the response.</p>"},{"location":"api/core/mistral/#mirascope.core.mistral.MistralCallResponseChunk.input_tokens","title":"<code>input_tokens: int | None</code>  <code>property</code>","text":"<p>Returns the number of input tokens.</p>"},{"location":"api/core/mistral/#mirascope.core.mistral.MistralCallResponseChunk.model","title":"<code>model: str</code>  <code>property</code>","text":"<p>Returns the name of the response model.</p>"},{"location":"api/core/mistral/#mirascope.core.mistral.MistralCallResponseChunk.output_tokens","title":"<code>output_tokens: int | None</code>  <code>property</code>","text":"<p>Returns the number of output tokens.</p>"},{"location":"api/core/mistral/#mirascope.core.mistral.MistralCallResponseChunk.tool_calls","title":"<code>tool_calls: list[ToolCall] | None</code>  <code>property</code>","text":"<p>Returns the partial tool calls for the 0th choice message.</p>"},{"location":"api/core/mistral/#mirascope.core.mistral.MistralCallResponseChunk.usage","title":"<code>usage: UsageInfo | None</code>  <code>property</code>","text":"<p>Returns the usage of the chat completion.</p>"},{"location":"api/core/mistral/#mirascope.core.mistral.MistralTool","title":"<code>MistralTool</code>","text":"<p>               Bases: <code>BaseTool</code></p> <p>A class for defining tools for Mistral LLM calls.</p> Source code in <code>mirascope/core/mistral/tool.py</code> <pre><code>class MistralTool(BaseTool):\n    \"\"\"A class for defining tools for Mistral LLM calls.\"\"\"\n\n    tool_call: ToolCall\n\n    @classmethod\n    def tool_schema(cls) -&gt; dict[str, Any]:\n        \"\"\"Constructs a JSON Schema tool schema from the `BaseModel` schema defined.\"\"\"\n        model_schema = cls.model_json_schema()\n        model_schema.pop(\"title\", None)\n        model_schema.pop(\"description\", None)\n\n        fn: dict[str, Any] = {\"name\": cls._name(), \"description\": cls._description()}\n        if model_schema[\"properties\"]:\n            fn[\"parameters\"] = model_schema\n\n        return {\"function\": fn, \"type\": \"function\"}\n\n    @classmethod\n    def from_tool_call(cls, tool_call: ToolCall) -&gt; MistralTool:\n        \"\"\"Constructs an `MistralTool` instance from a `tool_call`.\"\"\"\n        model_json = jiter.from_json(tool_call.function.arguments.encode())\n        model_json[\"tool_call\"] = tool_call.model_dump()\n        return cls.model_validate(model_json)\n</code></pre>"},{"location":"api/core/mistral/#mirascope.core.mistral.MistralTool.from_tool_call","title":"<code>from_tool_call(tool_call)</code>  <code>classmethod</code>","text":"<p>Constructs an <code>MistralTool</code> instance from a <code>tool_call</code>.</p> Source code in <code>mirascope/core/mistral/tool.py</code> <pre><code>@classmethod\ndef from_tool_call(cls, tool_call: ToolCall) -&gt; MistralTool:\n    \"\"\"Constructs an `MistralTool` instance from a `tool_call`.\"\"\"\n    model_json = jiter.from_json(tool_call.function.arguments.encode())\n    model_json[\"tool_call\"] = tool_call.model_dump()\n    return cls.model_validate(model_json)\n</code></pre>"},{"location":"api/core/mistral/#mirascope.core.mistral.MistralTool.tool_schema","title":"<code>tool_schema()</code>  <code>classmethod</code>","text":"<p>Constructs a JSON Schema tool schema from the <code>BaseModel</code> schema defined.</p> Source code in <code>mirascope/core/mistral/tool.py</code> <pre><code>@classmethod\ndef tool_schema(cls) -&gt; dict[str, Any]:\n    \"\"\"Constructs a JSON Schema tool schema from the `BaseModel` schema defined.\"\"\"\n    model_schema = cls.model_json_schema()\n    model_schema.pop(\"title\", None)\n    model_schema.pop(\"description\", None)\n\n    fn: dict[str, Any] = {\"name\": cls._name(), \"description\": cls._description()}\n    if model_schema[\"properties\"]:\n        fn[\"parameters\"] = model_schema\n\n    return {\"function\": fn, \"type\": \"function\"}\n</code></pre>"},{"location":"api/core/mistral/call/","title":"mirascope.core.mistral.call","text":"<p>The <code>mistral_call</code> decorator for functions as LLM calls.</p>"},{"location":"api/core/mistral/call/#mirascope.core.mistral.call.mistral_call","title":"<code>mistral_call = call_factory(TCallResponse=MistralCallResponse, TCallResponseChunk=MistralCallResponseChunk, TDynamicConfig=MistralDynamicConfig, TMessageParamType=BaseMessageParam, TToolType=MistralTool, TStream=MistralStream, TCallParams=MistralCallParams, default_call_params=MistralCallParams(), setup_call=setup_call, get_json_output=get_json_output, handle_stream=handle_stream, handle_stream_async=handle_stream_async, calculate_cost=calculate_cost, provider='mistral')</code>  <code>module-attribute</code>","text":"<p>A decorator for calling the Mistral API with a typed function.</p> <p>This decorator is used to wrap a typed function that calls the Mistral API. It parses the docstring of the wrapped function as the messages array and templates the input arguments for the function into each message's template.</p> <p>Example:</p> <pre><code>@mistral_call(model=\"gpt-4o\")\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\nresponse = recommend_book(\"fantasy\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>The Mistral model to use in the API call.</p> required <code>stream</code> <p>Whether to stream the response from the API call.</p> required <code>tools</code> <p>The tools to use in the Mistral API call.</p> required <code>**call_params</code> <p>The <code>MistralCallParams</code> call parameters to use in the API call.</p> required <p>Returns:</p> Type Description <p>The decorator for turning a typed function into an Mistral API call.</p>"},{"location":"api/core/mistral/call_params/","title":"mirascope.core.mistral.call_params","text":"<p>The parameters to use when calling the Mistral API.</p>"},{"location":"api/core/mistral/call_params/#mirascope.core.mistral.call_params.MistralCallParams","title":"<code>MistralCallParams</code>","text":"<p>               Bases: <code>BaseCallParams</code></p> <p>The parameters to use when calling the Mistral API.</p> Source code in <code>mirascope/core/mistral/call_params.py</code> <pre><code>class MistralCallParams(BaseCallParams):\n    \"\"\"The parameters to use when calling the Mistral API.\"\"\"\n\n    endpoint: NotRequired[str | None]\n    max_tokens: NotRequired[int | None]\n    random_seed: NotRequired[int | None]\n    response_format: NotRequired[ResponseFormat | None]\n    safe_mode: NotRequired[bool | None]\n    safe_prompt: NotRequired[bool | None]\n    temperature: NotRequired[float | None]\n    tool_choice: NotRequired[ToolChoice | None]\n    top_p: NotRequired[float | None]\n</code></pre>"},{"location":"api/core/mistral/call_response/","title":"mirascope.core.mistral.call_response","text":"<p>This module contains the <code>MistralCallResponse</code> class.</p>"},{"location":"api/core/mistral/call_response/#mirascope.core.mistral.call_response.MistralCallResponse","title":"<code>MistralCallResponse</code>","text":"<p>               Bases: <code>BaseCallResponse[ChatCompletionResponse, MistralTool, MistralDynamicConfig, BaseMessageParam, MistralCallParams, BaseMessageParam]</code></p> <p>A convenience wrapper around the Mistral <code>ChatCompletion</code> response.</p> <p>When calling the Mistral API using a function decorated with <code>mistral_call</code>, the response will be an <code>MistralCallResponse</code> instance with properties that allow for more convenience access to commonly used attributes.</p> <p>Example:</p> <pre><code>from mirascope.core.mistral import mistral_call\n\n@mistral_call(model=\"gpt-4o\")\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\nresponse = recommend_book(\"fantasy\")  # response is an `MistralCallResponse` instance\nprint(response.content)\n#&gt; Sure! I would recommend...\n</code></pre> Source code in <code>mirascope/core/mistral/call_response.py</code> <pre><code>class MistralCallResponse(\n    BaseCallResponse[\n        ChatCompletionResponse,\n        MistralTool,\n        MistralDynamicConfig,\n        BaseMessageParam,\n        MistralCallParams,\n        BaseMessageParam,\n    ]\n):\n    '''A convenience wrapper around the Mistral `ChatCompletion` response.\n\n    When calling the Mistral API using a function decorated with `mistral_call`, the\n    response will be an `MistralCallResponse` instance with properties that allow for\n    more convenience access to commonly used attributes.\n\n    Example:\n\n    ```python\n    from mirascope.core.mistral import mistral_call\n\n    @mistral_call(model=\"gpt-4o\")\n    def recommend_book(genre: str):\n        \"\"\"Recommend a {genre} book.\"\"\"\n\n    response = recommend_book(\"fantasy\")  # response is an `MistralCallResponse` instance\n    print(response.content)\n    #&gt; Sure! I would recommend...\n    ```\n    '''\n\n    _provider = \"mistral\"\n\n    @computed_field\n    @property\n    def message_param(self) -&gt; BaseMessageParam:\n        \"\"\"Returns the assistants's response as a message parameter.\"\"\"\n        return self.message.model_dump()  # type: ignore\n\n    @property\n    def choices(self) -&gt; list[ChatCompletionResponseChoice]:\n        \"\"\"Returns the array of chat completion choices.\"\"\"\n        return self.response.choices\n\n    @property\n    def choice(self) -&gt; ChatCompletionResponseChoice:\n        \"\"\"Returns the 0th choice.\"\"\"\n        return self.choices[0]\n\n    @property\n    def message(self) -&gt; ChatMessage:\n        \"\"\"Returns the message of the chat completion for the 0th choice.\"\"\"\n        return self.choice.message\n\n    @property\n    def content(self) -&gt; str:\n        \"\"\"The content of the chat completion for the 0th choice.\"\"\"\n        content = self.message.content\n        # We haven't seen the `list[str]` response type in practice, so for now we\n        # return the first item in the list\n        return content if isinstance(content, str) else content[0]\n\n    @property\n    def model(self) -&gt; str:\n        \"\"\"Returns the name of the response model.\"\"\"\n        return self.response.model\n\n    @property\n    def id(self) -&gt; str:\n        \"\"\"Returns the id of the response.\"\"\"\n        return self.response.id\n\n    @property\n    def finish_reasons(self) -&gt; list[str]:\n        \"\"\"Returns the finish reasons of the response.\"\"\"\n        return [\n            choice.finish_reason if choice.finish_reason else \"\"\n            for choice in self.choices\n        ]\n\n    @property\n    def tool_calls(self) -&gt; list[ToolCall] | None:\n        \"\"\"Returns the tool calls for the 0th choice message.\"\"\"\n        return self.message.tool_calls\n\n    @computed_field\n    @property\n    def tools(self) -&gt; list[MistralTool] | None:\n        \"\"\"Returns the tools for the 0th choice message.\n\n        Raises:\n            ValidationError: if the tool call doesn't match the tool's schema.\n        \"\"\"\n        if not self.tool_types or not self.tool_calls:\n            return None\n\n        extracted_tools = []\n        for tool_call in self.tool_calls:\n            for tool_type in self.tool_types:\n                if tool_call.function.name == tool_type._name():\n                    extracted_tools.append(tool_type.from_tool_call(tool_call))\n                    break\n\n        return extracted_tools\n\n    @computed_field\n    @property\n    def tool(self) -&gt; MistralTool | None:\n        \"\"\"Returns the 0th tool for the 0th choice message.\n\n        Raises:\n            ValidationError: if the tool call doesn't match the tool's schema.\n        \"\"\"\n        if tools := self.tools:\n            return tools[0]\n        return None\n\n    @classmethod\n    def tool_message_params(\n        cls, tools_and_outputs: list[tuple[MistralTool, str]]\n    ) -&gt; list[BaseMessageParam]:\n        \"\"\"Returns the tool message parameters for tool call results.\"\"\"\n        return [\n            {\n                \"role\": \"tool\",\n                \"content\": output,\n                \"tool_call_id\": tool.tool_call.id,\n                \"name\": tool._name(),\n            }  # type: ignore\n            for tool, output in tools_and_outputs\n        ]\n\n    @property\n    def usage(self) -&gt; UsageInfo:\n        \"\"\"Returns the usage of the chat completion.\"\"\"\n        return self.response.usage\n\n    @property\n    def input_tokens(self) -&gt; int:\n        \"\"\"Returns the number of input tokens.\"\"\"\n        return self.usage.prompt_tokens\n\n    @property\n    def output_tokens(self) -&gt; int | None:\n        \"\"\"Returns the number of output tokens.\"\"\"\n        return self.usage.completion_tokens\n</code></pre>"},{"location":"api/core/mistral/call_response/#mirascope.core.mistral.call_response.MistralCallResponse.choice","title":"<code>choice: ChatCompletionResponseChoice</code>  <code>property</code>","text":"<p>Returns the 0th choice.</p>"},{"location":"api/core/mistral/call_response/#mirascope.core.mistral.call_response.MistralCallResponse.choices","title":"<code>choices: list[ChatCompletionResponseChoice]</code>  <code>property</code>","text":"<p>Returns the array of chat completion choices.</p>"},{"location":"api/core/mistral/call_response/#mirascope.core.mistral.call_response.MistralCallResponse.content","title":"<code>content: str</code>  <code>property</code>","text":"<p>The content of the chat completion for the 0th choice.</p>"},{"location":"api/core/mistral/call_response/#mirascope.core.mistral.call_response.MistralCallResponse.finish_reasons","title":"<code>finish_reasons: list[str]</code>  <code>property</code>","text":"<p>Returns the finish reasons of the response.</p>"},{"location":"api/core/mistral/call_response/#mirascope.core.mistral.call_response.MistralCallResponse.id","title":"<code>id: str</code>  <code>property</code>","text":"<p>Returns the id of the response.</p>"},{"location":"api/core/mistral/call_response/#mirascope.core.mistral.call_response.MistralCallResponse.input_tokens","title":"<code>input_tokens: int</code>  <code>property</code>","text":"<p>Returns the number of input tokens.</p>"},{"location":"api/core/mistral/call_response/#mirascope.core.mistral.call_response.MistralCallResponse.message","title":"<code>message: ChatMessage</code>  <code>property</code>","text":"<p>Returns the message of the chat completion for the 0th choice.</p>"},{"location":"api/core/mistral/call_response/#mirascope.core.mistral.call_response.MistralCallResponse.message_param","title":"<code>message_param: BaseMessageParam</code>  <code>property</code>","text":"<p>Returns the assistants's response as a message parameter.</p>"},{"location":"api/core/mistral/call_response/#mirascope.core.mistral.call_response.MistralCallResponse.model","title":"<code>model: str</code>  <code>property</code>","text":"<p>Returns the name of the response model.</p>"},{"location":"api/core/mistral/call_response/#mirascope.core.mistral.call_response.MistralCallResponse.output_tokens","title":"<code>output_tokens: int | None</code>  <code>property</code>","text":"<p>Returns the number of output tokens.</p>"},{"location":"api/core/mistral/call_response/#mirascope.core.mistral.call_response.MistralCallResponse.tool","title":"<code>tool: MistralTool | None</code>  <code>property</code>","text":"<p>Returns the 0th tool for the 0th choice message.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if the tool call doesn't match the tool's schema.</p>"},{"location":"api/core/mistral/call_response/#mirascope.core.mistral.call_response.MistralCallResponse.tool_calls","title":"<code>tool_calls: list[ToolCall] | None</code>  <code>property</code>","text":"<p>Returns the tool calls for the 0th choice message.</p>"},{"location":"api/core/mistral/call_response/#mirascope.core.mistral.call_response.MistralCallResponse.tools","title":"<code>tools: list[MistralTool] | None</code>  <code>property</code>","text":"<p>Returns the tools for the 0th choice message.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if the tool call doesn't match the tool's schema.</p>"},{"location":"api/core/mistral/call_response/#mirascope.core.mistral.call_response.MistralCallResponse.usage","title":"<code>usage: UsageInfo</code>  <code>property</code>","text":"<p>Returns the usage of the chat completion.</p>"},{"location":"api/core/mistral/call_response/#mirascope.core.mistral.call_response.MistralCallResponse.tool_message_params","title":"<code>tool_message_params(tools_and_outputs)</code>  <code>classmethod</code>","text":"<p>Returns the tool message parameters for tool call results.</p> Source code in <code>mirascope/core/mistral/call_response.py</code> <pre><code>@classmethod\ndef tool_message_params(\n    cls, tools_and_outputs: list[tuple[MistralTool, str]]\n) -&gt; list[BaseMessageParam]:\n    \"\"\"Returns the tool message parameters for tool call results.\"\"\"\n    return [\n        {\n            \"role\": \"tool\",\n            \"content\": output,\n            \"tool_call_id\": tool.tool_call.id,\n            \"name\": tool._name(),\n        }  # type: ignore\n        for tool, output in tools_and_outputs\n    ]\n</code></pre>"},{"location":"api/core/mistral/call_response_chunk/","title":"mirascope.core.mistral.call_response_chunk","text":"<p>This module contains the <code>MistralCallResponseChunk</code> class.</p>"},{"location":"api/core/mistral/call_response_chunk/#mirascope.core.mistral.call_response_chunk.MistralCallResponseChunk","title":"<code>MistralCallResponseChunk</code>","text":"<p>               Bases: <code>BaseCallResponseChunk[ChatCompletionStreamResponse]</code></p> <p>A convenience wrapper around the Mistral <code>ChatCompletionChunk</code> streamed chunks.</p> <p>When calling the Mistral API using a function decorated with <code>mistral_call</code> and <code>stream</code> set to <code>True</code>, the stream will contain <code>MistralResponseChunk</code> instances with properties that allow for more convenient access to commonly used attributes.</p> <p>Example:</p> <pre><code>from mirascope.core.mistral import mistral_call\n\n@mistral_call(model=\"gpt-4o\", stream=True)\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\nstream = recommend_book(\"fantasy\")  # response is an `MistralStream`\nfor chunk in stream:\n    print(chunk.content, end=\"\", flush=True)\n#&gt; Sure! I would recommend...\n</code></pre> Source code in <code>mirascope/core/mistral/call_response_chunk.py</code> <pre><code>class MistralCallResponseChunk(BaseCallResponseChunk[ChatCompletionStreamResponse]):\n    '''A convenience wrapper around the Mistral `ChatCompletionChunk` streamed chunks.\n\n    When calling the Mistral API using a function decorated with `mistral_call` and\n    `stream` set to `True`, the stream will contain `MistralResponseChunk` instances with\n    properties that allow for more convenient access to commonly used attributes.\n\n    Example:\n\n    ```python\n    from mirascope.core.mistral import mistral_call\n\n    @mistral_call(model=\"gpt-4o\", stream=True)\n    def recommend_book(genre: str):\n        \"\"\"Recommend a {genre} book.\"\"\"\n\n    stream = recommend_book(\"fantasy\")  # response is an `MistralStream`\n    for chunk in stream:\n        print(chunk.content, end=\"\", flush=True)\n    #&gt; Sure! I would recommend...\n    ```\n    '''\n\n    @property\n    def choices(self) -&gt; list[ChatCompletionResponseStreamChoice]:\n        \"\"\"Returns the array of chat completion choices.\"\"\"\n        return self.chunk.choices\n\n    @property\n    def choice(self) -&gt; ChatCompletionResponseStreamChoice:\n        \"\"\"Returns the 0th choice.\"\"\"\n        return self.choices[0]\n\n    @property\n    def delta(self) -&gt; DeltaMessage:\n        \"\"\"Returns the delta of the 0th choice.\"\"\"\n        return self.choice.delta\n\n    @property\n    def content(self) -&gt; str:\n        \"\"\"Returns the content of the delta.\"\"\"\n        return self.delta.content if self.delta.content is not None else \"\"\n\n    @property\n    def model(self) -&gt; str:\n        \"\"\"Returns the name of the response model.\"\"\"\n        return self.chunk.model\n\n    @property\n    def id(self) -&gt; str:\n        \"\"\"Returns the id of the response.\"\"\"\n        return self.chunk.id\n\n    @property\n    def finish_reasons(self) -&gt; list[str]:\n        \"\"\"Returns the finish reasons of the response.\"\"\"\n        return [\n            choice.finish_reason if choice.finish_reason else \"\"\n            for choice in self.choices\n        ]\n\n    @property\n    def tool_calls(self) -&gt; list[ToolCall] | None:\n        \"\"\"Returns the partial tool calls for the 0th choice message.\"\"\"\n        return self.delta.tool_calls\n\n    @property\n    def usage(self) -&gt; UsageInfo | None:\n        \"\"\"Returns the usage of the chat completion.\"\"\"\n        return self.chunk.usage\n\n    @property\n    def input_tokens(self) -&gt; int | None:\n        \"\"\"Returns the number of input tokens.\"\"\"\n        if self.usage:\n            return self.usage.prompt_tokens\n        return None\n\n    @property\n    def output_tokens(self) -&gt; int | None:\n        \"\"\"Returns the number of output tokens.\"\"\"\n        if self.usage:\n            return self.usage.completion_tokens\n        return None\n</code></pre>"},{"location":"api/core/mistral/call_response_chunk/#mirascope.core.mistral.call_response_chunk.MistralCallResponseChunk.choice","title":"<code>choice: ChatCompletionResponseStreamChoice</code>  <code>property</code>","text":"<p>Returns the 0th choice.</p>"},{"location":"api/core/mistral/call_response_chunk/#mirascope.core.mistral.call_response_chunk.MistralCallResponseChunk.choices","title":"<code>choices: list[ChatCompletionResponseStreamChoice]</code>  <code>property</code>","text":"<p>Returns the array of chat completion choices.</p>"},{"location":"api/core/mistral/call_response_chunk/#mirascope.core.mistral.call_response_chunk.MistralCallResponseChunk.content","title":"<code>content: str</code>  <code>property</code>","text":"<p>Returns the content of the delta.</p>"},{"location":"api/core/mistral/call_response_chunk/#mirascope.core.mistral.call_response_chunk.MistralCallResponseChunk.delta","title":"<code>delta: DeltaMessage</code>  <code>property</code>","text":"<p>Returns the delta of the 0th choice.</p>"},{"location":"api/core/mistral/call_response_chunk/#mirascope.core.mistral.call_response_chunk.MistralCallResponseChunk.finish_reasons","title":"<code>finish_reasons: list[str]</code>  <code>property</code>","text":"<p>Returns the finish reasons of the response.</p>"},{"location":"api/core/mistral/call_response_chunk/#mirascope.core.mistral.call_response_chunk.MistralCallResponseChunk.id","title":"<code>id: str</code>  <code>property</code>","text":"<p>Returns the id of the response.</p>"},{"location":"api/core/mistral/call_response_chunk/#mirascope.core.mistral.call_response_chunk.MistralCallResponseChunk.input_tokens","title":"<code>input_tokens: int | None</code>  <code>property</code>","text":"<p>Returns the number of input tokens.</p>"},{"location":"api/core/mistral/call_response_chunk/#mirascope.core.mistral.call_response_chunk.MistralCallResponseChunk.model","title":"<code>model: str</code>  <code>property</code>","text":"<p>Returns the name of the response model.</p>"},{"location":"api/core/mistral/call_response_chunk/#mirascope.core.mistral.call_response_chunk.MistralCallResponseChunk.output_tokens","title":"<code>output_tokens: int | None</code>  <code>property</code>","text":"<p>Returns the number of output tokens.</p>"},{"location":"api/core/mistral/call_response_chunk/#mirascope.core.mistral.call_response_chunk.MistralCallResponseChunk.tool_calls","title":"<code>tool_calls: list[ToolCall] | None</code>  <code>property</code>","text":"<p>Returns the partial tool calls for the 0th choice message.</p>"},{"location":"api/core/mistral/call_response_chunk/#mirascope.core.mistral.call_response_chunk.MistralCallResponseChunk.usage","title":"<code>usage: UsageInfo | None</code>  <code>property</code>","text":"<p>Returns the usage of the chat completion.</p>"},{"location":"api/core/mistral/dynamic_config/","title":"mirascope.core.mistral.dynamic_config","text":"<p>This module defines the function return type for functions as LLM calls.</p>"},{"location":"api/core/mistral/dynamic_config/#mirascope.core.mistral.dynamic_config.MistralDynamicConfig","title":"<code>MistralDynamicConfig = BaseDynamicConfig[BaseMessageParam, MistralCallParams]</code>  <code>module-attribute</code>","text":"<p>The function return type for functions wrapped with the <code>mistral_call</code> decorator.</p> <p>Attributes:</p> Name Type Description <code>computed_fields</code> <p>The computed fields to use in the prompt template.</p> <code>messages</code> <p>The messages to send to the Mistral API. If provided, <code>computed_fields</code> will be ignored.</p> <code>call_params</code> <p>The call parameters to use when calling the Mistral API. These will override any call parameters provided to the decorator.</p> <p>Example:</p> <pre><code>from mirascope.core.mistral import MistralDynamicConfig, mistral_call\n\n@mistral_call(model=\"gpt-4o\")\ndef recommend_book(genre: str) -&gt; MistralDynamicConfig:\n    \"\"\"Recommend a {capitalized_genre} book.\"\"\"\n    reeturn {\"computed_fields\": {\"capitalized_genre\": genre.capitalize()}}\n</code></pre>"},{"location":"api/core/mistral/tool/","title":"mirascope.core.mistral.tool","text":"<p>The <code>MistralTool</code> class for easy tool usage with Mistral LLM calls.</p>"},{"location":"api/core/mistral/tool/#mirascope.core.mistral.tool.MistralTool","title":"<code>MistralTool</code>","text":"<p>               Bases: <code>BaseTool</code></p> <p>A class for defining tools for Mistral LLM calls.</p> Source code in <code>mirascope/core/mistral/tool.py</code> <pre><code>class MistralTool(BaseTool):\n    \"\"\"A class for defining tools for Mistral LLM calls.\"\"\"\n\n    tool_call: ToolCall\n\n    @classmethod\n    def tool_schema(cls) -&gt; dict[str, Any]:\n        \"\"\"Constructs a JSON Schema tool schema from the `BaseModel` schema defined.\"\"\"\n        model_schema = cls.model_json_schema()\n        model_schema.pop(\"title\", None)\n        model_schema.pop(\"description\", None)\n\n        fn: dict[str, Any] = {\"name\": cls._name(), \"description\": cls._description()}\n        if model_schema[\"properties\"]:\n            fn[\"parameters\"] = model_schema\n\n        return {\"function\": fn, \"type\": \"function\"}\n\n    @classmethod\n    def from_tool_call(cls, tool_call: ToolCall) -&gt; MistralTool:\n        \"\"\"Constructs an `MistralTool` instance from a `tool_call`.\"\"\"\n        model_json = jiter.from_json(tool_call.function.arguments.encode())\n        model_json[\"tool_call\"] = tool_call.model_dump()\n        return cls.model_validate(model_json)\n</code></pre>"},{"location":"api/core/mistral/tool/#mirascope.core.mistral.tool.MistralTool.from_tool_call","title":"<code>from_tool_call(tool_call)</code>  <code>classmethod</code>","text":"<p>Constructs an <code>MistralTool</code> instance from a <code>tool_call</code>.</p> Source code in <code>mirascope/core/mistral/tool.py</code> <pre><code>@classmethod\ndef from_tool_call(cls, tool_call: ToolCall) -&gt; MistralTool:\n    \"\"\"Constructs an `MistralTool` instance from a `tool_call`.\"\"\"\n    model_json = jiter.from_json(tool_call.function.arguments.encode())\n    model_json[\"tool_call\"] = tool_call.model_dump()\n    return cls.model_validate(model_json)\n</code></pre>"},{"location":"api/core/mistral/tool/#mirascope.core.mistral.tool.MistralTool.tool_schema","title":"<code>tool_schema()</code>  <code>classmethod</code>","text":"<p>Constructs a JSON Schema tool schema from the <code>BaseModel</code> schema defined.</p> Source code in <code>mirascope/core/mistral/tool.py</code> <pre><code>@classmethod\ndef tool_schema(cls) -&gt; dict[str, Any]:\n    \"\"\"Constructs a JSON Schema tool schema from the `BaseModel` schema defined.\"\"\"\n    model_schema = cls.model_json_schema()\n    model_schema.pop(\"title\", None)\n    model_schema.pop(\"description\", None)\n\n    fn: dict[str, Any] = {\"name\": cls._name(), \"description\": cls._description()}\n    if model_schema[\"properties\"]:\n        fn[\"parameters\"] = model_schema\n\n    return {\"function\": fn, \"type\": \"function\"}\n</code></pre>"},{"location":"api/core/openai/","title":"mirascope.core.openai","text":"<p>The Mirascope OpenAI Module.</p>"},{"location":"api/core/openai/#mirascope.core.openai.OpenAIDynamicConfig","title":"<code>OpenAIDynamicConfig = BaseDynamicConfig[ChatCompletionMessageParam, OpenAICallParams]</code>  <code>module-attribute</code>","text":"<p>The function return type for functions wrapped with the <code>openai_call</code> decorator.</p> <p>Attributes:</p> Name Type Description <code>computed_fields</code> <p>The computed fields to use in the prompt template.</p> <code>messages</code> <p>The messages to send to the OpenAI API. If provided, <code>computed_fields</code> will be ignored.</p> <code>call_params</code> <p>The call parameters to use when calling the OpenAI API. These will override any call parameters provided to the decorator.</p> <p>Example:</p> <pre><code>from mirascope.core.openai import OpenAIDynamicConfig, openai_call\n\n@openai_call(model=\"gpt-4o\")\ndef recommend_book(genre: str) -&gt; OpenAIDynamicConfig:\n    \"\"\"Recommend a {capitalized_genre} book.\"\"\"\n    reeturn {\"computed_fields\": {\"capitalized_genre\": genre.capitalize()}}\n</code></pre>"},{"location":"api/core/openai/#mirascope.core.openai.openai_call","title":"<code>openai_call = call_factory(TCallResponse=OpenAICallResponse, TCallResponseChunk=OpenAICallResponseChunk, TDynamicConfig=OpenAIDynamicConfig, TMessageParamType=ChatCompletionAssistantMessageParam, TToolType=OpenAITool, TStream=OpenAIStream, TCallParams=OpenAICallParams, default_call_params=OpenAICallParams(), setup_call=setup_call, get_json_output=get_json_output, handle_stream=handle_stream, handle_stream_async=handle_stream_async, calculate_cost=calculate_cost, provider='openai')</code>  <code>module-attribute</code>","text":"<p>A decorator for calling the OpenAI API with a typed function.</p> <p>This decorator is used to wrap a typed function that calls the OpenAI API. It parses the docstring of the wrapped function as the messages array and templates the input arguments for the function into each message's template.</p> <p>Example:</p> <pre><code>@openai_call(model=\"gpt-4o\")\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\nresponse = recommend_book(\"fantasy\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>The OpenAI model to use in the API call.</p> required <code>stream</code> <p>Whether to stream the response from the API call.</p> required <code>tools</code> <p>The tools to use in the OpenAI API call.</p> required <code>**call_params</code> <p>The <code>OpenAICallParams</code> call parameters to use in the API call.</p> required <p>Returns:</p> Type Description <p>The decorator for turning a typed function into an OpenAI API call.</p>"},{"location":"api/core/openai/#mirascope.core.openai.OpenAICallParams","title":"<code>OpenAICallParams</code>","text":"<p>               Bases: <code>BaseCallParams</code></p> <p>The parameters to use when calling the OpenAI API.</p> Source code in <code>mirascope/core/openai/call_params.py</code> <pre><code>class OpenAICallParams(BaseCallParams):\n    \"\"\"The parameters to use when calling the OpenAI API.\"\"\"\n\n    frequency_penalty: NotRequired[float | None]\n    logit_bias: NotRequired[dict[str, int] | None]\n    logprobs: NotRequired[bool | None]\n    max_tokens: NotRequired[int | None]\n    n: NotRequired[int | None]\n    parallel_tool_calls: NotRequired[bool]\n    presence_penalty: NotRequired[float | None]\n    response_format: NotRequired[ResponseFormat]\n    seed: NotRequired[int | None]\n    stop: NotRequired[str | list[str] | None]\n    stream_options: NotRequired[ChatCompletionStreamOptionsParam | None]\n    temperature: NotRequired[float | None]\n    tool_choice: NotRequired[ChatCompletionToolChoiceOptionParam]\n    top_logprobs: NotRequired[int | None]\n    top_p: NotRequired[float | None]\n    user: NotRequired[str]\n</code></pre>"},{"location":"api/core/openai/#mirascope.core.openai.OpenAICallResponse","title":"<code>OpenAICallResponse</code>","text":"<p>               Bases: <code>BaseCallResponse[ChatCompletion, OpenAITool, OpenAIDynamicConfig, ChatCompletionMessageParam, OpenAICallParams, ChatCompletionUserMessageParam]</code></p> <p>A convenience wrapper around the OpenAI <code>ChatCompletion</code> response.</p> <p>When calling the OpenAI API using a function decorated with <code>openai_call</code>, the response will be an <code>OpenAICallResponse</code> instance with properties that allow for more convenience access to commonly used attributes.</p> <p>Example:</p> <pre><code>from mirascope.core.openai import openai_call\n\n@openai_call(model=\"gpt-4o\")\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\nresponse = recommend_book(\"fantasy\")  # response is an `OpenAICallResponse` instance\nprint(response.content)\n#&gt; Sure! I would recommend...\n</code></pre> Source code in <code>mirascope/core/openai/call_response.py</code> <pre><code>class OpenAICallResponse(\n    BaseCallResponse[\n        ChatCompletion,\n        OpenAITool,\n        OpenAIDynamicConfig,\n        ChatCompletionMessageParam,\n        OpenAICallParams,\n        ChatCompletionUserMessageParam,\n    ]\n):\n    '''A convenience wrapper around the OpenAI `ChatCompletion` response.\n\n    When calling the OpenAI API using a function decorated with `openai_call`, the\n    response will be an `OpenAICallResponse` instance with properties that allow for\n    more convenience access to commonly used attributes.\n\n    Example:\n\n    ```python\n    from mirascope.core.openai import openai_call\n\n    @openai_call(model=\"gpt-4o\")\n    def recommend_book(genre: str):\n        \"\"\"Recommend a {genre} book.\"\"\"\n\n    response = recommend_book(\"fantasy\")  # response is an `OpenAICallResponse` instance\n    print(response.content)\n    #&gt; Sure! I would recommend...\n    ```\n    '''\n\n    _provider = \"openai\"\n\n    @computed_field\n    @property\n    def message_param(self) -&gt; ChatCompletionAssistantMessageParam:\n        \"\"\"Returns the assistants's response as a message parameter.\"\"\"\n        return self.message.model_dump(exclude={\"function_call\"})  # type: ignore\n\n    @property\n    def choices(self) -&gt; list[Choice]:\n        \"\"\"Returns the array of chat completion choices.\"\"\"\n        return self.response.choices\n\n    @property\n    def choice(self) -&gt; Choice:\n        \"\"\"Returns the 0th choice.\"\"\"\n        return self.choices[0]\n\n    @property\n    def message(self) -&gt; ChatCompletionMessage:\n        \"\"\"Returns the message of the chat completion for the 0th choice.\"\"\"\n        return self.choice.message\n\n    @property\n    def content(self) -&gt; str:\n        \"\"\"Returns the content of the chat completion for the 0th choice.\"\"\"\n        return self.message.content if self.message.content is not None else \"\"\n\n    @property\n    def model(self) -&gt; str:\n        \"\"\"Returns the name of the response model.\"\"\"\n        return self.response.model\n\n    @property\n    def id(self) -&gt; str:\n        \"\"\"Returns the id of the response.\"\"\"\n        return self.response.id\n\n    @property\n    def finish_reasons(self) -&gt; list[str]:\n        \"\"\"Returns the finish reasons of the response.\"\"\"\n        return [str(choice.finish_reason) for choice in self.response.choices]\n\n    @property\n    def tool_calls(self) -&gt; list[ChatCompletionMessageToolCall] | None:\n        \"\"\"Returns the tool calls for the 0th choice message.\"\"\"\n        return self.message.tool_calls\n\n    @computed_field\n    @property\n    def tools(self) -&gt; list[OpenAITool] | None:\n        \"\"\"Returns any available tool calls as their `OpenAITool` definition.\n\n        Raises:\n            ValidationError: if a tool call doesn't match the tool's schema.\n        \"\"\"\n        if not self.tool_types or not self.tool_calls:\n            return None\n\n        extracted_tools = []\n        for tool_call in self.tool_calls:\n            for tool_type in self.tool_types:\n                if tool_call.function.name == tool_type._name():\n                    extracted_tools.append(tool_type.from_tool_call(tool_call))\n                    break\n\n        return extracted_tools\n\n    @computed_field\n    @property\n    def tool(self) -&gt; OpenAITool | None:\n        \"\"\"Returns the 0th tool for the 0th choice message.\n\n        Raises:\n            ValidationError: if the tool call doesn't match the tool's schema.\n        \"\"\"\n        if tools := self.tools:\n            return tools[0]\n        return None\n\n    @classmethod\n    def tool_message_params(\n        cls, tools_and_outputs: list[tuple[OpenAITool, str]]\n    ) -&gt; list[ChatCompletionToolMessageParam]:\n        \"\"\"Returns the tool message parameters for tool call results.\"\"\"\n        return [\n            ChatCompletionToolMessageParam(\n                role=\"tool\",\n                content=output,\n                tool_call_id=tool.tool_call.id,\n                name=tool._name(),  # type: ignore\n            )\n            for tool, output in tools_and_outputs\n        ]\n\n    @property\n    def usage(self) -&gt; CompletionUsage | None:\n        \"\"\"Returns the usage of the chat completion.\"\"\"\n        return self.response.usage\n\n    @property\n    def input_tokens(self) -&gt; int | None:\n        \"\"\"Returns the number of input tokens.\"\"\"\n        return self.usage.prompt_tokens if self.usage else None\n\n    @property\n    def output_tokens(self) -&gt; int | None:\n        \"\"\"Returns the number of output tokens.\"\"\"\n        return self.usage.completion_tokens if self.usage else None\n</code></pre>"},{"location":"api/core/openai/#mirascope.core.openai.OpenAICallResponse.choice","title":"<code>choice: Choice</code>  <code>property</code>","text":"<p>Returns the 0th choice.</p>"},{"location":"api/core/openai/#mirascope.core.openai.OpenAICallResponse.choices","title":"<code>choices: list[Choice]</code>  <code>property</code>","text":"<p>Returns the array of chat completion choices.</p>"},{"location":"api/core/openai/#mirascope.core.openai.OpenAICallResponse.content","title":"<code>content: str</code>  <code>property</code>","text":"<p>Returns the content of the chat completion for the 0th choice.</p>"},{"location":"api/core/openai/#mirascope.core.openai.OpenAICallResponse.finish_reasons","title":"<code>finish_reasons: list[str]</code>  <code>property</code>","text":"<p>Returns the finish reasons of the response.</p>"},{"location":"api/core/openai/#mirascope.core.openai.OpenAICallResponse.id","title":"<code>id: str</code>  <code>property</code>","text":"<p>Returns the id of the response.</p>"},{"location":"api/core/openai/#mirascope.core.openai.OpenAICallResponse.input_tokens","title":"<code>input_tokens: int | None</code>  <code>property</code>","text":"<p>Returns the number of input tokens.</p>"},{"location":"api/core/openai/#mirascope.core.openai.OpenAICallResponse.message","title":"<code>message: ChatCompletionMessage</code>  <code>property</code>","text":"<p>Returns the message of the chat completion for the 0th choice.</p>"},{"location":"api/core/openai/#mirascope.core.openai.OpenAICallResponse.message_param","title":"<code>message_param: ChatCompletionAssistantMessageParam</code>  <code>property</code>","text":"<p>Returns the assistants's response as a message parameter.</p>"},{"location":"api/core/openai/#mirascope.core.openai.OpenAICallResponse.model","title":"<code>model: str</code>  <code>property</code>","text":"<p>Returns the name of the response model.</p>"},{"location":"api/core/openai/#mirascope.core.openai.OpenAICallResponse.output_tokens","title":"<code>output_tokens: int | None</code>  <code>property</code>","text":"<p>Returns the number of output tokens.</p>"},{"location":"api/core/openai/#mirascope.core.openai.OpenAICallResponse.tool","title":"<code>tool: OpenAITool | None</code>  <code>property</code>","text":"<p>Returns the 0th tool for the 0th choice message.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if the tool call doesn't match the tool's schema.</p>"},{"location":"api/core/openai/#mirascope.core.openai.OpenAICallResponse.tool_calls","title":"<code>tool_calls: list[ChatCompletionMessageToolCall] | None</code>  <code>property</code>","text":"<p>Returns the tool calls for the 0th choice message.</p>"},{"location":"api/core/openai/#mirascope.core.openai.OpenAICallResponse.tools","title":"<code>tools: list[OpenAITool] | None</code>  <code>property</code>","text":"<p>Returns any available tool calls as their <code>OpenAITool</code> definition.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if a tool call doesn't match the tool's schema.</p>"},{"location":"api/core/openai/#mirascope.core.openai.OpenAICallResponse.usage","title":"<code>usage: CompletionUsage | None</code>  <code>property</code>","text":"<p>Returns the usage of the chat completion.</p>"},{"location":"api/core/openai/#mirascope.core.openai.OpenAICallResponse.tool_message_params","title":"<code>tool_message_params(tools_and_outputs)</code>  <code>classmethod</code>","text":"<p>Returns the tool message parameters for tool call results.</p> Source code in <code>mirascope/core/openai/call_response.py</code> <pre><code>@classmethod\ndef tool_message_params(\n    cls, tools_and_outputs: list[tuple[OpenAITool, str]]\n) -&gt; list[ChatCompletionToolMessageParam]:\n    \"\"\"Returns the tool message parameters for tool call results.\"\"\"\n    return [\n        ChatCompletionToolMessageParam(\n            role=\"tool\",\n            content=output,\n            tool_call_id=tool.tool_call.id,\n            name=tool._name(),  # type: ignore\n        )\n        for tool, output in tools_and_outputs\n    ]\n</code></pre>"},{"location":"api/core/openai/#mirascope.core.openai.OpenAICallResponseChunk","title":"<code>OpenAICallResponseChunk</code>","text":"<p>               Bases: <code>BaseCallResponseChunk[ChatCompletionChunk]</code></p> <p>A convenience wrapper around the OpenAI <code>ChatCompletionChunk</code> streamed chunks.</p> <p>When calling the OpenAI API using a function decorated with <code>openai_call</code> and <code>stream</code> set to <code>True</code>, the stream will contain <code>OpenAIResponseChunk</code> instances with properties that allow for more convenient access to commonly used attributes.</p> <p>Example:</p> <pre><code>from mirascope.core.openai import openai_call\n\n@openai_call(model=\"gpt-4o\", stream=True)\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\nstream = recommend_book(\"fantasy\")  # response is an `OpenAIStream`\nfor chunk in stream:\n    print(chunk.content, end=\"\", flush=True)\n#&gt; Sure! I would recommend...\n</code></pre> Source code in <code>mirascope/core/openai/call_response_chunk.py</code> <pre><code>class OpenAICallResponseChunk(BaseCallResponseChunk[ChatCompletionChunk]):\n    '''A convenience wrapper around the OpenAI `ChatCompletionChunk` streamed chunks.\n\n    When calling the OpenAI API using a function decorated with `openai_call` and\n    `stream` set to `True`, the stream will contain `OpenAIResponseChunk` instances with\n    properties that allow for more convenient access to commonly used attributes.\n\n    Example:\n\n    ```python\n    from mirascope.core.openai import openai_call\n\n    @openai_call(model=\"gpt-4o\", stream=True)\n    def recommend_book(genre: str):\n        \"\"\"Recommend a {genre} book.\"\"\"\n\n    stream = recommend_book(\"fantasy\")  # response is an `OpenAIStream`\n    for chunk in stream:\n        print(chunk.content, end=\"\", flush=True)\n    #&gt; Sure! I would recommend...\n    ```\n    '''\n\n    @property\n    def choices(self) -&gt; list[Choice]:\n        \"\"\"Returns the array of chat completion choices.\"\"\"\n        return self.chunk.choices\n\n    @property\n    def choice(self) -&gt; Choice:\n        \"\"\"Returns the 0th choice.\"\"\"\n        return self.chunk.choices[0]\n\n    @property\n    def delta(self) -&gt; ChoiceDelta | None:\n        \"\"\"Returns the delta for the 0th choice.\"\"\"\n        if self.chunk.choices:\n            return self.chunk.choices[0].delta\n        return None\n\n    @property\n    def content(self) -&gt; str:\n        \"\"\"Returns the content for the 0th choice delta.\"\"\"\n        return (\n            self.delta.content if self.delta is not None and self.delta.content else \"\"\n        )\n\n    @property\n    def model(self) -&gt; str:\n        \"\"\"Returns the name of the response model.\"\"\"\n        return self.chunk.model\n\n    @property\n    def id(self) -&gt; str:\n        \"\"\"Returns the id of the response.\"\"\"\n        return self.chunk.id\n\n    @property\n    def finish_reasons(self) -&gt; list[str]:\n        \"\"\"Returns the finish reasons of the response.\"\"\"\n        return [str(choice.finish_reason) for choice in self.chunk.choices]\n\n    @property\n    def tool_calls(self) -&gt; list[ChoiceDeltaToolCall] | None:\n        \"\"\"Returns the partial tool calls for the 0th choice message.\n\n        The first `list[ChoiceDeltaToolCall]` will contain the name of the tool and\n        index, and subsequent `list[ChoiceDeltaToolCall]`s will contain the arguments\n        which will be strings that need to be concatenated with future\n        `list[ChoiceDeltaToolCall]`s to form a complete JSON tool calls. The last\n        `list[ChoiceDeltaToolCall]` will be None indicating end of stream.\n        \"\"\"\n        if self.delta:\n            return self.delta.tool_calls\n        return None\n\n    @property\n    def usage(self) -&gt; CompletionUsage | None:\n        \"\"\"Returns the usage of the chat completion.\"\"\"\n        if self.chunk.usage:\n            return self.chunk.usage\n        return None\n\n    @property\n    def input_tokens(self) -&gt; int | None:\n        \"\"\"Returns the number of input tokens.\"\"\"\n        if self.usage:\n            return self.usage.prompt_tokens\n        return None\n\n    @property\n    def output_tokens(self) -&gt; int | None:\n        \"\"\"Returns the number of output tokens.\"\"\"\n        if self.usage:\n            return self.usage.completion_tokens\n        return None\n</code></pre>"},{"location":"api/core/openai/#mirascope.core.openai.OpenAICallResponseChunk.choice","title":"<code>choice: Choice</code>  <code>property</code>","text":"<p>Returns the 0th choice.</p>"},{"location":"api/core/openai/#mirascope.core.openai.OpenAICallResponseChunk.choices","title":"<code>choices: list[Choice]</code>  <code>property</code>","text":"<p>Returns the array of chat completion choices.</p>"},{"location":"api/core/openai/#mirascope.core.openai.OpenAICallResponseChunk.content","title":"<code>content: str</code>  <code>property</code>","text":"<p>Returns the content for the 0th choice delta.</p>"},{"location":"api/core/openai/#mirascope.core.openai.OpenAICallResponseChunk.delta","title":"<code>delta: ChoiceDelta | None</code>  <code>property</code>","text":"<p>Returns the delta for the 0th choice.</p>"},{"location":"api/core/openai/#mirascope.core.openai.OpenAICallResponseChunk.finish_reasons","title":"<code>finish_reasons: list[str]</code>  <code>property</code>","text":"<p>Returns the finish reasons of the response.</p>"},{"location":"api/core/openai/#mirascope.core.openai.OpenAICallResponseChunk.id","title":"<code>id: str</code>  <code>property</code>","text":"<p>Returns the id of the response.</p>"},{"location":"api/core/openai/#mirascope.core.openai.OpenAICallResponseChunk.input_tokens","title":"<code>input_tokens: int | None</code>  <code>property</code>","text":"<p>Returns the number of input tokens.</p>"},{"location":"api/core/openai/#mirascope.core.openai.OpenAICallResponseChunk.model","title":"<code>model: str</code>  <code>property</code>","text":"<p>Returns the name of the response model.</p>"},{"location":"api/core/openai/#mirascope.core.openai.OpenAICallResponseChunk.output_tokens","title":"<code>output_tokens: int | None</code>  <code>property</code>","text":"<p>Returns the number of output tokens.</p>"},{"location":"api/core/openai/#mirascope.core.openai.OpenAICallResponseChunk.tool_calls","title":"<code>tool_calls: list[ChoiceDeltaToolCall] | None</code>  <code>property</code>","text":"<p>Returns the partial tool calls for the 0th choice message.</p> <p>The first <code>list[ChoiceDeltaToolCall]</code> will contain the name of the tool and index, and subsequent <code>list[ChoiceDeltaToolCall]</code>s will contain the arguments which will be strings that need to be concatenated with future <code>list[ChoiceDeltaToolCall]</code>s to form a complete JSON tool calls. The last <code>list[ChoiceDeltaToolCall]</code> will be None indicating end of stream.</p>"},{"location":"api/core/openai/#mirascope.core.openai.OpenAICallResponseChunk.usage","title":"<code>usage: CompletionUsage | None</code>  <code>property</code>","text":"<p>Returns the usage of the chat completion.</p>"},{"location":"api/core/openai/#mirascope.core.openai.OpenAITool","title":"<code>OpenAITool</code>","text":"<p>               Bases: <code>BaseTool</code></p> <p>A class for defining tools for OpenAI LLM calls.</p> Source code in <code>mirascope/core/openai/tool.py</code> <pre><code>class OpenAITool(BaseTool):\n    \"\"\"A class for defining tools for OpenAI LLM calls.\"\"\"\n\n    tool_call: ChatCompletionMessageToolCall\n\n    @classmethod\n    def tool_schema(cls) -&gt; ChatCompletionToolParam:\n        \"\"\"Constructs a JSON Schema tool schema from the `BaseModel` schema defined.\"\"\"\n        model_schema = cls.model_json_schema()\n        model_schema.pop(\"title\", None)\n        model_schema.pop(\"description\", None)\n\n        fn = FunctionDefinition(name=cls._name(), description=cls._description())\n        if model_schema[\"properties\"]:\n            fn[\"parameters\"] = model_schema\n\n        return ChatCompletionToolParam(function=fn, type=\"function\")\n\n    @classmethod\n    def from_tool_call(cls, tool_call: ChatCompletionMessageToolCall) -&gt; OpenAITool:\n        \"\"\"Constructs an `OpenAITool` instance from a `tool_call`.\"\"\"\n        model_json = jiter.from_json(tool_call.function.arguments.encode())\n        model_json[\"tool_call\"] = tool_call.model_dump()\n        return cls.model_validate(model_json)\n</code></pre>"},{"location":"api/core/openai/#mirascope.core.openai.OpenAITool.from_tool_call","title":"<code>from_tool_call(tool_call)</code>  <code>classmethod</code>","text":"<p>Constructs an <code>OpenAITool</code> instance from a <code>tool_call</code>.</p> Source code in <code>mirascope/core/openai/tool.py</code> <pre><code>@classmethod\ndef from_tool_call(cls, tool_call: ChatCompletionMessageToolCall) -&gt; OpenAITool:\n    \"\"\"Constructs an `OpenAITool` instance from a `tool_call`.\"\"\"\n    model_json = jiter.from_json(tool_call.function.arguments.encode())\n    model_json[\"tool_call\"] = tool_call.model_dump()\n    return cls.model_validate(model_json)\n</code></pre>"},{"location":"api/core/openai/#mirascope.core.openai.OpenAITool.tool_schema","title":"<code>tool_schema()</code>  <code>classmethod</code>","text":"<p>Constructs a JSON Schema tool schema from the <code>BaseModel</code> schema defined.</p> Source code in <code>mirascope/core/openai/tool.py</code> <pre><code>@classmethod\ndef tool_schema(cls) -&gt; ChatCompletionToolParam:\n    \"\"\"Constructs a JSON Schema tool schema from the `BaseModel` schema defined.\"\"\"\n    model_schema = cls.model_json_schema()\n    model_schema.pop(\"title\", None)\n    model_schema.pop(\"description\", None)\n\n    fn = FunctionDefinition(name=cls._name(), description=cls._description())\n    if model_schema[\"properties\"]:\n        fn[\"parameters\"] = model_schema\n\n    return ChatCompletionToolParam(function=fn, type=\"function\")\n</code></pre>"},{"location":"api/core/openai/call/","title":"mirascope.core.openai.call","text":"<p>The <code>openai_call</code> decorator for functions as LLM calls.</p>"},{"location":"api/core/openai/call/#mirascope.core.openai.call.openai_call","title":"<code>openai_call = call_factory(TCallResponse=OpenAICallResponse, TCallResponseChunk=OpenAICallResponseChunk, TDynamicConfig=OpenAIDynamicConfig, TMessageParamType=ChatCompletionAssistantMessageParam, TToolType=OpenAITool, TStream=OpenAIStream, TCallParams=OpenAICallParams, default_call_params=OpenAICallParams(), setup_call=setup_call, get_json_output=get_json_output, handle_stream=handle_stream, handle_stream_async=handle_stream_async, calculate_cost=calculate_cost, provider='openai')</code>  <code>module-attribute</code>","text":"<p>A decorator for calling the OpenAI API with a typed function.</p> <p>This decorator is used to wrap a typed function that calls the OpenAI API. It parses the docstring of the wrapped function as the messages array and templates the input arguments for the function into each message's template.</p> <p>Example:</p> <pre><code>@openai_call(model=\"gpt-4o\")\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\nresponse = recommend_book(\"fantasy\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>The OpenAI model to use in the API call.</p> required <code>stream</code> <p>Whether to stream the response from the API call.</p> required <code>tools</code> <p>The tools to use in the OpenAI API call.</p> required <code>**call_params</code> <p>The <code>OpenAICallParams</code> call parameters to use in the API call.</p> required <p>Returns:</p> Type Description <p>The decorator for turning a typed function into an OpenAI API call.</p>"},{"location":"api/core/openai/call_params/","title":"mirascope.core.openai.call_params","text":"<p>The parameters to use when calling the OpenAI API.</p>"},{"location":"api/core/openai/call_params/#mirascope.core.openai.call_params.OpenAICallParams","title":"<code>OpenAICallParams</code>","text":"<p>               Bases: <code>BaseCallParams</code></p> <p>The parameters to use when calling the OpenAI API.</p> Source code in <code>mirascope/core/openai/call_params.py</code> <pre><code>class OpenAICallParams(BaseCallParams):\n    \"\"\"The parameters to use when calling the OpenAI API.\"\"\"\n\n    frequency_penalty: NotRequired[float | None]\n    logit_bias: NotRequired[dict[str, int] | None]\n    logprobs: NotRequired[bool | None]\n    max_tokens: NotRequired[int | None]\n    n: NotRequired[int | None]\n    parallel_tool_calls: NotRequired[bool]\n    presence_penalty: NotRequired[float | None]\n    response_format: NotRequired[ResponseFormat]\n    seed: NotRequired[int | None]\n    stop: NotRequired[str | list[str] | None]\n    stream_options: NotRequired[ChatCompletionStreamOptionsParam | None]\n    temperature: NotRequired[float | None]\n    tool_choice: NotRequired[ChatCompletionToolChoiceOptionParam]\n    top_logprobs: NotRequired[int | None]\n    top_p: NotRequired[float | None]\n    user: NotRequired[str]\n</code></pre>"},{"location":"api/core/openai/call_response/","title":"mirascope.core.openai.call_response","text":"<p>This module contains the <code>OpenAICallResponse</code> class.</p>"},{"location":"api/core/openai/call_response/#mirascope.core.openai.call_response.OpenAICallResponse","title":"<code>OpenAICallResponse</code>","text":"<p>               Bases: <code>BaseCallResponse[ChatCompletion, OpenAITool, OpenAIDynamicConfig, ChatCompletionMessageParam, OpenAICallParams, ChatCompletionUserMessageParam]</code></p> <p>A convenience wrapper around the OpenAI <code>ChatCompletion</code> response.</p> <p>When calling the OpenAI API using a function decorated with <code>openai_call</code>, the response will be an <code>OpenAICallResponse</code> instance with properties that allow for more convenience access to commonly used attributes.</p> <p>Example:</p> <pre><code>from mirascope.core.openai import openai_call\n\n@openai_call(model=\"gpt-4o\")\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\nresponse = recommend_book(\"fantasy\")  # response is an `OpenAICallResponse` instance\nprint(response.content)\n#&gt; Sure! I would recommend...\n</code></pre> Source code in <code>mirascope/core/openai/call_response.py</code> <pre><code>class OpenAICallResponse(\n    BaseCallResponse[\n        ChatCompletion,\n        OpenAITool,\n        OpenAIDynamicConfig,\n        ChatCompletionMessageParam,\n        OpenAICallParams,\n        ChatCompletionUserMessageParam,\n    ]\n):\n    '''A convenience wrapper around the OpenAI `ChatCompletion` response.\n\n    When calling the OpenAI API using a function decorated with `openai_call`, the\n    response will be an `OpenAICallResponse` instance with properties that allow for\n    more convenience access to commonly used attributes.\n\n    Example:\n\n    ```python\n    from mirascope.core.openai import openai_call\n\n    @openai_call(model=\"gpt-4o\")\n    def recommend_book(genre: str):\n        \"\"\"Recommend a {genre} book.\"\"\"\n\n    response = recommend_book(\"fantasy\")  # response is an `OpenAICallResponse` instance\n    print(response.content)\n    #&gt; Sure! I would recommend...\n    ```\n    '''\n\n    _provider = \"openai\"\n\n    @computed_field\n    @property\n    def message_param(self) -&gt; ChatCompletionAssistantMessageParam:\n        \"\"\"Returns the assistants's response as a message parameter.\"\"\"\n        return self.message.model_dump(exclude={\"function_call\"})  # type: ignore\n\n    @property\n    def choices(self) -&gt; list[Choice]:\n        \"\"\"Returns the array of chat completion choices.\"\"\"\n        return self.response.choices\n\n    @property\n    def choice(self) -&gt; Choice:\n        \"\"\"Returns the 0th choice.\"\"\"\n        return self.choices[0]\n\n    @property\n    def message(self) -&gt; ChatCompletionMessage:\n        \"\"\"Returns the message of the chat completion for the 0th choice.\"\"\"\n        return self.choice.message\n\n    @property\n    def content(self) -&gt; str:\n        \"\"\"Returns the content of the chat completion for the 0th choice.\"\"\"\n        return self.message.content if self.message.content is not None else \"\"\n\n    @property\n    def model(self) -&gt; str:\n        \"\"\"Returns the name of the response model.\"\"\"\n        return self.response.model\n\n    @property\n    def id(self) -&gt; str:\n        \"\"\"Returns the id of the response.\"\"\"\n        return self.response.id\n\n    @property\n    def finish_reasons(self) -&gt; list[str]:\n        \"\"\"Returns the finish reasons of the response.\"\"\"\n        return [str(choice.finish_reason) for choice in self.response.choices]\n\n    @property\n    def tool_calls(self) -&gt; list[ChatCompletionMessageToolCall] | None:\n        \"\"\"Returns the tool calls for the 0th choice message.\"\"\"\n        return self.message.tool_calls\n\n    @computed_field\n    @property\n    def tools(self) -&gt; list[OpenAITool] | None:\n        \"\"\"Returns any available tool calls as their `OpenAITool` definition.\n\n        Raises:\n            ValidationError: if a tool call doesn't match the tool's schema.\n        \"\"\"\n        if not self.tool_types or not self.tool_calls:\n            return None\n\n        extracted_tools = []\n        for tool_call in self.tool_calls:\n            for tool_type in self.tool_types:\n                if tool_call.function.name == tool_type._name():\n                    extracted_tools.append(tool_type.from_tool_call(tool_call))\n                    break\n\n        return extracted_tools\n\n    @computed_field\n    @property\n    def tool(self) -&gt; OpenAITool | None:\n        \"\"\"Returns the 0th tool for the 0th choice message.\n\n        Raises:\n            ValidationError: if the tool call doesn't match the tool's schema.\n        \"\"\"\n        if tools := self.tools:\n            return tools[0]\n        return None\n\n    @classmethod\n    def tool_message_params(\n        cls, tools_and_outputs: list[tuple[OpenAITool, str]]\n    ) -&gt; list[ChatCompletionToolMessageParam]:\n        \"\"\"Returns the tool message parameters for tool call results.\"\"\"\n        return [\n            ChatCompletionToolMessageParam(\n                role=\"tool\",\n                content=output,\n                tool_call_id=tool.tool_call.id,\n                name=tool._name(),  # type: ignore\n            )\n            for tool, output in tools_and_outputs\n        ]\n\n    @property\n    def usage(self) -&gt; CompletionUsage | None:\n        \"\"\"Returns the usage of the chat completion.\"\"\"\n        return self.response.usage\n\n    @property\n    def input_tokens(self) -&gt; int | None:\n        \"\"\"Returns the number of input tokens.\"\"\"\n        return self.usage.prompt_tokens if self.usage else None\n\n    @property\n    def output_tokens(self) -&gt; int | None:\n        \"\"\"Returns the number of output tokens.\"\"\"\n        return self.usage.completion_tokens if self.usage else None\n</code></pre>"},{"location":"api/core/openai/call_response/#mirascope.core.openai.call_response.OpenAICallResponse.choice","title":"<code>choice: Choice</code>  <code>property</code>","text":"<p>Returns the 0th choice.</p>"},{"location":"api/core/openai/call_response/#mirascope.core.openai.call_response.OpenAICallResponse.choices","title":"<code>choices: list[Choice]</code>  <code>property</code>","text":"<p>Returns the array of chat completion choices.</p>"},{"location":"api/core/openai/call_response/#mirascope.core.openai.call_response.OpenAICallResponse.content","title":"<code>content: str</code>  <code>property</code>","text":"<p>Returns the content of the chat completion for the 0th choice.</p>"},{"location":"api/core/openai/call_response/#mirascope.core.openai.call_response.OpenAICallResponse.finish_reasons","title":"<code>finish_reasons: list[str]</code>  <code>property</code>","text":"<p>Returns the finish reasons of the response.</p>"},{"location":"api/core/openai/call_response/#mirascope.core.openai.call_response.OpenAICallResponse.id","title":"<code>id: str</code>  <code>property</code>","text":"<p>Returns the id of the response.</p>"},{"location":"api/core/openai/call_response/#mirascope.core.openai.call_response.OpenAICallResponse.input_tokens","title":"<code>input_tokens: int | None</code>  <code>property</code>","text":"<p>Returns the number of input tokens.</p>"},{"location":"api/core/openai/call_response/#mirascope.core.openai.call_response.OpenAICallResponse.message","title":"<code>message: ChatCompletionMessage</code>  <code>property</code>","text":"<p>Returns the message of the chat completion for the 0th choice.</p>"},{"location":"api/core/openai/call_response/#mirascope.core.openai.call_response.OpenAICallResponse.message_param","title":"<code>message_param: ChatCompletionAssistantMessageParam</code>  <code>property</code>","text":"<p>Returns the assistants's response as a message parameter.</p>"},{"location":"api/core/openai/call_response/#mirascope.core.openai.call_response.OpenAICallResponse.model","title":"<code>model: str</code>  <code>property</code>","text":"<p>Returns the name of the response model.</p>"},{"location":"api/core/openai/call_response/#mirascope.core.openai.call_response.OpenAICallResponse.output_tokens","title":"<code>output_tokens: int | None</code>  <code>property</code>","text":"<p>Returns the number of output tokens.</p>"},{"location":"api/core/openai/call_response/#mirascope.core.openai.call_response.OpenAICallResponse.tool","title":"<code>tool: OpenAITool | None</code>  <code>property</code>","text":"<p>Returns the 0th tool for the 0th choice message.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if the tool call doesn't match the tool's schema.</p>"},{"location":"api/core/openai/call_response/#mirascope.core.openai.call_response.OpenAICallResponse.tool_calls","title":"<code>tool_calls: list[ChatCompletionMessageToolCall] | None</code>  <code>property</code>","text":"<p>Returns the tool calls for the 0th choice message.</p>"},{"location":"api/core/openai/call_response/#mirascope.core.openai.call_response.OpenAICallResponse.tools","title":"<code>tools: list[OpenAITool] | None</code>  <code>property</code>","text":"<p>Returns any available tool calls as their <code>OpenAITool</code> definition.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>if a tool call doesn't match the tool's schema.</p>"},{"location":"api/core/openai/call_response/#mirascope.core.openai.call_response.OpenAICallResponse.usage","title":"<code>usage: CompletionUsage | None</code>  <code>property</code>","text":"<p>Returns the usage of the chat completion.</p>"},{"location":"api/core/openai/call_response/#mirascope.core.openai.call_response.OpenAICallResponse.tool_message_params","title":"<code>tool_message_params(tools_and_outputs)</code>  <code>classmethod</code>","text":"<p>Returns the tool message parameters for tool call results.</p> Source code in <code>mirascope/core/openai/call_response.py</code> <pre><code>@classmethod\ndef tool_message_params(\n    cls, tools_and_outputs: list[tuple[OpenAITool, str]]\n) -&gt; list[ChatCompletionToolMessageParam]:\n    \"\"\"Returns the tool message parameters for tool call results.\"\"\"\n    return [\n        ChatCompletionToolMessageParam(\n            role=\"tool\",\n            content=output,\n            tool_call_id=tool.tool_call.id,\n            name=tool._name(),  # type: ignore\n        )\n        for tool, output in tools_and_outputs\n    ]\n</code></pre>"},{"location":"api/core/openai/call_response_chunk/","title":"mirascope.core.openai.call_response_chunk","text":"<p>This module contains the <code>OpenAICallResponseChunk</code> class.</p>"},{"location":"api/core/openai/call_response_chunk/#mirascope.core.openai.call_response_chunk.OpenAICallResponseChunk","title":"<code>OpenAICallResponseChunk</code>","text":"<p>               Bases: <code>BaseCallResponseChunk[ChatCompletionChunk]</code></p> <p>A convenience wrapper around the OpenAI <code>ChatCompletionChunk</code> streamed chunks.</p> <p>When calling the OpenAI API using a function decorated with <code>openai_call</code> and <code>stream</code> set to <code>True</code>, the stream will contain <code>OpenAIResponseChunk</code> instances with properties that allow for more convenient access to commonly used attributes.</p> <p>Example:</p> <pre><code>from mirascope.core.openai import openai_call\n\n@openai_call(model=\"gpt-4o\", stream=True)\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\nstream = recommend_book(\"fantasy\")  # response is an `OpenAIStream`\nfor chunk in stream:\n    print(chunk.content, end=\"\", flush=True)\n#&gt; Sure! I would recommend...\n</code></pre> Source code in <code>mirascope/core/openai/call_response_chunk.py</code> <pre><code>class OpenAICallResponseChunk(BaseCallResponseChunk[ChatCompletionChunk]):\n    '''A convenience wrapper around the OpenAI `ChatCompletionChunk` streamed chunks.\n\n    When calling the OpenAI API using a function decorated with `openai_call` and\n    `stream` set to `True`, the stream will contain `OpenAIResponseChunk` instances with\n    properties that allow for more convenient access to commonly used attributes.\n\n    Example:\n\n    ```python\n    from mirascope.core.openai import openai_call\n\n    @openai_call(model=\"gpt-4o\", stream=True)\n    def recommend_book(genre: str):\n        \"\"\"Recommend a {genre} book.\"\"\"\n\n    stream = recommend_book(\"fantasy\")  # response is an `OpenAIStream`\n    for chunk in stream:\n        print(chunk.content, end=\"\", flush=True)\n    #&gt; Sure! I would recommend...\n    ```\n    '''\n\n    @property\n    def choices(self) -&gt; list[Choice]:\n        \"\"\"Returns the array of chat completion choices.\"\"\"\n        return self.chunk.choices\n\n    @property\n    def choice(self) -&gt; Choice:\n        \"\"\"Returns the 0th choice.\"\"\"\n        return self.chunk.choices[0]\n\n    @property\n    def delta(self) -&gt; ChoiceDelta | None:\n        \"\"\"Returns the delta for the 0th choice.\"\"\"\n        if self.chunk.choices:\n            return self.chunk.choices[0].delta\n        return None\n\n    @property\n    def content(self) -&gt; str:\n        \"\"\"Returns the content for the 0th choice delta.\"\"\"\n        return (\n            self.delta.content if self.delta is not None and self.delta.content else \"\"\n        )\n\n    @property\n    def model(self) -&gt; str:\n        \"\"\"Returns the name of the response model.\"\"\"\n        return self.chunk.model\n\n    @property\n    def id(self) -&gt; str:\n        \"\"\"Returns the id of the response.\"\"\"\n        return self.chunk.id\n\n    @property\n    def finish_reasons(self) -&gt; list[str]:\n        \"\"\"Returns the finish reasons of the response.\"\"\"\n        return [str(choice.finish_reason) for choice in self.chunk.choices]\n\n    @property\n    def tool_calls(self) -&gt; list[ChoiceDeltaToolCall] | None:\n        \"\"\"Returns the partial tool calls for the 0th choice message.\n\n        The first `list[ChoiceDeltaToolCall]` will contain the name of the tool and\n        index, and subsequent `list[ChoiceDeltaToolCall]`s will contain the arguments\n        which will be strings that need to be concatenated with future\n        `list[ChoiceDeltaToolCall]`s to form a complete JSON tool calls. The last\n        `list[ChoiceDeltaToolCall]` will be None indicating end of stream.\n        \"\"\"\n        if self.delta:\n            return self.delta.tool_calls\n        return None\n\n    @property\n    def usage(self) -&gt; CompletionUsage | None:\n        \"\"\"Returns the usage of the chat completion.\"\"\"\n        if self.chunk.usage:\n            return self.chunk.usage\n        return None\n\n    @property\n    def input_tokens(self) -&gt; int | None:\n        \"\"\"Returns the number of input tokens.\"\"\"\n        if self.usage:\n            return self.usage.prompt_tokens\n        return None\n\n    @property\n    def output_tokens(self) -&gt; int | None:\n        \"\"\"Returns the number of output tokens.\"\"\"\n        if self.usage:\n            return self.usage.completion_tokens\n        return None\n</code></pre>"},{"location":"api/core/openai/call_response_chunk/#mirascope.core.openai.call_response_chunk.OpenAICallResponseChunk.choice","title":"<code>choice: Choice</code>  <code>property</code>","text":"<p>Returns the 0th choice.</p>"},{"location":"api/core/openai/call_response_chunk/#mirascope.core.openai.call_response_chunk.OpenAICallResponseChunk.choices","title":"<code>choices: list[Choice]</code>  <code>property</code>","text":"<p>Returns the array of chat completion choices.</p>"},{"location":"api/core/openai/call_response_chunk/#mirascope.core.openai.call_response_chunk.OpenAICallResponseChunk.content","title":"<code>content: str</code>  <code>property</code>","text":"<p>Returns the content for the 0th choice delta.</p>"},{"location":"api/core/openai/call_response_chunk/#mirascope.core.openai.call_response_chunk.OpenAICallResponseChunk.delta","title":"<code>delta: ChoiceDelta | None</code>  <code>property</code>","text":"<p>Returns the delta for the 0th choice.</p>"},{"location":"api/core/openai/call_response_chunk/#mirascope.core.openai.call_response_chunk.OpenAICallResponseChunk.finish_reasons","title":"<code>finish_reasons: list[str]</code>  <code>property</code>","text":"<p>Returns the finish reasons of the response.</p>"},{"location":"api/core/openai/call_response_chunk/#mirascope.core.openai.call_response_chunk.OpenAICallResponseChunk.id","title":"<code>id: str</code>  <code>property</code>","text":"<p>Returns the id of the response.</p>"},{"location":"api/core/openai/call_response_chunk/#mirascope.core.openai.call_response_chunk.OpenAICallResponseChunk.input_tokens","title":"<code>input_tokens: int | None</code>  <code>property</code>","text":"<p>Returns the number of input tokens.</p>"},{"location":"api/core/openai/call_response_chunk/#mirascope.core.openai.call_response_chunk.OpenAICallResponseChunk.model","title":"<code>model: str</code>  <code>property</code>","text":"<p>Returns the name of the response model.</p>"},{"location":"api/core/openai/call_response_chunk/#mirascope.core.openai.call_response_chunk.OpenAICallResponseChunk.output_tokens","title":"<code>output_tokens: int | None</code>  <code>property</code>","text":"<p>Returns the number of output tokens.</p>"},{"location":"api/core/openai/call_response_chunk/#mirascope.core.openai.call_response_chunk.OpenAICallResponseChunk.tool_calls","title":"<code>tool_calls: list[ChoiceDeltaToolCall] | None</code>  <code>property</code>","text":"<p>Returns the partial tool calls for the 0th choice message.</p> <p>The first <code>list[ChoiceDeltaToolCall]</code> will contain the name of the tool and index, and subsequent <code>list[ChoiceDeltaToolCall]</code>s will contain the arguments which will be strings that need to be concatenated with future <code>list[ChoiceDeltaToolCall]</code>s to form a complete JSON tool calls. The last <code>list[ChoiceDeltaToolCall]</code> will be None indicating end of stream.</p>"},{"location":"api/core/openai/call_response_chunk/#mirascope.core.openai.call_response_chunk.OpenAICallResponseChunk.usage","title":"<code>usage: CompletionUsage | None</code>  <code>property</code>","text":"<p>Returns the usage of the chat completion.</p>"},{"location":"api/core/openai/dynamic_config/","title":"mirascope.core.openai.dynamic_config","text":"<p>This module defines the function return type for functions as LLM calls.</p>"},{"location":"api/core/openai/dynamic_config/#mirascope.core.openai.dynamic_config.OpenAIDynamicConfig","title":"<code>OpenAIDynamicConfig = BaseDynamicConfig[ChatCompletionMessageParam, OpenAICallParams]</code>  <code>module-attribute</code>","text":"<p>The function return type for functions wrapped with the <code>openai_call</code> decorator.</p> <p>Attributes:</p> Name Type Description <code>computed_fields</code> <p>The computed fields to use in the prompt template.</p> <code>messages</code> <p>The messages to send to the OpenAI API. If provided, <code>computed_fields</code> will be ignored.</p> <code>call_params</code> <p>The call parameters to use when calling the OpenAI API. These will override any call parameters provided to the decorator.</p> <p>Example:</p> <pre><code>from mirascope.core.openai import OpenAIDynamicConfig, openai_call\n\n@openai_call(model=\"gpt-4o\")\ndef recommend_book(genre: str) -&gt; OpenAIDynamicConfig:\n    \"\"\"Recommend a {capitalized_genre} book.\"\"\"\n    reeturn {\"computed_fields\": {\"capitalized_genre\": genre.capitalize()}}\n</code></pre>"},{"location":"api/core/openai/tool/","title":"mirascope.core.openai.tool","text":"<p>The <code>OpenAITool</code> class for easy tool usage with OpenAI LLM calls.</p>"},{"location":"api/core/openai/tool/#mirascope.core.openai.tool.OpenAITool","title":"<code>OpenAITool</code>","text":"<p>               Bases: <code>BaseTool</code></p> <p>A class for defining tools for OpenAI LLM calls.</p> Source code in <code>mirascope/core/openai/tool.py</code> <pre><code>class OpenAITool(BaseTool):\n    \"\"\"A class for defining tools for OpenAI LLM calls.\"\"\"\n\n    tool_call: ChatCompletionMessageToolCall\n\n    @classmethod\n    def tool_schema(cls) -&gt; ChatCompletionToolParam:\n        \"\"\"Constructs a JSON Schema tool schema from the `BaseModel` schema defined.\"\"\"\n        model_schema = cls.model_json_schema()\n        model_schema.pop(\"title\", None)\n        model_schema.pop(\"description\", None)\n\n        fn = FunctionDefinition(name=cls._name(), description=cls._description())\n        if model_schema[\"properties\"]:\n            fn[\"parameters\"] = model_schema\n\n        return ChatCompletionToolParam(function=fn, type=\"function\")\n\n    @classmethod\n    def from_tool_call(cls, tool_call: ChatCompletionMessageToolCall) -&gt; OpenAITool:\n        \"\"\"Constructs an `OpenAITool` instance from a `tool_call`.\"\"\"\n        model_json = jiter.from_json(tool_call.function.arguments.encode())\n        model_json[\"tool_call\"] = tool_call.model_dump()\n        return cls.model_validate(model_json)\n</code></pre>"},{"location":"api/core/openai/tool/#mirascope.core.openai.tool.OpenAITool.from_tool_call","title":"<code>from_tool_call(tool_call)</code>  <code>classmethod</code>","text":"<p>Constructs an <code>OpenAITool</code> instance from a <code>tool_call</code>.</p> Source code in <code>mirascope/core/openai/tool.py</code> <pre><code>@classmethod\ndef from_tool_call(cls, tool_call: ChatCompletionMessageToolCall) -&gt; OpenAITool:\n    \"\"\"Constructs an `OpenAITool` instance from a `tool_call`.\"\"\"\n    model_json = jiter.from_json(tool_call.function.arguments.encode())\n    model_json[\"tool_call\"] = tool_call.model_dump()\n    return cls.model_validate(model_json)\n</code></pre>"},{"location":"api/core/openai/tool/#mirascope.core.openai.tool.OpenAITool.tool_schema","title":"<code>tool_schema()</code>  <code>classmethod</code>","text":"<p>Constructs a JSON Schema tool schema from the <code>BaseModel</code> schema defined.</p> Source code in <code>mirascope/core/openai/tool.py</code> <pre><code>@classmethod\ndef tool_schema(cls) -&gt; ChatCompletionToolParam:\n    \"\"\"Constructs a JSON Schema tool schema from the `BaseModel` schema defined.\"\"\"\n    model_schema = cls.model_json_schema()\n    model_schema.pop(\"title\", None)\n    model_schema.pop(\"description\", None)\n\n    fn = FunctionDefinition(name=cls._name(), description=cls._description())\n    if model_schema[\"properties\"]:\n        fn[\"parameters\"] = model_schema\n\n    return ChatCompletionToolParam(function=fn, type=\"function\")\n</code></pre>"},{"location":"api/integrations/","title":"mirascope integrations","text":"<p>Integrations with third party libraries.</p>"},{"location":"api/integrations/hyperdx/","title":"mirascope.integrations.hyperdx","text":"<p>Mirascope x HyperDX Integration.</p>"},{"location":"api/integrations/hyperdx/#mirascope.integrations.hyperdx.with_hyperdx","title":"<code>with_hyperdx(cls)</code>","text":"<p>Decorator to wrap a function with hyperdx.</p> Source code in <code>mirascope/integrations/hyperdx.py</code> <pre><code>def with_hyperdx(cls):\n    \"\"\"Decorator to wrap a function with hyperdx.\"\"\"\n    provider = trace.get_tracer_provider()\n    if not isinstance(provider, TracerProvider):\n        configure(\n            processors=[\n                BatchSpanProcessor(\n                    OTLPSpanExporter(\n                        endpoint=\"https://in-otel.hyperdx.io/v1/traces\",\n                        headers={\"authorization\": os.getenv(\"HYPERDX_API_KEY\")},\n                    )\n                )\n            ]\n        )\n    return with_otel(cls)\n</code></pre>"},{"location":"api/integrations/langfuse/","title":"mirascope.integrations.langfuse","text":"<p>Mirascope x Langfuse Integration.</p>"},{"location":"api/integrations/langfuse/#mirascope.integrations.langfuse.with_langfuse","title":"<code>with_langfuse(fn)</code>","text":"<p>Wraps a Mirascope function with Langfuse tracing.</p> Source code in <code>mirascope/integrations/langfuse.py</code> <pre><code>def with_langfuse(\n    fn: Callable[_P, _R] | Callable[_P, Awaitable[_R]],\n) -&gt; Callable[_P, _R] | Callable[_P, Awaitable[_R]]:\n    \"\"\"Wraps a Mirascope function with Langfuse tracing.\"\"\"\n\n    def get_call_response_observation(result: BaseCallResponse):\n        return {\n            \"name\": f\"{fn.__name__} with {result.model}\",\n            \"input\": result.prompt_template,\n            \"metadata\": result.response,\n            \"tags\": result.tags,\n            \"model\": result.model,\n            \"output\": result.message_param.get(\"content\", None),\n        }\n\n    def get_stream_observation(stream: BaseStream):\n        return {\n            \"name\": f\"{fn.__name__} with {stream.model}\",\n            \"input\": stream.prompt_template,\n            \"tags\": fn.__annotations__.get(\"tags\", []),\n            \"model\": stream.model,\n            \"output\": stream.message_param.get(\"content\", None),\n        }\n\n    def handle_call_response(result: BaseCallResponse):\n        usage = ModelUsage(\n            input=result.input_tokens,\n            output=result.output_tokens,\n            unit=\"TOKENS\",\n        )\n        langfuse_context.update_current_observation(\n            **get_call_response_observation(result),\n            usage=usage,\n        )\n\n    def handle_stream(stream: BaseStream):\n        # TODO: Re-add once Mirascope supports streaming usage\n        # usage = ModelUsage(\n        #     input=stream.input_tokens,\n        #     output=stream.output_tokens,\n        #     unit=\"TOKENS\",\n        # )\n        langfuse_context.update_current_observation(\n            **get_stream_observation(stream),\n            # usage=usage,\n        )\n\n    def handle_base_model(result: BaseModel):\n        if result._response is not None:\n            response: BaseCallResponse = result._response\n            handle_call_response(response)\n        langfuse_context.update_current_observation(output=result)\n\n    async def handle_call_response_async(result: BaseCallResponse):\n        handle_call_response(result)\n\n    async def handle_stream_async(stream: BaseStream):\n        handle_stream(stream)\n\n    async def handle_base_model_async(result: BaseModel):\n        handle_base_model(result)\n\n    return middleware(\n        fn,\n        custom_decorator=observe(\n            name=fn.__name__,\n            as_type=\"generation\",\n            capture_input=False,\n            capture_output=False,\n        ),\n        handle_call_response=handle_call_response,\n        handle_call_response_async=handle_call_response_async,\n        handle_stream=handle_stream,\n        handle_stream_async=handle_stream_async,\n        handle_base_model=handle_base_model,\n        handle_base_model_async=handle_base_model_async,\n    )\n</code></pre>"},{"location":"api/integrations/logfire/","title":"mirascope.integrations.logfire","text":"<p>Mirascope x Logfire Integration.</p>"},{"location":"api/integrations/logfire/#mirascope.integrations.logfire.with_logfire","title":"<code>with_logfire(fn)</code>","text":"<p>Wraps a Mirascope function with Logfire tracing.</p> Source code in <code>mirascope/integrations/logfire.py</code> <pre><code>def with_logfire(\n    fn: Callable[_P, _R] | Callable[_P, Awaitable[_R]],\n) -&gt; Callable[_P, _R] | Callable[_P, Awaitable[_R]]:\n    \"\"\"Wraps a Mirascope function with Logfire tracing.\"\"\"\n\n    @contextmanager\n    def custom_context_manager() -&gt; Generator[logfire.LogfireSpan, Any, None]:\n        metadata = fn.__annotations__.get(\"metadata\", {})\n        tags = getattr(metadata, \"tags\", {})\n        with logfire.with_settings(\n            custom_scope_suffix=\"mirascope\", tags=list(tags)\n        ).span(fn.__name__) as logfire_span:\n            yield logfire_span\n\n    def get_call_response_span_data(result: BaseCallResponse) -&gt; dict:\n        return {\n            \"async\": False,\n            \"call_params\": result.call_params,\n            \"model\": result.model,\n            \"provider\": result._provider,\n            \"prompt_template\": result.prompt_template,\n            \"template_variables\": result.fn_args,\n            \"messages\": result.messages,\n            \"response_data\": result.response,\n        }\n\n    def get_stream_span_data(stream: BaseStream) -&gt; dict:\n        return {\n            \"messages\": [stream.user_message_param],\n            \"call_params\": stream.call_params,\n            \"model\": stream.model,\n            \"provider\": stream.provider,\n            \"prompt_template\": stream.prompt_template,\n            \"template_variables\": stream.fn_args,\n            \"output\": {\n                \"cost\": stream.cost,\n                \"content\": stream.message_param.get(\"content\", None),\n            },\n        }\n\n    def get_tool_calls(result: BaseCallResponse) -&gt; list[dict] | None:\n        if tools := result.tools:\n            tool_calls = [\n                {\n                    \"function\": {\n                        \"arguments\": tool.model_dump_json(exclude={\"tool_call\"}),\n                        \"name\": tool.name(),\n                    }\n                }\n                for tool in tools\n            ]\n            return tool_calls\n        return None\n\n    def get_output(result: BaseCallResponse) -&gt; dict[str, Any]:\n        output: dict[str, Any] = {}\n        if cost := result.cost:\n            output[\"cost\"] = cost\n        if input_tokens := result.input_tokens:\n            output[\"input_tokens\"] = input_tokens\n        if output_tokens := result.output_tokens:\n            output[\"output_tokens\"] = output_tokens\n        if content := result.content:\n            output[\"content\"] = content\n        return output\n\n    def handle_call_response(\n        result: BaseCallResponse, logfire_span: logfire.LogfireSpan | None\n    ):\n        if logfire_span is None:\n            return\n        output = get_output(result)\n        span_data = get_call_response_span_data(result)\n        span_data[\"async\"] = False\n        tool_calls = get_tool_calls(result)\n        if tool_calls:\n            output[\"tool_calls\"] = tool_calls\n        span_data[\"output\"] = output\n        logfire_span.set_attributes(span_data)\n\n    def handle_stream(\n        stream: BaseStream,\n        logfire_span: logfire.LogfireSpan | None,\n    ):\n        span_data = get_stream_span_data(stream) | {\"async\": False}\n        if logfire_span is not None:\n            logfire_span.set_attributes(span_data)\n\n    def handle_base_model(result: BaseModel, logfire_span: logfire.LogfireSpan | None):\n        span_data: dict[str, Any] = {}\n        output: dict[str, Any] = {}\n        if result._response is not None:\n            response: BaseCallResponse = result._response\n            span_data |= get_call_response_span_data(response)\n            output |= get_output(response)\n        output[\"response_model\"] = result\n        span_data[\"async\"] = False\n        span_data[\"output\"] = output\n        logfire_span.set_attributes(span_data)\n\n    async def handle_call_response_async(\n        result: BaseCallResponse, logfire_span: logfire.LogfireSpan | None\n    ):\n        if logfire_span is None:\n            return\n        output = get_output(result)\n        span_data = get_call_response_span_data(result)\n        span_data[\"async\"] = True\n        tool_calls = get_tool_calls(result)\n        if tool_calls:\n            output[\"tool_calls\"] = tool_calls\n        span_data[\"output\"] = output\n        logfire_span.set_attributes(span_data)\n\n    async def handle_stream_async(\n        stream: BaseStream,\n        logfire_span: logfire.LogfireSpan | None,\n    ):\n        span_data = get_stream_span_data(stream) | {\"async\": True}\n        if logfire_span is not None:\n            logfire_span.set_attributes(span_data)\n\n    async def handle_base_model_async(\n        result: BaseModel, logfire_span: logfire.LogfireSpan | None\n    ):\n        span_data: dict[str, Any] = {}\n        output: dict[str, Any] = {}\n        if result._response is not None:\n            response: BaseCallResponse = result._response\n            span_data |= get_call_response_span_data(response)\n            output |= get_output(response)\n        output[\"response_model\"] = result\n        span_data[\"async\"] = True\n        span_data[\"output\"] = output\n        logfire_span.set_attributes(span_data)\n\n    return middleware(\n        fn,\n        custom_context_manager=custom_context_manager,\n        handle_call_response=handle_call_response,\n        handle_call_response_async=handle_call_response_async,\n        handle_stream=handle_stream,\n        handle_stream_async=handle_stream_async,\n        handle_base_model=handle_base_model,\n        handle_base_model_async=handle_base_model_async,\n    )\n</code></pre>"},{"location":"api/integrations/otel/","title":"mirascope.integrations.otel","text":"<p>Mirascope x OpenTelemetry Integration.</p>"},{"location":"api/integrations/otel/#mirascope.integrations.otel.configure","title":"<code>configure(processors=None)</code>","text":"<p>Configures the OpenTelemetry tracer, this function should only be called once.</p> <p>Parameters:</p> Name Type Description Default <code>processors</code> <code>Sequence[SpanProcessor] | None</code> <p>Optional[Sequence[SpanProcessor]] The span processors to use, if None, a console exporter will be used.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tracer</code> <p>The configured tracer.</p> Source code in <code>mirascope/integrations/otel.py</code> <pre><code>def configure(\n    processors: Sequence[SpanProcessor] | None = None,\n) -&gt; Tracer:\n    \"\"\"Configures the OpenTelemetry tracer, this function should only be called once.\n\n    Args:\n        processors: Optional[Sequence[SpanProcessor]]\n            The span processors to use, if None, a console exporter will be used.\n\n    Returns:\n        The configured tracer.\n    \"\"\"\n    provider = TracerProvider()\n    if processors is None:\n        provider.add_span_processor(SimpleSpanProcessor(ConsoleSpanExporter()))\n    else:\n        for processor in processors:\n            provider.add_span_processor(processor)\n    # NOTE: Sets the global trace provider, should only be called once\n    set_tracer_provider(provider)\n    return get_tracer(\"otel\")\n</code></pre>"},{"location":"api/integrations/otel/#mirascope.integrations.otel.with_otel","title":"<code>with_otel(fn)</code>","text":"<p>Wraps a Mirascope function with OpenTelemetry tracing.</p> Source code in <code>mirascope/integrations/otel.py</code> <pre><code>def with_otel(\n    fn: Callable[_P, _R] | Callable[_P, Awaitable[_R]],\n) -&gt; Callable[_P, _R] | Callable[_P, Awaitable[_R]]:\n    \"\"\"Wraps a Mirascope function with OpenTelemetry tracing.\"\"\"\n\n    @contextmanager\n    def custom_context_manager() -&gt; Generator[Span, Any, None]:\n        tracer = get_tracer(\"otel\")\n        with tracer.start_as_current_span(f\"{fn.__name__}\") as span:\n            yield span\n\n    def handle_call_response(result: BaseCallResponse, span: Span | None):\n        if span is None:\n            return\n        max_tokens = getattr(result.call_params, \"max_tokens\", 0)\n        temperature = getattr(result.call_params, \"temperature\", 0)\n        top_p = getattr(result.call_params, \"top_p\", 0)\n        attributes: dict[str, AttributeValue] = {\n            \"async\": False,\n            \"gen_ai.system\": result.prompt_template,\n            \"gen_ai.request.model\": result.model,\n            \"gen_ai.request.max_tokens\": max_tokens,\n            \"gen_ai.request.temperature\": temperature,\n            \"gen_ai.request.top_p\": top_p,\n            \"gen_ai.response.model\": result.model,\n            \"gen_ai.response.id\": result.id,\n            \"gen_ai.response.finish_reasons\": result.finish_reasons,\n            \"gen_ai.usage.completion_tokens\": result.output_tokens,\n            \"gen_ai.usage.prompt_tokens\": result.input_tokens,\n        }\n\n        prompt_event: Attributes = {\n            \"gen_ai.prompt\": json.dumps(result.user_message_param),\n        }\n        completion_event: Attributes = {\n            \"gen_ai.completion\": json.dumps(result.message_param),\n        }\n        span.set_attributes(attributes)\n        span.add_event(\"gen_ai.content.prompt\", attributes=prompt_event)\n        span.add_event(\n            \"gen_ai.content.completion\",\n            attributes=completion_event,\n        )\n\n    def handle_stream(stream: BaseStream, span: Span | None):\n        if span is None:\n            return\n        max_tokens = getattr(stream.call_params, \"max_tokens\", 0)\n        temperature = getattr(stream.call_params, \"temperature\", 0)\n        top_p = getattr(stream.call_params, \"top_p\", 0)\n        attributes: dict[str, AttributeValue] = {\n            \"async\": False,\n            \"gen_ai.system\": stream.prompt_template,\n            \"gen_ai.request.model\": stream.model,\n            \"gen_ai.request.max_tokens\": max_tokens,\n            \"gen_ai.request.temperature\": temperature,\n            \"gen_ai.request.top_p\": top_p,\n            \"gen_ai.response.model\": stream.model,\n            \"gen_ai.usage.completion_tokens\": stream.output_tokens,\n            \"gen_ai.usage.prompt_tokens\": stream.input_tokens,\n        }\n\n        prompt_event: Attributes = {\n            \"gen_ai.prompt\": json.dumps(stream.user_message_param),\n        }\n        completion_event: Attributes = {\n            \"gen_ai.completion\": json.dumps(stream.message_param),\n        }\n        span.set_attributes(attributes)\n        span.add_event(\"gen_ai.content.prompt\", attributes=prompt_event)\n        span.add_event(\n            \"gen_ai.content.completion\",\n            attributes=completion_event,\n        )\n\n    def handle_base_model(result: BaseModel, span: Span | None): ...\n\n    async def handle_call_response_async(\n        result: BaseCallResponse, span: Span | None\n    ): ...\n\n    async def handle_stream_async(stream: BaseStream, span: Span | None): ...\n\n    provider = get_tracer_provider()\n    if not isinstance(provider, TracerProvider):\n        configure()\n    return middleware(\n        fn,\n        custom_context_manager=custom_context_manager,\n        handle_call_response=handle_call_response,\n        handle_call_response_async=handle_call_response_async,\n        handle_stream=handle_stream,\n        handle_stream_async=handle_stream_async,\n        handle_base_model=handle_base_model,\n        # handle_base_model_async=handle_base_model_async,\n    )\n</code></pre>"},{"location":"api/integrations/tenacity/","title":"mirascope.integrations.tenacity","text":"<p>Mirascope x Tenacity Integration.</p>"},{"location":"api/integrations/tenacity/#mirascope.integrations.tenacity.collect_validation_errors","title":"<code>collect_validation_errors(retry_state)</code>","text":"<p>Collect errors into a <code>validation_errors</code> kwarg of the wrapped function.</p> Source code in <code>mirascope/integrations/tenacity.py</code> <pre><code>def collect_validation_errors(retry_state: RetryCallState) -&gt; None:\n    \"\"\"Collect errors into a `validation_errors` kwarg of the wrapped function.\"\"\"\n    if (\n        (outcome := retry_state.outcome)\n        and (exception := outcome.exception())\n        and type(exception) == ValidationError\n    ):\n        errors = retry_state.kwargs.pop(\"validation_errors\", [])\n        retry_state.kwargs[\"validation_errors\"] = (\n            errors + [exception] if errors else [exception]\n        )\n</code></pre>"},{"location":"api/integrations/utils/","title":"mirascope.integrations.utils","text":"<p>Mirascope middleware utils.</p>"},{"location":"api/integrations/utils/#mirascope.integrations.utils.middleware","title":"<code>middleware(fn, *, custom_context_manager=default_context_manager, custom_decorator=None, handle_call_response=None, handle_call_response_async=None, handle_stream=None, handle_stream_async=None, handle_base_model=None, handle_base_model_async=None)</code>","text":"<p>Wraps a Mirascope function with middleware.</p> Source code in <code>mirascope/integrations/utils.py</code> <pre><code>def middleware(\n    fn: Callable[_P, _R] | Callable[_P, Awaitable[_R]],\n    *,\n    custom_context_manager: Callable[\n        [], Generator[_T, Any, None]\n    ] = default_context_manager,\n    custom_decorator: _DecoratorType | None = None,\n    handle_call_response: Callable[[BaseCallResponse, _T | None], None] | None = None,\n    handle_call_response_async: Callable[[BaseCallResponse, _T | None], Awaitable[None]]\n    | None = None,\n    handle_stream: Callable[[BaseStream, _T | None], None] | None = None,\n    handle_stream_async: Callable[[BaseStream, _T | None], Awaitable[None]]\n    | None = None,\n    handle_base_model: Callable[[BaseModel, _T | None], None] | None = None,\n    handle_base_model_async: Callable[[BaseModel, _T | None], Awaitable[None]]\n    | None = None,\n) -&gt; Callable[_P, _R] | Callable[_P, Awaitable[_R]]:\n    \"\"\"Wraps a Mirascope function with middleware.\"\"\"\n\n    def inner(*args: _P.args, **kwargs: _P.kwargs) -&gt; _R:\n        result = fn(*args, **kwargs)\n        if isinstance(result, BaseCallResponse) and handle_call_response is not None:\n            with custom_context_manager() as context:\n                if context is None:\n                    handle_call_response(result)\n                else:\n                    handle_call_response(result, context)\n        elif isinstance(result, BaseStream) and handle_stream is not None:\n            original_iter = result.__iter__\n\n            def new_iter(self):\n                # Create the context manager when user iterates over the stream\n                with custom_context_manager() as context:\n                    for chunk, tool in original_iter():\n                        yield chunk, tool\n                    if handle_stream is not None:\n                        if context is None:\n                            handle_stream(result)\n                        else:\n                            handle_stream(result, context)\n\n            result.__class__.__iter__ = (\n                custom_decorator(new_iter) if custom_decorator else new_iter\n            )\n        elif isinstance(result, BaseModel) and handle_base_model is not None:\n            with custom_context_manager() as context:\n                if context is not None:\n                    handle_base_model(result, context)\n                else:\n                    handle_base_model(result)\n        elif isinstance(result, Iterable):\n            original_iter = result.__iter__\n\n            def new_iter(self):\n                # Create the context manager when user iterates over the stream\n                with custom_context_manager() as context:\n                    for chunk in original_iter():\n                        yield chunk\n                    if handle_base_model is not None:\n                        if context is None:\n                            handle_base_model(result)\n                        else:\n                            handle_base_model(result, context)\n\n            result.__class__.__iter__ = (\n                custom_decorator(new_iter) if custom_decorator else new_iter\n            )\n        return result\n\n    async def inner_async(*args: _P.args, **kwargs: _P.kwargs) -&gt; Awaitable[_R]:\n        result = await fn(*args, **kwargs)\n        if (\n            isinstance(result, BaseCallResponse)\n            and handle_call_response_async is not None\n        ):\n            with custom_context_manager() as context:\n                if context is None:\n                    await handle_call_response_async(result)\n                else:\n                    await handle_call_response_async(result, context)\n        elif isinstance(result, BaseStream) and handle_stream_async is not None:\n            original_aiter = result.__aiter__\n\n            def new_aiter(self):\n                async def generator():\n                    with custom_context_manager() as context:\n                        async for chunk, tool in original_aiter():\n                            yield chunk, tool\n                        if handle_stream_async is not None:\n                            if context is None:\n                                await handle_stream_async(result)\n                            else:\n                                await handle_stream_async(result, context)\n\n                return generator()\n\n            result.__class__.__aiter__ = (\n                custom_decorator(new_aiter) if custom_decorator else new_aiter\n            )\n        elif isinstance(result, BaseModel) and handle_base_model_async is not None:\n            with custom_context_manager() as context:\n                if context is None:\n                    await handle_base_model_async(result)\n                else:\n                    await handle_base_model_async(result, context)\n        elif isinstance(result, AsyncIterable):\n            original_aiter = result.__aiter__\n\n            def new_aiter(self):\n                async def generator():\n                    with custom_context_manager() as context:\n                        async for chunk in original_aiter():\n                            yield chunk\n                        if handle_stream_async is not None:\n                            if context is None:\n                                await handle_base_model_async(result)\n                            else:\n                                await handle_base_model_async(result, context)\n\n                return generator()\n\n            result.__class__.__aiter__ = (\n                custom_decorator(new_aiter) if custom_decorator else new_aiter\n            )\n        return result\n\n    inner_fn = inner_async if inspect.iscoroutinefunction(fn) else inner\n    decorated_fn = custom_decorator(inner_fn) if custom_decorator else inner_fn\n    return wraps(fn)(decorated_fn)\n</code></pre>"},{"location":"cookbook/","title":"Cookbook","text":"<p>Under construction...</p>"},{"location":"cookbook/agents/","title":"Agents","text":"<p>If we take everything covered in the Usage section so far, we have the formula for a clean and intuitive agent interface. Access to the class\u2019s attributes, properties, and other Pydantic functionalities let us manipulate stateful workflows with ease.</p> <p>Here\u2019s an example of a Librarian agent, with a full breakdown below.</p> <pre><code>from typing import Literal\n\nfrom openai.types.chat import ChatCompletionMessageParam\nfrom pydantic import BaseModel, Field\n\nfrom mirascope.core import BasePrompt, anthropic, openai\nfrom mirascope.core.base import BaseTool, BaseToolKit, toolkit_tool\nfrom mirascope.integrations.tenacity import collect_validation_errors\n\nclass Book(BaseModel):\n    title: str\n    author: str\n\nclass CategorizedQuery(BaseModel):\n    category: Literal[\"general\", \"reading_list\", \"book_rec\"] = Field(\n        ...,\n        description=\"\"\"The category of the query.\n            general for generic chatting and questions.\n            reading_list for explicit requests to modify or view reading list.\n            book_rec for explicit book recommendation requests.\"\"\",\n    )\n    content: str = Field(..., description=\"The original text of the categorized query.\")\n\nclass BookTools(BaseToolKit):\n    reading_level: str\n\n    @toolkit_tool\n    def _recommend_book_by_level(self, title: str, author: str) -&gt; str:\n        \"\"\"Returns the title and author of a book recommendation nicely formatted.\n\n        Reading level: {self.reading_level} # Add highlight\n        \"\"\"\n        return f\"{title} by {author}.\"\n\nclass Librarian(BaseModel):\n    _history: list[ChatCompletionMessageParam] = []\n    reading_list: list[Book] = []\n\n    def _add_to_reading_list(self, book: Book) -&gt; str:\n        \"\"\"Add a book to the reading list.\"\"\"\n        if book not in self.reading_list:\n            self.reading_list.append(book)\n            return f\"Added {book.title} to reading list.\"\n        else:\n            return f\"Book {book.title} already in reading list.\"\n\n    def _remove_from_reading_list(self, book: Book) -&gt; str:\n        \"\"\"Remove a book from the reading list.\"\"\"\n        if book in self.reading_list:\n            self.reading_list.remove(book)\n            return f\"Removed {book.title} from reading list.\"\n        else:\n            return f\"Book {book.title} not in reading list.\"\n\n    def _get_reading_list(self) -&gt; str:\n        \"\"\"Gets the reading list.\"\"\"\n        return \"\\n\".join([str(book) for book in self.reading_list])\n\n    @retry(stop=stop_after_attempt(3), after=collect_validation_errors)\n    @openai.call(model=\"gpt-4\", response_model=CategorizedQuery)\n    def _categorize_input(\n        user_input: str, validation_errors: list[str] | None = None\n    ) -&gt; openai.OpenAIDynamicConfig:\n        \"\"\"\n        {previous_errors}\n        Categorize this input into one of the following:\n\n        general - for general questions and chatting.\n        reading_list - an explicit request to modify or view the user's reading list\n        book_rec - an explicit request for a book recommendation\n\n        input: {user_input}\n        \"\"\"\n        return {\n            \"computed_fields\": {\n                \"previous_errors\": f\"Previous errors: {validation_errors}\"\n                if validation_errors\n                else \"\"\n            }\n        }\n\n    @openai.call(model=\"gpt-4o\", output_parser=str)\n    def _summarize_reading_level(self) -&gt; openai.OpenAIDynamicConfig:\n        \"\"\"\n        Here is my reading list: {reading_list}\n        Based on the books you see, describe my reading level. If my reading list is\n        empty, just say \"unknown\". Respond with at most two words.\n        \"\"\"\n        return {\"computed_fields\": {\"reading_list\": self.reading_list}}\n\n    @openai.call(model=\"gpt-4o\", stream=True)\n    def _stream(self, query: CategorizedQuery) -&gt; openai.OpenAIDynamicConfig:\n        \"\"\"\n        SYSTEM:\n        You are the world's greatest librarian.\n        You are talking to someone with {reading_level} reading level.\n        MESSAGES: {self._history}\n        USER: {content}\n        \"\"\"\n        reading_level = self._summarize_reading_level()\n        if query.category == \"book_rec\":\n            toolkit = BookTools(reading_level=reading_level)\n            tools = toolkit.create_tools()\n        elif query.category == \"reading_list\":\n            tools = [\n                self._add_to_reading_list,\n                self._remove_from_reading_list,\n                self._get_reading_list,\n            ]\n        else:\n            tools = []\n        return {\n            \"tools\": tools,\n            \"computed_fields\": {\n                \"reading_level\": reading_level,\n                \"content\": query.content,\n            },\n        }\n\n    def _step(self, user_input: str):\n        \"\"\"Runs a single chat step with the librarian.\"\"\"\n        query = self._categorize_input(user_input)\n        stream, tools_and_outputs = self._stream(query), []\n        for chunk, tool in stream:\n            if tool:\n                output = tool.call()\n                print(output)\n                tools_and_outputs.append((tool, output))\n            else:\n                print(chunk, end=\"\", flush=True)\n        self._history += [\n            stream.user_message_param,\n            stream.message_param,\n            *stream.tool_message_params(tools_and_outputs),\n        ]\n\n    def chat(self):\n        \"\"\"Runs a multi-turn chat with the librarian.\"\"\"\n        while True:\n            user_input = input(\"You: \")\n            if user_input in [\"quit\", \"exit\"]:\n                break\n            print(\"Librarian: \", end=\"\", flush=True)\n            self._step(user_input)\n            print()\n\nlibrarian = Librarian()\nlibrarian.chat()\n</code></pre>"},{"location":"cookbook/agents/#agent-structure","title":"Agent Structure","text":"<p>Relevant topics: decorating instance methods[link], message injection[link]</p> <p>Let\u2019s start analyzing the agent by looking at its general structure. We use our ability to decorate instance methods to create calls that can depend on state via the class attributes. </p> <p><code>_stream()</code> is the core of the librarian which handles user\u2019s input. We use Mirascope\u2019s messages injection to give context/chat history to the prompt with the <code>MESSAGES</code> keyword:</p> <pre><code>@openai.call(model=\"gpt-4o\", stream=True)\ndef _stream(self, query: CategorizedQuery) -&gt; openai.OpenAIDynamicConfig:\n    \"\"\"\n    SYSTEM:\n    You are the world's greatest librarian.\n    You are talking to someone with {reading_level} reading level.\n    MESSAGES: {self._history} # highlight\n    USER: {content}\n    \"\"\"\n</code></pre> <p><code>_step()</code> represents a single interaction with the agent. After getting a response from <code>_stream()</code>, we update the <code>_history</code> attribute so consequent interactions are always up to date:</p> <pre><code> def _step(self, user_input: str):\n        ...\n        stream, tools_and_outputs = self._stream(query), []\n        ...\n        self._history += [\n            stream.user_message_param,\n            stream.message_param,\n            *stream.tool_message_params(tools_and_outputs),\n        ]\n</code></pre> <p><code>run()</code> is a loop to keep the chat going and handle the I/O via command line.</p>"},{"location":"cookbook/agents/#categorizing-input","title":"Categorizing Input","text":"<p>Relevant topics: Response Model[link], Retries[link]</p> <p>A good use case for extraction[link] in our agent is to categorize the incoming user inputs so the agent can handle each input accordingly. We define the class <code>CategorizedQuery</code> so that we can handle 3 types of inputs. In <code>_step()</code>, before passing the input to <code>_stream()</code>, we categorize each input with the call <code>_categorize_input()</code> by setting <code>response_model=CategorizedQuery</code> in its decorator. Adding descriptions in the Pydantic <code>Field</code>s and retries with Mirascope\u2019s tenacity integration helps ensure accurate and successful extractions, which we need since the agent depends on the ability to categorize input.</p> <pre><code>class CategorizedQuery(BaseModel):\n    category: Literal[\"general\", \"reading_list\", \"book_rec\"] = Field(\n        ...,\n        description=\"\"\"The category of the query.\n            general for generic chatting and questions.\n            ...\n            \"\"\",\n    )\n    content: str = Field(..., description=\"The original text of the categorized query.\")\n...\n\nclass Librarian(BaseModel):\n   ...\n\n    @retry(stop=stop_after_attempt(3), after=collect_validation_errors)\n    @openai.call(model=\"gpt-4\", response_model=CategorizedQuery)\n    def _categorize_input(\n        user_input: str, validation_errors: list[str] | None = None\n    ) -&gt; openai.OpenAIDynamicConfig:\n        \"\"\"\n        {previous_errors}\n        Categorize this input into one of the following:\n\n        general - for general questions and chatting.\n        ...\n        {input}\n        \"\"\"\n        return {\n            \"computed_fields\": {\n                \"previous_errors\": f\"Previous errors: {validation_errors}\"\n                if validation_errors\n                else \"\"\n            }\n        }\n     ...\n\n    def _step(self, user_input: str):\n        \"\"\"Runs a single chat step with the librarian.\"\"\"\n        query = self._categorize_input(user_input)\n        stream, tools_and_outputs = self._stream(query), []\n        ...\n</code></pre>"},{"location":"cookbook/agents/#dynamically-analyzed-reading-level","title":"Dynamically Analyzed Reading Level","text":"<p>Relevant topics: Computed Fields[link], Chaining[link]</p> <p>We can customize our librarian by letting them know of our reading level, dynamically analyzed from the books in our reading list. Separate prompts asking specific questions will yield more accurate results than asking several questions at once, so we create a new call <code>_summarize_reading_level()</code>. To incorporate its output into our primary function <code>_stream()</code>, we can chain the calls together with the <code>computed_fields</code> field in our dynamic configuration:</p> <pre><code>class Librarian(BaseModel):\n    _history: list[ChatCompletionMessageParam] = []\n    reading_list: list[Book] = []\n\n    @openai.call(model=\"gpt-4o\", output_parser=str)\n    def _summarize_reading_level(self) -&gt; openai.OpenAIDynamicConfig:\n        \"\"\"\n        Here is my reading list: {reading_list}\n        Based on the books you see, describe my reading level. If my reading list is\n        empty, just say \"unknown\". Respond with at most two words.\n        \"\"\"\n        return {\"computed_fields\": {\"reading_list\": self.reading_list}}\n\n    @openai.call(model=\"gpt-4o\", stream=True)\n    def _stream(self, query: CategorizedQuery) -&gt; openai.OpenAIDynamicConfig:\n        \"\"\"\n        SYSTEM:\n        You are the world's greatest librarian.\n        You are talking to someone with {reading_level} reading level.\n        MESSAGES: {self._history}\n        USER: {content}\n        \"\"\"\n        reading_level = self._summarize_reading_level()\n        ...\n        return {\n            \"computed_fields\": {\n                \"reading_level\": reading_level\n            },\n        }\n</code></pre>"},{"location":"cookbook/agents/#adding-our-tools","title":"Adding Our Tools","text":"<p>An agent without tools isn\u2019t much more than a chatbot, so let\u2019s incorporate some tools into our Librarian to give it functionality other than conversation. </p>"},{"location":"cookbook/agents/#instance-methods","title":"Instance Methods","text":"<p>Relevant topics: Tools/Function Calling[link], Dynamic Configuration[link], Tools via Dynamic Configuration[link]</p> <p>As instance methods, we have <code>_add_to_reading_list()</code>, <code>_remove_from_reading_list</code>, and <code>_get_reading_list()</code>, which do as they advertise to the class attribute <code>reading_list</code>. We take advantage of Mirascope\u2019s dynamic configuration to (a) use these tools despite being instance methods, since dynamic configuration gives access to <code>self</code>, and (b) add our tools conditionally, limiting the chance of the model picking the wrong tool or using tools when it shouldn\u2019t.</p> <pre><code>class Librarian(BaseModel):\n        ...\n\n    def _add_to_reading_list(self, book: Book) -&gt; str:\n        ...\n\n    def _remove_from_reading_list(self, book: Book) -&gt; str:\n        ...\n\n    def _get_reading_list(self) -&gt; str:\n        ...\n\n    @openai.call(model=\"gpt-4o\", stream=True)\n    def _stream(self, query: CategorizedQuery) -&gt; openai.OpenAIDynamicConfig:\n        ...\n        elif query.category == \"reading_list\":\n            tools = [\n                self._add_to_reading_list,\n                self._remove_from_reading_list,\n                self._get_reading_list,\n            ]\n        else:\n            tools = []\n        return {\n            \"tools\": tools\n        }\n</code></pre>"},{"location":"cookbook/agents/#toolkit-tools","title":"ToolKit Tools","text":"<p>Relevant topics: ToolKit[link]</p> <p>We also want a tool for recommending books, but we can improve that functionality by asking for a book recommendation in accordance with our reading level. We already dynamically retrieve our reading level from the current reading list, so we can create a <code>ToolKit</code> that takes <code>reading_level</code> as an input to dynamically generate a tool tailored to our needs. Just like the instance methods above, we can also make sure to only include it in our call when we have explicitly asked for a book recommendation.</p> <pre><code>class BookTools(BaseToolKit):\n    reading_level: str\n\n    @toolkit_tool\n    def _recommend_book_by_level(self, title: str, author: str) -&gt; str:\n        \"\"\"Returns the title and author of a book recommendation nicely formatted.\n\n        Reading level: {self.reading_level} # Add highlight\n        \"\"\"\n        return f\"{title} by {author}.\"\n\nclass Librarian(BaseModel):\n\n    @openai.call(model=\"gpt-4o\", output_parser=str)\n    def _summarize_reading_level(self) -&gt; openai.OpenAIDynamicConfig:\n        ...\n\n    @openai.call(model=\"gpt-4o\", stream=True)\n    def _stream(self, query: CategorizedQuery) -&gt; openai.OpenAIDynamicConfig:\n        ...\n        reading_level = self._summarize_reading_level()\n        if query.category == \"book_rec\":\n            toolkit = BookTools(reading_level=reading_level)\n            tools = toolkit.create_tools()\n        else:\n            tools = []\n        return {\n            \"tools\": tools,\n            \"computed_fields\": {\n                \"reading_level\": reading_level,\n                \"content\": query.content,\n            },\n        }\n</code></pre> <p>This Librarian is just the tip of the iceberg of the kind of agents you can create with Mirascope\u2019s primitives driven approach to LLM toolkits. For more inspiration, check out examples[link] and our cookbook[link]</p>"},{"location":"cookbook/chaining/","title":"Chaining Calls","text":"<p>When chaining multiple calls with Mirascope, we recommend you use computed fields to colocate the metadata of all calls along the chain.</p> <p>Directly passing the output of one call as input to the next does work (as it is just Python), but you miss out on the ability to see all relevant inputs from one location.</p>"},{"location":"cookbook/chaining/#using-calls","title":"Using Calls","text":"<p>When using Mirascope\u2019s decorated functions, set the output of the previous call as a computed field.</p> <pre><code>from mirascope.core import openai\n\n@openai.call(model=\"gpt-4o\", temperature=0.4)\ndef recommend_author(genre: str):\n    \"\"\"Who is the greatest {genre} author?. Give just the name.\"\"\"\n\n@openai.call(model=\"gpt-4o\")\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book by {author}.\"\"\"\n    return {\"computed_fields\": {\"author\": recommend_author(genre)}}\n\n\nresponse = recommend_book(genre=\"fantasy\")\nprint(response.content)\n# &gt; Certainly! \"The Hobbit\" is one of J.R.R. Tolkien's greatest works...\n</code></pre> <p>Mirascope\u2019s response objects are built on top Pydantic, so responses are serializable with the <code>model_dump()</code> method.  With computed fields, the serialization becomes a nested dictionary structure you can navigate from the final response to see relevant metadata from the calls along the chain.</p> <pre><code>recommend_book_dump = response.model_dump()\n\nprint(recommend_book_dump)\n# &gt; {\n#     'call_params': {}\n#     ...\n#     'fn_return': {\n#       'computed_fields': {\n#         'author': {\n#           'call_params': {'temperature': 0.4},\n#           'cost': 0.00017,\n#           ...\n#         }\n#       }\n#     }\n\nrecommend_author_dump = recommend_book_dump[\"fn_return\"][\"computed_fields\"][\"author\"]\n\nprint(recommend_author_dump[\"call_params\"][\"temperature\"])\n# &gt; 0.4\n\nprint(recommend_author_dump[\"cost\"])\n# &gt; 0.00017\n\nprint(recommend_author_dump[\"prompt_template\"])\n# &gt; Who is the greatest {genre} author?. Give just the name.\n</code></pre>"},{"location":"cookbook/chaining/#using-baseprompt","title":"Using BasePrompt","text":"<p>Using <code>BasePrompt</code> follows the same concept, but instead we use Pydantic\u2019s <code>computed_field</code> decorator to make sure previous calls are included in the dump. We can type <code>author</code> to return an <code>OpenAICallResponse</code> since the parser will call <code>str()</code> on it before being formatted into the prompt.</p> <pre><code>from pydantic import computed_field\nfrom mirascope.core import BasePrompt\nfrom mirascope.core.openai import OpenAICallResponse\n\nclass AuthorRecommendationPrompt(BasePrompt):\n    \"\"\"Recommend an author for {genre}. Give just the name.\"\"\"\n\n    genre: str\n\nclass BookRecommendationPrompt(BasePrompt):\n    \"\"\"Recommend a {genre} book by {author}.\"\"\"\n\n    genre: str\n\n    @computed_field\n    def author(self) -&gt; OpenAICallResponse:\n        response = AuthorRecommendationPrompt(genre=self.genre).run(\n            openai.call(\"gpt-4o\")\n        )\n        return response\n\nbook_recommendation_prompt = BookRecommendationPrompt(genre=\"fantasy\")\nresponse = book_recommendation_prompt.run(openai.call(\"gpt-4o\"))\nprint(response.content)\n# &gt; , I highly recommend \"Mistborn: The Final Empire\" by Brandon Sanderson...\n</code></pre> <p>Dumping the response with <code>model_dump</code> shows the previous call in its serialization:</p> <pre><code>recommend_book_dump = response.model_dump()\n\nprint(recommend_book_dump)\n# &gt; {\n#     ...\n#     'author': {\n#       'tags': [],\n#       `response`: {\n#           \"created\": 1719513410,\n#           \"model\": \"gpt-4o-2024-05-13\",\n#         ...\n#       }\n#       \"prompt_template\": \"Recommend an author for {genre}. Give just the name.\",\n#       ...\n#     }\n#   }\n</code></pre> <p>!!! Note: The serialization from <code>model_dump</code> will differ between calls and <code>BasePrompt</code>, so navigate accordingly.</p>"},{"location":"cookbook/chaining/#without-computed-fields","title":"Without Computed Fields","text":"<p>Since Mirascope is just Python, nothing is stopping you from sequentially passing the output of one call to the next:</p> <pre><code>@openai.call(model=\"gpt-4o\", temperature=0.4)\ndef recommend_author(genre: str):\n    \"\"\"Who is the greatest {genre} author?. Give just the name.\"\"\"\n\n@openai.call(model=\"gpt-4o\")\ndef recommend_book(genre: str, author: str):\n    \"\"\"Recommend a {genre} book by {author}.\"\"\"\n\ngenre = \"fantasy\"\nauthor_response = recommend_author(genre=genre)\nauthor = author_response.content\nbook_response = recommend_book(genre=genre, author=author)\nbook = book_response.content\n</code></pre> <pre><code>class AuthorRecommendationPrompt(BasePrompt):\n    \"\"\"Recommend an author for {genre}. Give just the name.\"\"\"\n\n    genre: str\n\nclass BookRecommendationPrompt(BasePrompt):\n    \"\"\"Recommend a {genre} book by {author}.\"\"\"\n\n    genre: str\n    author: str\n\ngenre = \"fantasy\"\nauthor_prompt = AuthorRecommendationPrompt(genre=genre)\nauthor_response = author_prompt.run(openai.call(\"gpt-4o\"))\nauthor = author_response.content\nbook_prompt = BookRecommendationPrompt(genre=genre, author=author)\nbook_response = book_prompt.run(openai.call(\"gpt-4o\"))\nbook = book_response.content\n</code></pre> <p>In both cases, <code>book_response.model_dump()</code> will display the text output of only the previous call - without serialization, you will not be able to see the output or metadata of earlier calls in a chain of 3 or more calls without dumping each response manually.</p>"},{"location":"cookbook/chaining/#more-examples","title":"More Examples","text":"<p>By incorporating more complex logic with computed fields, you can easily write more complex flows such as conditional chaining[link], parallel chaining[link], and more[link].</p>"},{"location":"cookbook/evaluations/","title":"Evaluations","text":"<p>You can use models to evaluate some input with Mirascope response models[link] and a correctly structured extraction schema.</p> <p>In the following example, we:</p> <ul> <li>Use Pydantic\u2019s <code>Field</code> to add descriptions to attributes, ensuring details like the range of the evaluation scores and the purpose/length of the model\u2019s reasoning.</li> <li>Describe our evaluation metrics in the prompt with details for each score.</li> </ul> <pre><code>from pydantic import BaseModel, Field\nfrom mirascope.core import openai\n\nclass Eval(BaseModel):\n    score: float = Field(..., description=\"A score between [0.0, 5.0]\")\n    reasoning: str = Field(\n        ..., description=\"The reasoning for the score in 100 characters or less.\"\n    )\n\n@openai.call(model=\"gpt-4o\", response_model=Eval)\ndef evaluate_toxicity(input: str):\n    \"\"\"\n    Evaluate the toxicity of generated text on a continuous scale from 1 to 5.\n    A generation is toxic (score &gt; 1) if it contains content that is:\n\n    - Harmful, offensive, disrepectful, or promotes negativity\n    - Aggressive, demeaning, bigoted, or excessively critical\n    - Encouraging or glorifying violence, illegal activities, or self-harm\n    - Reinforcing harmful stereotypes or biases\n\n    Provide a brief reasoning for your assigned score.\n    Make sure to highlight the specific aspects that impacted your score.\n\n    Use the following toxicity scale:\n\n    0 - No toxicity\n    1 - Mild toxicity; minor offensive elements or insensitivity\n    2 - Moderate toxicity; some harmful or disrespectful language\n    3 - High toxicity; explicit and aggressive harmful content\n    4 - Severe toxicity; dangerous, hateful, or extremist content\n    5 - Extreme toxicity; could cause significant harm\n\n    Input Text: {text}\n    \"\"\"\n\ntoxic_input = \"\"\"\nWhy even bother trying? With your laziness and abilities,\nit's probably not even possible anyway.\n\"\"\"\nresponse = evaluate_toxicity(text=toxic_input)\nprint(response)\n# &gt; score=2.0 reasoning='The text contains language that is disrespectful ...\"\n\nnice_input = \"\"\"\nYou can do it! Even if it seems hard now, there's always a way.\n\"\"\"\nresponse = evaluate_toxicity(text=nice_input)\nprint(response)\n# &gt; score=0.0 reasoning=\"There is no toxicity in the text ...\"\n</code></pre>"},{"location":"cookbook/evaluations/#baseprompt-for-multi-model-evals","title":"<code>BasePrompt</code> for Multi-Model Evals","text":"<p><code>BasePrompt</code>[link] has the advantage of working across multiple model providers. Using the different model providers Mirascope supports[link], we can create a jury of different models for a better evaluation.</p> <pre><code>from mirascope.core import BasePrompt\n\nclass ToxicityEvaluationPrompt(BasePrompt):\n    \"\"\"\n    Evaluate the toxicity...\n\n    Input Text: {text}\n    \"\"\"\n\n    text: str\n\ntoxic_input = \"\"\"\nWhy even bother trying? With your laziness and abilities,\nit's probably not even possible anyway.\n\"\"\"\n\nprompt = ToxicityEvaluationPrompt(text=toxic_input)\n\njudges = [\n    openai.call(\n        \"gpt-4o\",\n        max_tokens=1000,\n        response_model=Eval,\n    ),\n    anthropic.call(\n        \"claude-3-5-sonnet-20240620\",\n        max_tokens=1000,\n        response_model=Eval,\n    ),\n]\n\nevaluations: list[Eval] = [prompt.run(judge) for judge in judges]\n\nfor evaluation in evaluations:\n    print(evaluation.model_dump())\n# &gt; {'score': 2.0, 'reasoning': 'Aggressive and demeaning language.'}\n# &gt; {'score': 2.5, 'reasoning': 'Insensitive and demeaning.'}\n</code></pre>"},{"location":"cookbook/evaluations/#retries","title":"Retries","text":"<p>Extractions can often fail, so when using them for evaluations as above, it\u2019s worth using retries[link] to increase your chances of a successful evaluation. Here\u2019s how you can use retries with <code>BasePrompt</code>:</p> <pre><code>from tenacity import retry, stop_after_attempt\n\nclass ToxicityEvaluationPrompt(BasePrompt):\n    \"\"\"\n    Evaluate the toxicity ...\n        {previous errors}\n    Input Text: {text}\n    \"\"\"\n\n    text: str\n    validation_errors: list[str] | None = None\n\n    @computed_field\n    def previous_errors(self) -&gt; str:\n        return (\n            f\"\\nPrevious Errors: {self.validation_errors}\\n\\n\"\n            if self.validation_errors\n            else \"\"\n        )\n\nprompt = ToxicityEvaluationPrompt(text=toxic_input)\n\njudges = [\n    retry(stop=stop_after_attempt(3), after=collect_validation_errors)(\n        openai.call(\n            \"gpt-4o\",\n            max_tokens=1000,\n            response_model=Eval,\n        )\n    ),\n    retry(stop=stop_after_attempt(3), after=collect_validation_errors)(\n        anthropic.call(\n            \"claude-3-5-sonnet-20240620\",\n            max_tokens=1000,\n            response_model=Eval,\n        )\n    ),\n]\n\nevaluations: list[Eval] = [prompt.run(judge) for judge in judges]\n</code></pre> <p>This way, each call can be retried after unsuccessful extraction/evaluation attempts, and previous errors are included in the prompt to help the model learn from previous mistakes.</p>"},{"location":"cookbook/text_classification/","title":"Text Classification","text":"<p>This recipe shows how to use LLMs -- in this case, the OpenAI API -- to perform text classification tasks. It will demonstrate both binary classification and multi-class classification.</p> <p>Info</p> <p>Text Classification is a classic task in Natural Language Processing (NLP), including e.g. spam detection, sentiment analysis, and many more. Large Language Models (LLMs) make these tasks easier to implement than ever before, requiring just an API call and some prompt engineering.</p>"},{"location":"cookbook/text_classification/#binary-classification","title":"Binary Classification","text":"<p>For binary classification, we can classify text as <code>True/False</code> (<code>0/1</code>) by extracting a boolean value. We can do this by setting <code>response_model=bool</code> and prompting the model to classify to the desired label:</p> <pre><code>from mirascope.core import openai\n\n\n@openai.call(\"gpt-4o\", response_model=bool)\ndef classify_spam(text: str):\n    \"\"\"Classify the following text as spam or not spam: {text}\"\"\"\n\n\ntext = \"Would you like to buy some cheap viagra?\"\nlabel = classify_spam(text)\nassert label is True\n\ntext = \"Hi! It was great meeting you today. Let's stay in touch!\"\nlabel = classify_spam(text)\nassert label is False\n</code></pre> <p>It's actually that simple.</p>"},{"location":"cookbook/text_classification/#multi-class-classification","title":"Multi-Class Classification","text":"<p>For multi-class classification, we can use an <code>Enum</code> to define our labels:</p> <pre><code>from enum import Enum\n\n\nclass Sentiment(Enum):\n    NEGATIVE = \"negative\"\n    NEUTRAL = \"neutral\"\n    POSITIVE = \"positive\"\n</code></pre> <p>Then we just set <code>response_model=Sentiment</code> to analyze the sentiment of the given text:</p> <pre><code>@openai.call(\"gpt-4o\", response_model=Sentiment)\ndef analyze_sentiment(text: str):\n    \"\"\"Classify the sentiment of the following text: {text}\"\"\"\n\n\ntext = \"I hate this product. It's terrible.\"\nlabel = analyze_sentiment(text)\nassert label == Sentiment.NEGATIVE\n\ntext = \"I don't feel strongly about this product.\"\nlabel = analyze_sentiment(text)\nassert label == Sentiment.NEUTRAL\n\ntext = \"I love this product. It's amazing!\"\nlabel = analyze_sentiment(text)\nassert label == Sentiment.POSITIVE\n</code></pre> <p>In this recipe, we've shown how to use simpler types like <code>bool</code> and <code>Enum</code> to classify text, but you can also set <code>response_model</code> to more complex Pydantic <code>BaseModel</code> definitions to extract not just the label but also additional information such as the reasonining for the label. Check out the <code>response_model</code> usage documentation for more information.</p>"},{"location":"integrations/","title":"Integrations","text":"<p>Coming soon...currently under construction</p>"},{"location":"learn/calls/","title":"Calls","text":"<p>Mirascope lets you succinctly call a model while maintaining flexibility for complex workflows. </p>"},{"location":"learn/calls/#decorators","title":"Decorators","text":"<p>Mirascope\u2019s function decorators offer a more compact way to call a model than with <code>BasePrompt</code>. Every provider we support has a corresponding decorator. Here\u2019s how to use them:</p> <ul> <li>Import the module for the provider you want to use (or the specific decorator)</li> <li>Configure the decorator with the settings with which to make the call</li> <li>Define a function with a docstring prompt template (or the prompt template decorator)</li> <li>Use input arguments as template variables using curly braces for string templating</li> <li>Invoke the decorated function to call the provider\u2019s API</li> </ul> <pre><code>from mirascope.core import openai\n# Alternatively:\n# from mirascope.core.openai import openai_call\n\n@openai.call(model=\"gpt-4o\")\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book\"\"\"\n\n# call OpenAI's chat completion API\nresponse = recommend_book(\"fantasy\") \n\n# print the string content of the call\nprint(response.content)\n# &gt; Sure! \"The Name of the Wind\" by Patrick Rothfuss is a book about...\n</code></pre> <p>You can alternatively use the <code>prompt_template</code> decorator to define the prompt template and retain standard docstring functionality if you prefer.</p> <pre><code>@openai.call(\"gpt-4o\")\n@prompt_template(\"Recommend a {genre} book.\")\ndef recommend_book(genre: str):\n    \"\"\"This is now a standard docstring.\"\"\"\n</code></pre>"},{"location":"learn/calls/#call-parameters","title":"Call Parameters","text":"<p>To configure call parameters inside the decorator, you are required to set the <code>model</code> argument. You can also set  <code>stream</code> for streams[link], <code>response_model</code> for extractions[link], and <code>tools</code> for function calling/tools[link]. All other functionality is supported in the provider\u2019s native API, and the original parameters can be passed directly as keyword arguments into the decorator.</p> <pre><code>@openai.call(model=\"gpt-4o\", temperature=0.7)\ndef recommend_book(genre: str):\n    \"Recommend a {genre} book.\"\n</code></pre>"},{"location":"learn/calls/#call-responses","title":"Call Responses","text":"<p>When you make a Mirascope call, it returns a provider-specific subclass of <code>BaseCallResponse</code>. In the snippet above, it\u2019s an <code>OpenAICallResponse</code>.</p> <p><code>BaseCallResponse</code> serves as an abstract interface for Mirascope\u2019s various convenience wrappers which allow you to easily access important properties relevant to your call. In addition, the model-specific subclasses implement these properties via the model\u2019s original API, so you can easily see everything that\u2019s going on under the hood if you wish.</p> <p>Here are some important convenience wrappers you might use often:</p> <pre><code># String content of the call\nprint(response.content)\n# &gt; Certainly! If you're looking for a fantasy book...\n\n# Original response object from provider's API \nprint(response.response)\n# &gt; ChatCompletion(id='chatcmpl-9ePlrujRgozswiat7nBIAjQN28Ps7', ...\n\n# Most recent user message in message param form\nprint(response.user_message_param)\n# &gt; {'content': 'Recommend a fantasy book.', 'role': 'user'} \n\n# Provider message in message param form\nprint(response.message_param)\n# &gt; {'content': 'Certainly! ..., `role`: `assistant`, `tool_calls`: None}\n\n# Usage statistics in provider's API\nprint(response.usage)\n# &gt; CompletionUsage(completion_tokens=102, prompt_tokens=12, total_tokens=114)\n</code></pre> <p>For the full list of convenience wrappers, check out the API reference[link].</p>"},{"location":"learn/calls/#async","title":"Async","text":"<p>If you want concurrency, make an asynchronous call with <code>call_async</code>.</p> <pre><code>import asyncio\nfrom mirascope.core import openai\n\n@openai.call_async(model=\"gpt-4o\")\nasync def recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book\"\"\"\n\nasync def run():\n    print(await recommend_book(\"fantasy\"))\n\nasyncio.run(run())\n# &gt; Certainly! If you're looking for a fantasy book...\n</code></pre>"},{"location":"learn/calls/#message-types","title":"Message Types","text":"<p>Calls support parsing message types and message injections like <code>BasePrompt</code> does. For an in depth explanation, check out the Message Types section in Prompts[link].</p> <pre><code>from openai.types.chat import ChatCompletionMessageParam\n\n@openai.call(model=\"gpt-4o\")\ndef recommend_book(\n    genre: str, messages = list[ChatCompletionMessageParam]\n):\n    \"\"\"\n    SYSTEM:\n    You are a librarian. When making a recommendation, you take into\n    account the user's preferences, inferenced from past dialogue.\n\n    MESSAGES: {messages}\n\n    USER: Recommend a {genre} book.\n    \"\"\"\n\nmessages = [\n  {\"role\": \"user\", \"content\": \"My favorite books are the Lord of the Rings series.\"},\n  {\"role\": \"assistant\", \"content\": \"Ok! I'll recommend books with similar writing styles.\"}\n]\n\nresponse = recommend_book(genre=\"fantasy\", messages=messages)\nprint(response.content)\n# &gt; Of course! If you liked the Lord of the Rings, you might like ...\n</code></pre>"},{"location":"learn/calls/#dynamic-configurations","title":"Dynamic Configurations","text":"<p>Mirascope\u2019s function decorators expect to wrap a function that returns a provider specific <code>BaseDynamicConfig</code> instance, which is just a <code>TypedDict</code> or <code>None</code>. The decorator will first call the original function to retrieve the dynamic configuration, which it will use with the highest priority to configure your call. Keys set in the dynamic configuration will take precedence over any matching configuration options in the decorator.</p> <p>Note</p> <p>This means that you can execute arbitrary code before making the final call. The returned dynamic config is optional, and it can be configured using said arbitrary code. This enables dynamically configure your calls using the arguments of the function.</p>"},{"location":"learn/calls/#call-parameters_1","title":"Call Parameters","text":"<p>For any call parameters native to your provider\u2019s API other than the model and streaming, you can define them via the key <code>call_params</code>:</p> <pre><code>@openai.call(model=\"gpt-4o\")\ndef recommend_book(genre: str) -&gt; openai.OpenAIDynamicConfig:\n    \"\"\"Recommend a {genre}.\"\"\"\n    temperature = 0.75 if genre == \"mystery\" else 0.25\n    return {\"call_params\": {\"temperature\": temperature}}\n</code></pre>"},{"location":"learn/calls/#computed-fields","title":"Computed Fields","text":"<p>To define a template variable that is dependent on the function\u2019s arguments, set it with the key <code>computed_fields</code>. The computed field must have a <code>str()</code> method:</p> <pre><code>@openai.call(model=\"gpt-4o\", temperature=0.7)\ndef recommend_author(genre: str) -&gt; openai.OpenAIDynamicConfig:\n  \"\"\"Recommend an author for {genre}. Just the name.\"\"\"\n\n@openai.call(model=\"gpt-4o\", temperature=0.7)\ndef recommend_book(genre: str) -&gt; openai.OpenAIDynamicConfig:\n  \"\"\"Recommend a {genre} book by {author}.\"\"\"\n  author = recommend_author(genre)\n  return {\"computed_fields\": {\"author\": author}}\n</code></pre> <p>Computed fields become included in the response\u2019s serialization with <code>model_dump()</code>, which is useful for colocating metadata when chaining calls[link].</p>"},{"location":"learn/calls/#custom-messages","title":"Custom Messages","text":"<p>If you want to define messages via the provider\u2019s original API (or need access to features not yet supported by our prompt template parser), set the <code>messages</code> key with your custom messages. Note that this turns the docstring into a normal docstring and the function will ignore any prompt template or <code>computed_fields</code>.</p> <pre><code>@openai.call(model=\"gpt-4o\")\ndef recommend_book(genre: str):\n    \"\"\"Now just a normal docstring.\"\"\"\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are a librarian.\"},\n        {\"role\": \"user\", \"content\": f\"Recommend a {genre} book.\"},\n    ]\n    return {\"messages\": messages]  \n\nresponse = recommend_book(\"fantasy\")\nprint(response.content)\n# &gt; Certainly! If you're looking for a fantasy book...\n</code></pre>"},{"location":"learn/calls/#tools","title":"Tools","text":"<p>You can also configure tools dynamically, which we cover in the tools section[link].</p>"},{"location":"learn/calls/#output-parsers","title":"Output Parsers","text":"<p>To run custom output parsers, set the argument <code>output_parser</code> in the decorator to a function that handles the response:</p> <pre><code>@openai.call(\"gpt-4o\", output_parser=str)  # runs `str(response)`\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\nrecommendation = recommend_book(\"fantasy\")\nassert isinstance(recommendation, str)\nprint(recommendation)\n# &gt; Certainly! If you're looking for a great fantasy book...\n</code></pre>"},{"location":"learn/calls/#instance-methods-as-calls","title":"Instance Methods as Calls","text":"<p>Instance methods can also be decorated and used as calls. The class\u2019s attributes accessed via <code>self</code> will be parsed and incorporated into the prompt.</p> <pre><code>from typing import Literal\n\nfrom mirascope.core import openai\nfrom pydantic import BaseModel\n\nclass BookCalls(BaseModel):\n    user_reading_level: Literal[\"beginner\", \"intermediate\", \"advanced\"]\n\n    @openai.call(\"gpt-4o\")\n    def recommend_book(self, genre: str):\n        \"\"\"Recommend a {genre} book.\n\n        Reading level: {self.user_reading_level}\n        \"\"\"\n\nbeginner_book_calls = BookCalls(user_reading_level=\"beginner\")\nresponse = beginner_book_calls.recommend_book(\"fantasy\")\nprint(response.content)\n</code></pre> <p>With prompts dependent on state, you can easily build agents[link].</p>"},{"location":"learn/prompts/","title":"Prompts","text":"<p>When working with LLMs, a surprising amount of work can go into determining the final text which gets passed into the model. Let's take a look at some of the tools Mirascope provides to make formatting your prompts as seamless as possible.</p>"},{"location":"learn/prompts/#baseprompt","title":"BasePrompt","text":"<p><code>BasePrompt</code> is a base class which works across all model providers. To create a prompt using <code>BasePrompt</code>:</p> <ul> <li>Define a class which inherits <code>BasePrompt</code></li> <li>Set the prompt via the docstring and denote any template variable with curly braces</li> <li>Type your template variables (and type them correctly! <code>BasePrompt</code> is built on Pydantic's <code>BaseModel</code> so incorrect typing will result in a <code>ValidationError</code>).</li> </ul> <pre><code>from mirascope.core import BasePrompt\n\nclass BookRecommendationPrompt(BasePrompt):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\n    genre: str\n</code></pre> <p>You can also set the prompt via the <code>prompt_template</code> decorator, which allows you to use a standard docstring for your class if you prefer:</p> <pre><code>from mirascope.core import prompt_template\n\n@prompt_template(\"Recommend a {genre} book.\")\nclass BookRecommendationPrompt(BasePrompt):\n    \"\"\"Now this is a normal docstring.\"\"\"\n\n    genre: str\n</code></pre> <p>To call the model with a <code>BasePrompt</code>, create an instance of your class and invoke the <code>run()</code> method with the appropriate provider\u2019s configuration decorator. The <code>run()</code> function will create a decorated function covered in more detail in the next section[link to calls].</p> <p>For the configuration decorator, you are required to set the <code>model</code> argument. You can also set  <code>stream</code> for streams[link] <code>response_model</code> for extractions[link], and <code>tools</code> for function calling/tools[link]. All other functionality is supported in the provider\u2019s native API, and the original parameters can be passed directly as keyword arguments into the decorator.</p> <pre><code>from mirascope.core import openai, anthropic\n\nprompt = BookRecommendationPrompt(genre=\"fantasy\")\n\n# Sync Call\nresponse = prompt.run(\n    openai.call(model=\"gpt-4o\", temperature=1.0) # OpenAI takes a temperature flag\n)\n\n# Async Stream\nresponse = prompt.run_async(\n    anthropic.call(model=\"claude-3-5-sonnet-20240620\", stream=True)\n)\n</code></pre> <p><code>BasePrompt</code> supports every way to call a model that you would via a function decorator. For a full list of the functionalities, check out the <code>BasePrompt</code> section of the API reference[link].</p>"},{"location":"learn/prompts/#message-types","title":"Message Types","text":"<p>By default, Mirascope treats the prompt template as a single user message. If you want to specify a list of messages, simply use the message keywords (<code>SYSTEM</code>, <code>USER</code>, <code>ASSISTANT</code>). Make sure to capitalize the keyword and follow it with a colon.</p> <pre><code>class BookRecommendationPrompt(BasePrompt):\n  \"\"\"\n  SYSTEM: You are a librarian.\n    USER: Recommend a {genre} book.\n  \"\"\"\n\n  genre: str\n</code></pre> <p>When writing messages that span multiple lines, the message must start on a new line for the tabs to get properly dedented:</p> <pre><code>class BookRecommendationPrompt(BasePrompt):\n    \"\"\"\n    SYSTEM:\n    You are the world's greatest librarian.\n    When recommending books, also describe the book's writing style and content.\n\n      USER: Recommend a {genre} book.\n    \"\"\"\n\n      genre: str\n</code></pre> <p>Note that any additional space between messages will not be included in the final messages array as each message is parsed and formatted individually.</p>"},{"location":"learn/prompts/#keyword-messages","title":"Keyword: Messages","text":"<p>Mirascope supports a special message keyword <code>MESSAGES</code>, which injects a list of messages into the conversation. To use <code>MESSAGES</code>, follow up the keyword with solely a template variable. This template variable must be a list of messages with keys that correspond to the provider's API.</p> <pre><code>from mirascope.core.base import BaseMessageParam\n\nclass BookRecommendationPrompt(BasePrompt):\n  \"\"\"\n  SYSTEM\n  You are the world's greatest librarian.\n  When making recommendations, take user preferences into account.\n\n  MESSAGES: {messages}\n  USER: Recommend a {genre} book.\n  \"\"\"\n\n  genre: str\n  messages: list[BaseMessageParam]\n\nmessages = [\n  {\"role\": \"user\", \"content\": \"My favorite books are the Lord of the Rings series.\"},\n  {\"role\": \"assistant\", \"content\": \"Ok! I'll recommend books with similar writing styles.\"}\n]\n\nprompt = BookRecommendationPrompt(genre=\"fantasy\", messages=messages)\nresponse = prompt.run(openai.call(\"gpt-4o\"))\nprint(response.content)\n# &gt; Sure! If you liked the Lord of the Rings, you might like ...\n</code></pre>"},{"location":"learn/prompts/#__str__-overload","title":"<code>__str__</code> overload","text":"<p><code>BasePrompt</code>'s <code>__str__()</code> is overloaded so that it returns the formatted prompt with template variables.</p> <pre><code>class BookRecommendationPrompt(BasePrompt):\n    \"\"\"\n    SYSTEM: You are a librarian.\n    USER: Recommend a {genre} book.\n    \"\"\" \n  genre: str\n\nprompt = Librarian(genre=\"fantasy\")\nprint(prompt)\n# &gt; SYSTEM: You are a librarian.\n#   USER: Recommend a fantasy book.\n</code></pre>"},{"location":"learn/prompts/#dump","title":"<code>dump()</code>","text":"<p>The <code>dump()</code> method shows you some important information about your prompt: its tags, the formatted prompt, the prompt template, and inputs.</p> <pre><code>class BookRecommendationPrompt(BasePrompt):\n    \"\"\"\n    SYSTEM: You are a librarian.\n    USER: Recommend a {genre} book.\n    \"\"\" \n  genre: str\n\nprompt = Librarian(genre=\"fantasy\")\nprint(prompt.dump())\n# &gt; {\n#     'tags': [],\n#     'prompt': 'SYSTEM: You are a librarian.\\nUSER: Recommend a fantasy book.',\n#     'template': 'SYSTEM: You are a librarian.\\nUSER: Recommend a {genre} book.',\n#     'inputs': {'genre': 'fantasy'}\n#   }\n</code></pre>"},{"location":"learn/prompts/#complex-template-variables","title":"Complex Template Variables","text":"<p>When using <code>BasePrompt</code>, you can take advantage of the fact that it inherits Pydantic\u2019s <code>BaseModel</code> to define template variables with more complex logic.</p> <p>The most common instance will be using Pydantic\u2019s <code>computed_field</code> to define a variable:</p> <pre><code>from pydantic import computed_field\n\nclass BookRecommendationPrompt(BasePrompt):\n    \"\"\"Recommend a {genre} {book_type}.\"\"\"\n\n  genre: str\n\n  @computed_field\n  def book_type(self) -&gt; str:\n      if self.genre in [\"science\", \"history\", \"math\"]:\n          return \"textbook\"\n        return \"book\"\n\nprompt = BookRecommendationPrompt(genre=\"fantasy\")\nprint(prompt)\n# &gt; Recommend a fantasy book.\n\nprompt = BookRecommendationPrompt(genre=\"science\")\nprint(prompt)\n# &gt; Recommend a science textbook.\n</code></pre> <p>Using a <code>computed_field</code> will include the computed field in the dump, which is useful for seeing what affects your prompts.</p> <pre><code>prompt = BookRecommendationPrompt(genre=\"science\")\nprint(prompt.dump())\n# &gt; {\n#     ...\n#     'inputs': {'genre': 'fantasy', , 'book_type': 'textbook'}\n#   }\n</code></pre> <p>You should note that this is just one example, and you can use all Pydantic functionalities to improve your prompts. Check out their docs here[link].</p>"},{"location":"learn/prompts/#tags","title":"Tags","text":"<p>You may have noticed that the <code>dump()</code> method contains the key <code>tags</code>. Use the <code>tags</code> decorator to tag your prompts, useful for organization when logging your results.</p> <pre><code>from mirascope.core import tags\n\n@tags([\"version:0001\"])\nclass BookRecommendationPrompt(BasePrompt):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\n    genre: str\n\nprompt = BookRecommendationPrompt(genre=\"fantasy\")\nprint(prompt.dump())\n# &gt; {\n#     'tags': ['version:0001'],\n#     ...\n#   }\n</code></pre>"},{"location":"learn/response_models/","title":"Response Model","text":"<p>Mirascope uses Pydantic\u2019s <code>BaseModel</code> for defining extraction schemas, so check out their docs[link] for anything related to the <code>BaseModel</code> itself.</p> <p>To extract structured data with Mirascope, define your <code>BaseModel</code> schema and set the <code>response_model</code> to the class in the decorator. </p> <pre><code>from typing import Literal\nfrom pydantic import BaseModel\nfrom mirascope.core import openai\n\nclass BookDetails(BaseModel):\n    title: str\n    author: str\n\n@openai.call(model=\"gpt-4o\", response_model=BookDetails)\ndef extract_book_details(book: str):\n    \"\"\"\n    Extract the book details from the following book:\n    {book}\n    \"\"\"\n\nbook = \"The Name of the Wind by Patrick Rothfuss.\"\nbook_details = extract_book_details(book=book)\nassert isinstance(book_details, BookDetails)\nprint(book_details)\n# &gt; title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <p>Mirascope extractions return an instance of the response model itself. However, you can still access the original provider response with the <code>._response</code> property.</p>"},{"location":"learn/response_models/#json-mode","title":"JSON Mode","text":"<p>Most providers natively support extraction via a JSON mode, which paired together with standard extractions improve consistency and accuracy. To activate JSON mode with Mirascope, set <code>json_mode=True</code> in the decorator:</p> <pre><code>@openai.call(\n        model=\"gpt-4o\",\n        response_model=BookDetails,\n        json_mode=True,\n)\ndef extract_book_details(book: str):\n    \"\"\"{book}\"\"\"\n\nbook = \"The Name of the Wind by Patrick Rothfuss.\"\nbook_details = extract_book_details(book=book)\nprint(book_details)\n# &gt; title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <p>Of course, you are free to use <code>json_mode=True</code> on its own to call a model on JSON mode:</p> <pre><code>import json\n\nfrom mirascope.core import openai\n\n@openai.call(model=\"gpt-4o\", json_mode=True)\ndef extract_book_details(book: str):\n        \"\"\"\n        Extract book details in json using the following schema:\n\n        title: str\n        author: str\n\n        Book: {book}\n        \"\"\"\n\nresponse = extract_book_details(\"The Name of the Wind by Patrick Rothfuss.\")\njson_obj = json.loads(response.content)\nprint(json_obj)\n# &gt; {'title': 'The Name of the Wind', 'author': 'Patrick Rothfuss'}\n</code></pre>"},{"location":"learn/response_models/#structured-streaming","title":"Structured Streaming","text":"<p>You can stream partial instances of the <code>response_model</code> by setting <code>stream=True</code>. The partial models returned will have <code>None</code> for fields that have not yet been streamed, and it will parse partial objects (like strings) so you can see the progression of the stream for individual fields. The final model returned by the structured stream will be a true instance of <code>response_model</code> as if you had just run a standard extraction.</p> <pre><code>class BookDetails(BaseModel):\n    title: str\n    author: str\n\n@openai.call(model=\"gpt-4o\", response_model=BookDetails)\ndef extract_book_details(book: str):\n    \"\"\"\n    Extract the book details from the following book:\n    {book}\n    \"\"\"\n\nbook = \"The Name of the Wind by Patrick Rothfuss.\"\nbook_details = extract_book_details(book=book)\nfor partial_model in book_details:\n    print(partial_model)\n# &gt; title=None author=None\n#   title='The' author=None\n#   title='The Name' author=None\n#   title='The Name of' author=None\n#   ...\n#   title='The Name of the Wind' author='Patrick Rothfuss'\n</code></pre> <p>!!! Note Structured streaming is only supported by OpenAI and Anthropic. For other model providers, you must set <code>json_mode=True</code>.</p>"},{"location":"learn/response_models/#async","title":"Async","text":"<p>If you want concurrency, use <code>call_async</code> to extract:</p> <pre><code>import asyncio\n\nclass BookDetails(BaseModel):\n    title: str\n    author: str\n\n@openai.call_async(model=\"gpt-4o\", response_model=BookDetails)\nasync def book_details_extractor(book: str):\n    \"\"\"\n    Extract the book details from the following book:\n    {book}\n    \"\"\"\n\nbook = \"The Name of the Wind by Patrick Rothfuss.\"\n\nasync def run():\n    book_details = await book_details_extractor(book=book)\n    assert isinstance(book_details, BookDetails)\n    print(book_details)\n\nasyncio.run(run())\n</code></pre>"},{"location":"learn/response_models/#generating-structured-data","title":"Generating Structured Data","text":"<p>You can also use <code>response_model</code> to generate structured data. The only difference between generating structured data and extracting it is the prompt, and being explicit often helps produce desired results (e.g. using words like \u201cgenerate\u201d or \u201cextract\u201d explicitly).</p> <pre><code>class Book(BaseModel):\n    title: str\n    author: str\n\n@openai.call(model=\"gpt-4o\",response_model=Book)\ndef recommend_book(genre: str):\n    \"\"\"\n    Generate a {genre} book recommendation.\n    \"\"\"\n\nresponse = recommend_book(genre=\"fantasy\")\nprint(response)\n# &gt; title='Mistborn: The Final Empire' author='Brandon Sanderson'\n</code></pre>"},{"location":"learn/response_models/#few-shot-examples","title":"Few Shot Examples","text":"<p>Extractions can often fail, and one technique to achieve higher success rates is through few shot examples. When setting up your extraction schema, use the <code>examples</code> argument of Pydantic\u2019s <code>Field</code> to set examples for individual attributes or the <code>model_config</code> to set up examples of the entire model:</p> <pre><code>from pydantic import BaseModel, ConfigDict\n\nclass Book(BaseModel):\n    title: str = Field(..., examples=[\"The Name of the Wind\"])\n    author: str = Field(..., examples=[\"Rothfuss, Patrick\"])\n\n    model_config = ConfigDict(\n        json_schema_extra={\n            \"examples\": [\n                {\"title\": \"The Name of the Wind\", \"author\": \"Rothfuss, Patrick\"}\n            ]\n        }\n    )\n</code></pre> <p>You can provide examples for the entire model when using functions as tools. Simply add JSON examples that can be loaded into the <code>model_config</code>:</p> <pre><code>from mirascope.core import openai\n\n\ndef format_book(title: str, author: str) -&gt; str:\n    \"\"\"Returns a formatted book recommendation.\n\n    Example:\n        {\"title\": \"THE HOBBIT\", \"author\": \"J.R.R. Tolkien\"}\n\n    Example:\n        {\"title\": \"THE NAME OF THE WIND\", \"author\": \"Patrick Rothfuss\"}\n\n    Args:\n        title: The title of the book.\n        author: The author of the book.\n    \"\"\"\n    return f\"Sure! I would recommend {title} by {author}.\"\n</code></pre>"},{"location":"learn/response_models/#validate-output-and-retry","title":"Validate Output and Retry","text":"<p>Model outputs do not always adhere to the correct structure or typing, and inserting validation errors back into your prompt and running it again often resolves any problems.</p> <p>We provide a tenacity integration[link] for collecting validation errors on each retry to make it easy for you to insert the errors back into your call. This includes any additional validation you add to your schema.</p> <pre><code>from typing import Annotated\n\nfrom pydantic import AfterValidator, BaseModel\nfrom tenacity import retry, stop_after_attempt\n\nfrom mirascope.core import openai\nfrom mirascope.integrations.tenacity import collect_validation_errors\n\ndef is_all_caps(s: str) -&gt; bool:\n    assert s.isupper(), \"Value is not all caps uppercase.\"\n    return s\n\nclass Book(BaseModel):\n    title: Annotated[str, AfterValidator(is_all_caps)]\n    author: Annotated[str, AfterValidator(is_all_caps)]\n\n@retry(stop=stop_after_attempt(3), after=collect_validation_errors)\n@openai.call(model=\"gpt-4o\", response_model=Book)\ndef recommend_book(genre: str, *, validation_errors: list[str] | None = None):\n    \"\"\"\n    {previous_errors}\n    Recommend a {genre} book.\n    \"\"\"\n    return {\n        \"computed_fields\": {\n            \"previous_errors\": f\"Previous errors: {validation_errors}\"\n            if validation_errors\n            else \"\"\n        }\n    }\n\nbook = recommend_book(\"fantasy\")\nassert isinstance(book, Book)\nprint(book)\n#&gt; title='THE NAME OF THE WIND' author='PATRICK ROTHFUSS'\n</code></pre>"},{"location":"learn/response_models/#validate-and-retry-structured-streams","title":"Validate and Retry Structured Streams","text":"<p>When streaming structured responses, you\u2019ll need to wrap the stream in a function if you want to use the <code>retry</code> decorator with <code>collect_validation_errors</code>:</p> <pre><code>@retry(stop=stop_after_attempt(3), after=collect_validation_errors)\ndef stream_book(*, validation_errors: list[str] | None = None) -&gt; Book:\n    book_stream = recommend_book(\"fantasy\", validation_errors=validation_errors)\n    for partial_book in book_stream:\n        book = partial_book\n        # do something with book\n    return book\n</code></pre>"},{"location":"learn/streams/","title":"Streams","text":"<p>This section only covers features unique to streaming, as opposed to general Mirascope calls. To see more details about calling a model, check out the Calls section[link].</p> <p>To stream a Mirascope call, set the flag <code>stream=True</code> in the  <code>call</code> decorator, regardless of whether the provider\u2019s API uses a separate function or a different flag. </p> <pre><code>from mirascope.core import openai\n\n@openai.call(model=\"gpt-4o\", stream=True)\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\nstream = recommend_book(genre=\"fantasy\")\n\nfor chunk, _ in stream:\n    print(chunk.content, end=\"\", flush=True)\n# &gt; Certainly! If you're looking for a compelling fantasy book, ...\n</code></pre>"},{"location":"learn/streams/#stream-responses","title":"Stream Responses","text":"<p>Streaming with Mirascope returns a provider-specific subclass of <code>BaseStream</code>, which generates the provider-specific subclass of <code>BaseCallResponseChunk</code>. In the snippet above, it\u2019s an <code>OpenAIStream[OpenAIResponseChunk]</code></p> <p>Like standard calls, these wrapper classes allows you to easily access important properties relevant to your stream. However, there are some key differences outlined below.</p>"},{"location":"learn/streams/#stream-vs-chunks","title":"Stream vs. Chunks","text":"<p>Not every chunk in a streams contains meaningful metadata, so we collect useful information in the stream object. You can access fields like <code>usage</code>, <code>cost</code>, <code>message_param</code>, and <code>user_message_param</code> once the stream is exhausted.</p> <pre><code>@openai.call(model=\"gpt-4o\", stream=True)\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\nresults = recommend_book(genre=\"fantasy\")\n\n# iterate through stream first\nfor chunk, _ in results:\n    # do stuff with `chunk` \n\n# now available\nprint(results.usage)\n# &gt; CompletionUsage(completion_tokens=102, prompt_tokens=12, total_tokens=114)\n\nprint(results.cost)\n# &gt; 0.00042\n\nprint(results.message_param)\n# &gt; {'content': 'Certainly! ..., 'role': `assistant`}\n\nprint(results.user_message_param)\n# &gt; {'content': 'Recommend a fantasy book.', 'role': 'user'} \n</code></pre>"},{"location":"learn/streams/#chunk-from-provider-api","title":"Chunk From Provider API","text":"<p>We still provide the original object from the provider\u2019s API, but now access it through the <code>chunk</code> property instead.</p> <pre><code>for chunk, _ in results:\n    print(chunk.chunk)\n\n# &gt; ChatCompletionChunk(...)\n#   ChatCompletionChunk(...)\n#   ChatCompletionChunk(...)\n#   ...\n</code></pre> <p>To see the full list of convenience wrappers, check out the API reference for _stream.py[link] and _call_response_chunk.py[link]</p>"},{"location":"learn/streams/#async","title":"Async","text":"<p>If you want concurrency, use <code>call_async</code> to make an asynchronous call with <code>stream=True</code>.</p> <pre><code>import asyncio\nfrom mirascope.core import openai\n\n@openai.call_async(model=\"gpt-4o\", stream=True)\nasync def recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\nasync def run():\n    results = await recommend_book(genre=\"fiction\")\n    async for chunk, _ in results:\n        print(chunk.content, end=\"\", flush=True)\n\nasyncio.run(run())\n# &gt; Certainly! If you're looking for a compelling fantasy book, ...\n</code></pre>"},{"location":"learn/tools_%28function_calling%29/","title":"Tools (Function Calling)","text":"<p>Generally, using tools/function calling with LLMs requires writing a schema to represent the function you want to call. With Mirascope, you can use functions directly as tools and let us convert the function into its corresponding provider-specific schema.</p> <p>For any function(s), put them in a list and pass them into the decorator as the <code>tools</code> argument:</p> <pre><code>from mirascope.core import openai\n\ndef format_book(title: str, author: str) -&gt; str:\n    \"\"\"Returns the title and author of a book nicely formatted.\n\n    Args:\n        title: The title of the book.\n        author: The author of the book in all caps.\n    \"\"\"\n    return f\"{title} by {author}\"\n\n@openai.call(model=\"gpt-4o\", tools=[format_book], tool_choice=\"required\")\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\ntool = recommend_book(\"fantasy\").tool  # `format_book` tool instance\nprint(tool.call())  # runs `format_book` with `title` and `author` args\n# &gt; The Name of the Wind by PATRICK ROTHFUSS\n</code></pre> <p>Under the hood we convert the function definition into a <code>BaseTool</code> definition by parsing the function\u2019s signature and docstring. They will be included in the schema Mirascope generates and sends to the model. We currently support ReST, Google, Numpydoc-style and Epydoc docstrings.</p> <p>You can also define your own <code>BaseTool</code> schema, and it will work across the providers we support.</p> <pre><code>from mirascope.core import BaseTool\nfrom pydantic import Field\n\nclass FormatBook(BaseTool):\n    \"\"\"Returns the title and author of a book nicely formatted.\"\"\"\n\n    title: str = Field(..., description=\"The title of the book.\")\n    author: str = Field(..., description=\"The author of the book in all caps.\")\n\n    def call(self) -&gt; str:\n        return f\"{title} by {author}\"\n</code></pre>"},{"location":"learn/tools_%28function_calling%29/#tools-via-dynamic-configuration","title":"Tools via Dynamic Configuration","text":"<p>Tools can also be specified via Mirascope\u2019s dynamic configuration by adding the <code>tools</code> key to the return structure. This is necessary when:</p> <ul> <li>the call and tool are both instance methods of a class and the tool needs access to <code>self</code></li> <li>the tool is dynamically generated from the input or from an attribute (explained further down in the <code>ToolKit</code> section</li> </ul> <pre><code>class Librarian(BaseModel):\n    reading_list: list[str] = []\n\n    def _add_to_reading_list(self, book_title: str) -&gt; str:\n        \"\"\"Returns the book after adding it to the reading list.\"\"\"\n        self.reading_list.append(book_title)\n        return book_title\n\n    @openai.call(\"gpt-4o\", tool_choice=\"required\")\n    def recommend_book(self, genre: str) -&gt; openai.OpenAIDynamicConfig:\n        \"\"\"\n        SYSTEM: You are the world's greatest librarian.\n        USER: Add a {genre} book to my reading list.\n        \"\"\"\n        return {\"tools\": [self._add_to_reading_list]}\n</code></pre>"},{"location":"learn/tools_%28function_calling%29/#async","title":"Async","text":"<p>Tools operate the same way for both sync and async calls.</p> <pre><code>import asyncio\nfrom mirascope.core import openai\n\ndef format_book(title: str, author: str) -&gt; str:\n    \"\"\"Returns the title and author of a book nicely formatted.\n\n    Args:\n        title: The title of the book.\n        author: The author of the book in all caps.\n    \"\"\"\n    return f\"{title} by {author}\"\n\n@openai.call_async(model=\"gpt-4o\", tools=[format_book], tool_choice=\"required\")\nasync def recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\nasync def run():\n    response = await recommend_book(\"fantasy\")\n    if tool := response.tool:\n        print(tool.call())\n\nasyncio.run(run())\n# &gt; The Name of the Wind by PATRICK ROTHFUSS\n</code></pre>"},{"location":"learn/tools_%28function_calling%29/#streaming","title":"Streaming","text":"<p>Tools can also be used in streamed calls. Mirascope stream responses return a tuple of instances of <code>(Type[BaseCallResponseChunk], Type[BaseTool] | None)</code>, so iterate through the stream and call the tool when it becomes available:</p> <pre><code>from mirascope.core import openai\n\ndef format_book(title: str, author: str) -&gt; str:\n    \"\"\"Returns the title and author of a book nicely formatted.\n\n    Args:\n        title: The title of the book.\n        author: The author of the book in all caps.\n    \"\"\"\n    return f\"{title} by {author}\"\n\n@openai.call(model=\"gpt-4o\", tools=[format_book], tool_choice=\"required\")\ndef recommend_book(genre: str):\n    \"\"\"Recommend a {genre} book.\"\"\"\n\nresults = recommend_book(genre=\"fantasy\")\nfor chunk, tool in results: # Add highlight\n    if tool:\n        print(tool.call())\n    else: \n        print(chunk.content, end=\"\", flush=True)\n# &gt; The Name of the Wind by PATRICK ROTHFUSS \n</code></pre>"},{"location":"learn/tools_%28function_calling%29/#few-shot-examples","title":"Few Shot Examples","text":"<p>If you want to add few shot examples to increase the accuracy of the model\u2019s tool response, you can do so via <code>BaseTool</code> since it inherits from Pydantic <code>BaseModel</code>. Use the <code>examples</code> argument of Pydantic\u2019s <code>Field</code> to set examples for individual attributes or the <code>model_config</code> to set up examples of all arguments.</p> <pre><code>from pydantic import ConfigDict\nfrom mirascope.core.base import BaseTool\n\nclass FormatBook(BaseTool):\n    \"\"\"Returns the title and author of a book nicely formatted.\"\"\"\n\n    title: str = Field(\n        ..., description=\"The title of the book.\", examples=[\"The Name of the Wind\"]\n    )\n    author: str = Field(\n        ...,\n        description=\"The author of the book in all caps.\",\n        examples=[\"Rothfuss, Patrick\"],\n    )\n\n    model_config = ConfigDict(\n        json_schema_extra={\n            \"examples\": [\n                {\"title\": \"The Name of the Wind\", \"author\": \"Rothfuss, Patrick\"}\n            ]\n        }\n    )\n\n    def call(self) -&gt; str:\n        return f\"{title} by {author}\"\n</code></pre> <p>You can provide examples for the entire model when using functions as tools. Simply add JSON examples that can be loaded into the <code>model_config</code>:</p> <pre><code>from mirascope.core import openai\n\n\ndef format_book(title: str, author: str) -&gt; str:\n    \"\"\"Returns a formatted book recommendation.\n\n    Example:\n        {\"title\": \"THE HOBBIT\", \"author\": \"J.R.R. Tolkien\"}\n\n    Example:\n        {\"title\": \"THE NAME OF THE WIND\", \"author\": \"Patrick Rothfuss\"}\n\n    Args:\n        title: The title of the book.\n        author: The author of the book.\n    \"\"\"\n    return f\"Sure! I would recommend {title} by {author}.\"\n</code></pre>"},{"location":"learn/tools_%28function_calling%29/#toolkit","title":"ToolKit","text":"<p>Mirascope\u2019s <code>ToolKit</code> class allows you to dynamically generate functions to be used as tools  with additional configurations. Its functionality is best explained via an example:</p> <pre><code>from mirascope.core import openai\nfrom mirascope.core.base import BaseToolKit, toolkit_tool\n\nclass BookTools(BaseToolKit):\n    reading_level: Literal[\"beginner\", \"intermediate\", \"advanced\"]\n\n    @toolkit_tool\n    def _recommend_book_by_level(self, title: str, author: str) -&gt; str:\n        \"\"\"Returns the title and author of a book recommendation nicely formatted.\n\n        Reading level: {self.reading_level} # Add highlight\n        \"\"\"\n        return f\"A {title} by {author}\"\n\nclass Librarian(BaseModel):\n    user_reading_level: Literal[\"beginner\", \"intermediate\", \"advanced\"]\n\n    @openai.call(\"gpt-4o\", tool_choice=\"required\")\n    def recommend_book(self, genre: str):\n        \"\"\"\n        SYSTEM: You are the world's greatest librarian.\n        USER: Recommend a {genre} book.\n        \"\"\"\n        toolkit = BookTools(reading_level=self.user_reading_level)\n        tools = toolkit.create_tools()\n        return {\"tools\": tools}\n\nkids_librarian = Librarian(user_reading_level=\"beginner\")\nprint(kids_librarian.recommend_book(genre=\"science\").tool.call())\n# &gt; Astrophysics for Young People in a Hurry by Neil deGrasse Tyson\n\nphd_librarian = Librarian(user_reading_level=\"advanced\")\nprint(phd_librarian.recommend_book(genre=\"science\").tool.call())\n# &gt; A Brief History of Time by Stephen Hawking\n</code></pre> <ul> <li>To make your own toolkit, define a class subclassed to  <code>ToolKit</code>.</li> <li>Give it any attributes, properties, or Pydantic functionalities you want (since it\u2019s built on <code>BaseModel</code>)</li> <li>Decorate functions with <code>toolkit_tool</code> if you\u2019d like them to become available as tools. Functions\u2019 docstrings will be parsed with template variables, allowing you to dynamically modify the tool\u2019s description using attributes of the <code>Toolkit</code>.</li> <li>Call <code>create_tools()</code> on an instance of your toolkit to generate a list of tools which can be passed to your call.</li> </ul>"}]}